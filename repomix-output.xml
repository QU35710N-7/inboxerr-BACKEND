This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
alembic.ini
alembic/env.py
alembic/README
alembic/script.py.mako
alembic/versions/4669eb7a8878_add_user_metrics_table.py
alembic/versions/65b12145bf1a_add_message_templates_table.py
alembic/versions/a5a3f106534b_make_datetime_fields_timezone_aware.py
alembic/versions/be43f8bb42d0_add_campaign_personalization_and_import_.py
app/api/router.py
app/api/v1/dependencies.py
app/api/v1/endpoints/auth.py
app/api/v1/endpoints/campaigns.py
app/api/v1/endpoints/imports.py
app/api/v1/endpoints/messages.py
app/api/v1/endpoints/metrics.py
app/api/v1/endpoints/templates.py
app/api/v1/endpoints/webhooks.py
app/core/config.py
app/core/events.py
app/core/exceptions.py
app/core/security.py
app/db/base.py
app/db/repositories/base.py
app/db/repositories/campaigns.py
app/db/repositories/contacts.py
app/db/repositories/import_jobs.py
app/db/repositories/messages.py
app/db/repositories/metrics.py
app/db/repositories/templates.py
app/db/repositories/users.py
app/db/repositories/webhooks.py
app/db/session.py
app/main.py
app/models/base.py
app/models/campaign.py
app/models/contact.py
app/models/import_job.py
app/models/message.py
app/models/metrics.py
app/models/user.py
app/models/webhook.py
app/schemas/campaign.py
app/schemas/contact.py
app/schemas/import_job.py
app/schemas/message.py
app/schemas/metrics.py
app/schemas/template.py
app/schemas/user.py
app/scripts/create_admin.py
app/services/campaigns/processor.py
app/services/event_bus/bus.py
app/services/event_bus/events.py
app/services/imports/events.py
app/services/imports/parser.py
app/services/metrics/collector.py
app/services/rate_limiter.py
app/services/sms/retry_engine.py
app/services/sms/sender.py
app/services/webhooks/manager.py
app/services/webhooks/models.py
app/utils/datetime.py
app/utils/error_handling.py
app/utils/ids.py
app/utils/pagination.py
app/utils/phone.py
c.xml
database-schema.md
docker-compose.yml
Dockerfile
docs/FRONTEND_API_SETUP.md
docs/FRONTEND_DEVELOPER_GUIDE.md
docs/Message Template System - User Guide.md
docs/sprint.md
docs/tofix.md
project_structure.md
README.md
requirements.txt
scripts/generate_migration.py
scripts/mvp.sh
scripts/reset_db.py
scripts/run_tests.py
scripts/seed_db.py
scripts/seed_frontend_data.py
scripts/setup_test_db.py
tests/conftest.py
tests/core_functionality_test.py
tests/unit/api/messages/test_messages_endpoints.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/db/repositories/contacts.py">
"""
Contact repository for database operations related to imported contacts.
"""
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any, Tuple
from uuid import uuid4

from sqlalchemy import select, update, delete, and_, or_, desc, func, text
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.exc import IntegrityError, SQLAlchemyError

import logging
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.db.repositories.base import BaseRepository
from app.models.contact import Contact
from app.schemas.contact import ContactCreate, ContactUpdate

logger = logging.getLogger("inboxerr.db")


class ContactRepository(BaseRepository[Contact, ContactCreate, ContactUpdate]):
    """Contact repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and Contact model."""
        super().__init__(session=session, model=Contact)
    
    async def get_by_import_id(
        self,
        import_id: str,
        skip: int = 0,
        limit: int = 100
    ) -> Tuple[List[Contact], int]:
        """
        Get contacts for a specific import job with pagination.
        
        Args:
            import_id: Import job ID
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Contact], int]: (contacts, total_count)
        """
        # Build queries
        query = select(Contact).where(Contact.import_id == import_id)
        count_query = select(func.count(Contact.id)).where(Contact.import_id == import_id)
        
        # Add ordering
        query = query.order_by(desc(Contact.created_at))
        
        # Get total count
        total_result = await self.session.execute(count_query)
        total = total_result.scalar() or 0
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute query
        result = await self.session.execute(query)
        contacts = result.scalars().all()
        
        return contacts, total
    
    async def get_by_phone(self, phone: str, import_id: Optional[str] = None) -> Optional[Contact]:
        """
        Get contact by phone number, optionally within a specific import.
        
        Args:
            phone: Phone number to search for
            import_id: Optional import ID to scope search
            
        Returns:
            Contact: Found contact or None
        """
        query = select(Contact).where(Contact.phone == phone)
        
        if import_id:
            query = query.where(Contact.import_id == import_id)
        
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def search_contacts(
        self,
        import_id: Optional[str] = None,
        phone_query: Optional[str] = None,
        name_query: Optional[str] = None,
        tags: Optional[List[str]] = None,
        skip: int = 0,
        limit: int = 100
    ) -> Tuple[List[Contact], int]:
        """
        Search contacts with various filters.
        
        Args:
            import_id: Optional import ID filter
            phone_query: Optional phone number search (partial match)
            name_query: Optional name search (partial match)
            tags: Optional list of tags to filter by (OR operation)
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Contact], int]: (contacts, total_count)
        """
        # Build base query
        query = select(Contact)
        count_query = select(func.count(Contact.id))
        
        conditions = []
        
        # Apply filters
        if import_id:
            conditions.append(Contact.import_id == import_id)
        
        if phone_query:
            conditions.append(Contact.phone.ilike(f"%{phone_query}%"))
        
        if name_query:
            conditions.append(Contact.name.ilike(f"%{name_query}%"))
        
        if tags:
            # Search for any of the provided tags in the tags JSON array
            tag_conditions = []
            for tag in tags:
                tag_conditions.append(Contact.tags.op('?')(tag))
            conditions.append(or_(*tag_conditions))
        
        # Apply all conditions
        if conditions:
            query = query.where(and_(*conditions))
            count_query = count_query.where(and_(*conditions))
        
        # Add ordering
        query = query.order_by(desc(Contact.created_at))
        
        # Get total count
        total_result = await self.session.execute(count_query)
        total = total_result.scalar() or 0
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute query
        result = await self.session.execute(query)
        contacts = result.scalars().all()
        
        return contacts, total
    
    async def bulk_create_contacts(
        self,
        contacts: List[Contact],
        ignore_duplicates: bool = True
    ) -> Tuple[int, int, List[str]]:
        """
        Bulk create contacts with duplicate handling.
        
        Args:
            contacts: List of Contact objects to create
            ignore_duplicates: Whether to ignore duplicate phone numbers
            
        Returns:
            Tuple[int, int, List[str]]: (created_count, skipped_count, error_phones)
        """
        if not contacts:
            return 0, 0, []
        
        created_count = 0
        skipped_count = 0
        error_phones = []
        
        if ignore_duplicates:
            # Insert contacts one by one to handle duplicates gracefully
            for contact in contacts:
                try:
                    # Check if contact already exists
                    existing = await self.get_by_phone(contact.phone, contact.import_id)
                    if existing:
                        skipped_count += 1
                        logger.debug(f"Skipping duplicate contact: {contact.phone}")
                        continue
                    
                    self.session.add(contact)
                    created_count += 1
                    
                except IntegrityError:
                    await self.session.rollback()
                    skipped_count += 1
                    logger.debug(f"Constraint violation for contact: {contact.phone}")
                except SQLAlchemyError as e:
                    await self.session.rollback()
                    error_phones.append(contact.phone)
                    logger.error(f"Failed to create contact {contact.phone}: {str(e)}")
        else:
            # Try bulk insert first
            try:
                self.session.add_all(contacts)
                created_count = len(contacts)
            except IntegrityError:
                await self.session.rollback()
                # Fall back to individual inserts
                created_count, skipped_count, error_phones = await self.bulk_create_contacts(
                    contacts, ignore_duplicates=True
                )
        
        logger.info(f"Bulk contact creation: {created_count} created, {skipped_count} skipped, {len(error_phones)} errors")
        return created_count, skipped_count, error_phones
    
    async def get_contacts_count_by_import(self, import_id: str) -> int:
        """
        Get total count of contacts for an import job.
        
        Args:
            import_id: Import job ID
            
        Returns:
            int: Number of contacts
        """
        result = await self.session.execute(
            select(func.count(Contact.id)).where(Contact.import_id == import_id)
        )
        return result.scalar() or 0
    
    async def get_duplicate_phones(self, import_id: str) -> List[Dict[str, Any]]:
        """
        Find duplicate phone numbers within an import.
        
        Args:
            import_id: Import job ID
            
        Returns:
            List[Dict[str, Any]]: List of duplicate phone info
        """
        # Query to find phone numbers that appear more than once
        query = text("""
            SELECT phone, COUNT(*) as count, array_agg(id) as contact_ids
            FROM contact 
            WHERE import_id = :import_id 
            GROUP BY phone 
            HAVING COUNT(*) > 1
            ORDER BY count DESC
        """)
        
        result = await self.session.execute(query, {"import_id": import_id})
        duplicates = []
        
        for row in result:
            duplicates.append({
                "phone": row.phone,
                "count": row.count,
                "contact_ids": row.contact_ids
            })
        
        return duplicates
    
    async def delete_contacts_by_import(self, import_id: str) -> int:
        """
        Delete all contacts for a specific import job.
        
        Args:
            import_id: Import job ID
            
        Returns:
            int: Number of contacts deleted
        """
        result = await self.session.execute(
            delete(Contact).where(Contact.import_id == import_id)
        )
        
        deleted_count = result.rowcount
        
        logger.info(f"Deleted {deleted_count} contacts for import {import_id}")
        return deleted_count
    
    async def update_contact_tags(
        self,
        contact_id: str,
        tags: List[str]
    ) -> Optional[Contact]:
        """
        Update tags for a specific contact.
        
        Args:
            contact_id: Contact ID
            tags: New list of tags
            
        Returns:
            Contact: Updated contact or None
        """
        # Remove duplicates and empty tags
        clean_tags = list(set([tag.strip() for tag in tags if tag.strip()]))
        
        return await self.update(
            id=contact_id,
            obj_in={"tags": clean_tags}
        )
    
    async def add_tag_to_contacts(
        self,
        import_id: str,
        tag: str,
        phone_numbers: Optional[List[str]] = None
    ) -> int:
        """
        Add a tag to multiple contacts.
        
        Args:
            import_id: Import job ID
            tag: Tag to add
            phone_numbers: Optional list of specific phone numbers to tag
            
        Returns:
            int: Number of contacts updated
        """
        if not tag.strip():
            return 0
        
        # Build query conditions
        conditions = [Contact.import_id == import_id]
        if phone_numbers:
            conditions.append(Contact.phone.in_(phone_numbers))
        
        # Get contacts to update
        query = select(Contact).where(and_(*conditions))
        result = await self.session.execute(query)
        contacts = result.scalars().all()
        
        updated_count = 0
        for contact in contacts:
            current_tags = contact.tags or []
            if tag not in current_tags:
                current_tags.append(tag)
                contact.tags = current_tags
                updated_count += 1
        
        
        logger.info(f"Added tag '{tag}' to {updated_count} contacts in import {import_id}")
        return updated_count
    
    async def get_tags_summary(self, import_id: str) -> Dict[str, int]:
        """
        Get summary of all tags used in an import.
        
        Args:
            import_id: Import job ID
            
        Returns:
            Dict[str, int]: Tag name to count mapping
        """
        # This requires a more complex query to extract tags from JSON arrays
        query = text("""
            SELECT tag, COUNT(*) as count
            FROM contact, jsonb_array_elements_text(
                CASE 
                    WHEN tags IS NULL THEN '[]'::jsonb 
                    ELSE tags::jsonb 
                END
            ) AS tag
            WHERE import_id = :import_id
            GROUP BY tag
            ORDER BY count DESC
        """)
        
        result = await self.session.execute(query, {"import_id": import_id})
        tag_counts = {}
        
        for row in result:
            tag_counts[row.tag] = row.count
        
        return tag_counts
    
    async def export_contacts_to_dict(
        self,
        import_id: str,
        include_raw_data: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Export contacts to dictionary format for CSV/JSON export.
        
        Args:
            import_id: Import job ID
            include_raw_data: Whether to include original CSV data
            
        Returns:
            List[Dict[str, Any]]: List of contact dictionaries
        """
        contacts, _ = await self.get_by_import_id(import_id, limit=10000)  # Large limit for export
        
        export_data = []
        for contact in contacts:
            contact_dict = {
                "id": contact.id,
                "phone": contact.phone,
                "name": contact.name or "",
                "tags": ",".join(contact.tags or []),
                "csv_row_number": contact.csv_row_number,
                "created_at": contact.created_at.isoformat()
            }
            
            if include_raw_data and contact.raw_data:
                contact_dict["raw_data"] = contact.raw_data
            
            export_data.append(contact_dict)
        
        return export_data
    
    async def cleanup_orphaned_contacts(self) -> int:
        """
        Clean up contacts that reference non-existent import jobs.
        
        Returns:
            int: Number of orphaned contacts cleaned up
        """
        # This would require a JOIN with import_jobs table
        # For now, we'll implement a basic cleanup
        query = text("""
            DELETE FROM contact 
            WHERE import_id NOT IN (SELECT id FROM importjob)
        """)
        
        result = await self.session.execute(query)
        deleted_count = result.rowcount
        
        logger.info(f"Cleaned up {deleted_count} orphaned contacts")
        return deleted_count
</file>

<file path="app/db/repositories/import_jobs.py">
"""
ImportJob repository for database operations related to CSV import jobs.
"""
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any, Tuple
from uuid import uuid4

from sqlalchemy import select, update, delete, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.exc import IntegrityError

import logging
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.db.repositories.base import BaseRepository
from app.models.import_job import ImportJob, ImportStatus
from app.schemas.import_job import ImportJobCreate, ImportJobUpdate

logger = logging.getLogger("inboxerr.db")


class ImportJobRepository(BaseRepository[ImportJob, ImportJobCreate, ImportJobUpdate]):
    """ImportJob repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and ImportJob model."""
        super().__init__(session=session, model=ImportJob)
    
    async def get_by_owner(
        self, 
        owner_id: str, 
        status: Optional[ImportStatus] = None,
        skip: int = 0,
        limit: int = 100
    ) -> Tuple[List[ImportJob], int]:
        """
        Get import jobs for a specific owner with optional filtering.
        
        Args:
            owner_id: Owner user ID
            status: Optional status filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[ImportJob], int]: (jobs, total_count)
        """
        # Build base query
        query = select(ImportJob).where(ImportJob.owner_id == owner_id)
        count_query = select(func.count(ImportJob.id)).where(ImportJob.owner_id == owner_id)
        
        # Apply status filter if provided
        if status:
            query = query.where(ImportJob.status == status)
            count_query = count_query.where(ImportJob.status == status)
        
        # Add ordering
        query = query.order_by(desc(ImportJob.created_at))
        
        # Get total count
        total_result = await self.session.execute(count_query)
        total = total_result.scalar() or 0
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute query
        result = await self.session.execute(query)
        jobs = result.scalars().all()
        
        return jobs, total
    
    async def get_active_jobs_count(self, owner_id: str) -> int:
        """
        Get count of active (processing) import jobs for a user.
        
        Args:
            owner_id: Owner user ID
            
        Returns:
            int: Number of active import jobs
        """
        result = await self.session.execute(
            select(func.count(ImportJob.id)).where(
                ImportJob.owner_id == owner_id,
                ImportJob.status == ImportStatus.PROCESSING
            )
        )
        return result.scalar() or 0
    
    async def update_progress(
        self,
        job_id: str,
        rows_processed: int,
        errors: Optional[List[Dict[str, Any]]] = None
    ) -> Optional[ImportJob]:
        """
        Update import job progress.
        
        Args:
            job_id: Import job ID
            rows_processed: Number of rows processed
            errors: Optional list of error objects
            
        Returns:
            ImportJob: Updated import job or None
        """
        update_data = {
            "rows_processed": rows_processed,
            "updated_at": datetime.now(timezone.utc)
        }
        
        if errors is not None:
            update_data["errors"] = errors
        
        return await self.update(id=job_id, obj_in=update_data)
    
    async def complete_job(
        self,
        job_id: str,
        status: ImportStatus,
        rows_processed: int,
        errors: Optional[List[Dict[str, Any]]] = None
    ) -> Optional[ImportJob]:
        """
        Mark an import job as completed with final status.
        
        Args:
            job_id: Import job ID
            status: Final status (SUCCESS, FAILED, CANCELLED)
            rows_processed: Final number of rows processed
            errors: Optional list of error objects
            
        Returns:
            ImportJob: Updated import job or None
        """
        update_data = {
            "status": status,
            "rows_processed": rows_processed,
            "completed_at": datetime.now(timezone.utc),
            "updated_at": datetime.now(timezone.utc)
        }
        
        if errors is not None:
            update_data["errors"] = errors
        
        return await self.update(id=job_id, obj_in=update_data)
    
    async def get_by_hash(self, sha256: str, owner_id: str) -> Optional[ImportJob]:
        """
        Get import job by file hash and owner.
        
        Args:
            sha256: File SHA-256 hash
            owner_id: Owner user ID
            
        Returns:
            ImportJob: Found import job or None
        """
        result = await self.session.execute(
            select(ImportJob).where(
                ImportJob.sha256 == sha256,
                ImportJob.owner_id == owner_id
            )
        )
        return result.scalar_one_or_none()
    
    async def get_jobs_by_status(
        self,
        status: ImportStatus,
        limit: int = 100
    ) -> List[ImportJob]:
        """
        Get import jobs by status (useful for background processing).
        
        Args:
            status: Import status to filter by
            limit: Maximum number of jobs to return
            
        Returns:
            List[ImportJob]: List of import jobs
        """
        result = await self.session.execute(
            select(ImportJob)
            .where(ImportJob.status == status)
            .order_by(ImportJob.created_at)
            .limit(limit)
        )
        return result.scalars().all()
    
    async def cleanup_old_jobs(self, days_old: int = 30) -> int:
        """
        Clean up old completed import jobs.
        
        Args:
            days_old: Number of days old to consider for cleanup
            
        Returns:
            int: Number of jobs cleaned up
        """
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_old)
        
        # Delete old completed jobs
        result = await self.session.execute(
            delete(ImportJob).where(
                ImportJob.completed_at < cutoff_date,
                ImportJob.status.in_([ImportStatus.SUCCESS, ImportStatus.FAILED, ImportStatus.CANCELLED])
            )
        )
        
        deleted_count = result.rowcount
        
        logger.info(f"Cleaned up {deleted_count} old import jobs older than {days_old} days")
        return deleted_count
    
    async def get_user_statistics(self, owner_id: str) -> Dict[str, Any]:
        """
        Get import statistics for a user.
        
        Args:
            owner_id: Owner user ID
            
        Returns:
            Dict[str, Any]: Statistics dictionary
        """
        # Get counts by status
        status_counts = {}
        for status in ImportStatus:
            result = await self.session.execute(
                select(func.count(ImportJob.id)).where(
                    ImportJob.owner_id == owner_id,
                    ImportJob.status == status
                )
            )
            status_counts[status.value] = result.scalar() or 0
        
        # Get total contacts imported
        total_contacts_result = await self.session.execute(
            select(func.sum(ImportJob.rows_processed)).where(
                ImportJob.owner_id == owner_id,
                ImportJob.status == ImportStatus.SUCCESS
            )
        )
        total_contacts = total_contacts_result.scalar() or 0
        
        # Get recent activity (last 30 days)
        thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
        recent_jobs_result = await self.session.execute(
            select(func.count(ImportJob.id)).where(
                ImportJob.owner_id == owner_id,
                ImportJob.created_at >= thirty_days_ago
            )
        )
        recent_jobs = recent_jobs_result.scalar() or 0
        
        return {
            "total_jobs": sum(status_counts.values()),
            "status_counts": status_counts,
            "total_contacts_imported": total_contacts,
            "recent_jobs_30_days": recent_jobs
        }
    
    async def create_import_job(
        self,
        *,
        id: Optional[str] = None, # Accept pre-generated ID
        filename: str,
        file_size: int,
        sha256: str,
        owner_id: str,
        rows_total: int = 0
    ) -> ImportJob:
        """
        Create a new import job with generated ID.
        
        Args:
            filename: Original filename
            file_size: File size in bytes
            sha256: File SHA-256 hash
            owner_id: Owner user ID
            rows_total: Total rows to process
            
        Returns:
            ImportJob: Created import job
        """
        job_id = id or generate_prefixed_id(IDPrefix.BATCH)  # Resuing ID from the top layer.
        
        db_obj = ImportJob(
            id=job_id,
            filename=filename,
            file_size=file_size,
            sha256=sha256,
            owner_id=owner_id,
            rows_total=rows_total,
            rows_processed=0,
            status=ImportStatus.PROCESSING,
            started_at=datetime.now(timezone.utc),
            errors=[]
        )
        
        self.session.add(db_obj)
        
        logger.info(f"Created import job {job_id} for user {owner_id}")
        return db_obj
    
    async def bulk_update_status(
        self,
        job_ids: List[str],
        status: ImportStatus,
        completed_at: Optional[datetime] = None
    ) -> int:
        """
        Bulk update status for multiple import jobs.
        
        Args:
            job_ids: List of import job IDs
            status: New status
            completed_at: Optional completion timestamp
            
        Returns:
            int: Number of jobs updated
        """
        if not job_ids:
            return 0
        
        update_data = {
            "status": status,
            "updated_at": datetime.now(timezone.utc)
        }
        
        if completed_at:
            update_data["completed_at"] = completed_at
        
        result = await self.session.execute(
            update(ImportJob)
            .where(ImportJob.id.in_(job_ids))
            .values(**update_data)
        )
        
        updated_count = result.rowcount
        
        logger.info(f"Bulk updated {updated_count} import jobs to status {status.value}")
        return updated_count
</file>

<file path="app/services/imports/events.py">
# app/services/imports/events.py
"""
Production-ready import progress event schemas for CSV upload processing.

This module provides type-safe, versioned event schemas for real-time progress tracking
during CSV import operations. Designed for WebSocket streaming and background task updates.
"""
from typing import List, Optional, TypedDict, Union
from enum import Enum
from datetime import datetime

__all__ = ["ImportProgressV1", "ImportEventType", "ImportErrorV1", "ImportCompletedV1", "ImportFailedV1"]


class ImportEventType(str, Enum):
    """Import event types for real-time progress tracking."""
    PROGRESS = "progress"      # Incremental progress updates
    COMPLETED = "completed"    # Final success state
    FAILED = "failed"         # Final failure state
    CANCELLED = "cancelled"   # User-cancelled job


class ImportErrorV1(TypedDict):
    """
    Standardized error structure matching backend ImportError schema.
    Compatible with app.schemas.import_job.ImportError for consistency.
    """
    row: int                           # CSV row number (1-based)
    column: Optional[str]              # Column name where error occurred
    message: str                       # Human-readable error description
    value: Optional[str]               # The problematic value that caused error


class ImportProgressV1(TypedDict):
    """
    Core progress event payload for incremental updates during CSV processing.
    
    **Real-time Progress Features:**
    - Row-level processing statistics with percentages
    - Error sampling with structured details
    - Performance metrics for ETA calculation
    - Memory-efficient for high-frequency updates
    
    **Frontend Integration:**
    - Optimized for progress bars and live dashboards
    - Includes all data needed for UX without additional API calls
    - Rate-limited to max 1 update/second to prevent UI flooding
    
    **Backward Compatibility:**
    - Versioned schema (V1) allows future extensions without breaking changes
    - All fields are required for type safety and predictable behavior
    """
    type: ImportEventType              # Always "progress" for this event
    job_id: str                        # Import job identifier for tracking
    
    # Core processing metrics
    processed: int                     # Total rows processed (cumulative)
    successful: int                    # Successfully imported contacts (cumulative)
    total_rows: int                    # Total rows in CSV file (for progress calculation)
    percent: float                     # Completion percentage (0.00-100.00, 2 decimals)
    
    # Error tracking and sampling
    errors: List[ImportErrorV1]        # Recent errors (sampled, max 100 per backend config)
    error_count: int                   # Total error count (includes non-sampled errors)
    has_critical_errors: bool          # Whether processing should be stopped
    
    # Performance and UX enhancements
    estimated_completion: Optional[str] # Human-readable ETA ("~2 minutes", "~30 seconds")
    processing_rate: Optional[int]      # Rows processed per second (for performance monitoring)
    memory_usage_mb: Optional[float]    # Current memory usage for monitoring


class ImportCompletedV1(TypedDict):
    """
    Final completion event with comprehensive job statistics.
    
    **Success Metrics:**
    - Final counts and percentages for reporting
    - File integrity verification results
    - Performance benchmarks for optimization
    
    **Data Preservation:**
    - Links to created contacts for immediate access
    - Detailed error summary for quality assessment
    - Processing metadata for audit trails
    """
    type: ImportEventType              # Always "completed"
    job_id: str
    
    # Final statistics
    total_rows: int                    # Total rows in original CSV
    successful_contacts: int           # Contacts successfully created
    error_count: int                   # Total errors encountered
    final_status: str                  # "success" or "partial_success"
    
    # Quality metrics
    success_rate: float                # Percentage of successful imports (0.00-100.00)
    data_quality_score: float          # Overall data quality assessment (0.00-100.00)
    
    # Performance metrics
    total_processing_time: float       # Total seconds from start to completion
    average_processing_rate: float     # Average rows per second
    peak_memory_usage_mb: float        # Maximum memory used during processing
    
    # File verification
    sha256_verified: bool              # Whether file integrity was maintained
    detected_columns: dict             # Column mapping results for user feedback
    
    # Error summary (for detailed reporting)
    error_summary: List[ImportErrorV1] # Sample of errors (max 100 as per backend config)
    common_error_patterns: List[str]   # Most frequent error types for user guidance
    
    # Timestamps (ISO format for frontend parsing)
    started_at: str                    # Processing start time
    completed_at: str                  # Processing completion time


class ImportFailedV1(TypedDict):
    """
    Failure event with diagnostic information for error recovery.
    
    **Failure Analysis:**
    - Root cause identification for user guidance
    - Partial results preservation when possible
    - Recovery suggestions and next steps
    
    **Support Information:**
    - Detailed error context for debugging
    - System state at failure for troubleshooting
    - User-friendly error messages with actionable guidance
    """
    type: ImportEventType              # Always "failed"
    job_id: str
    
    # Failure classification
    failure_reason: str                # Primary failure cause ("validation_error", "system_error", etc.)
    user_message: str                  # User-friendly error description
    technical_details: Optional[str]   # Technical details for support (optional for security)
    
    # Partial progress preservation
    rows_processed_before_failure: int # Rows successfully processed before failure
    successful_contacts: int           # Contacts created before failure
    
    # Error context
    failure_point: dict               # Where exactly the failure occurred
    system_state: dict                # System metrics at failure time
    recovery_suggestions: List[str]   # Actionable steps for user
    
    # Support metadata
    error_id: Optional[str]           # Unique error identifier for support tickets
    support_context: Optional[dict]  # Additional context for customer support
    
    # Timestamps
    started_at: str
    failed_at: str


# Type union for all possible import events (useful for event handlers)
ImportEvent = Union[ImportProgressV1, ImportCompletedV1, ImportFailedV1]


# Helper functions for creating events with validation
def create_progress_event(
    job_id: str,
    processed: int,
    successful: int,
    total_rows: int,
    errors: List[ImportErrorV1],
    error_count: int,
    has_critical_errors: bool,
    estimated_completion: Optional[str] = None,
    processing_rate: Optional[int] = None,
    memory_usage_mb: Optional[float] = None,
) -> ImportProgressV1:
    """
    Create a validated progress event with computed fields.
    
    **Automatic Calculations:**
    - Progress percentage with proper rounding
    - Rate limiting for high-frequency updates
    - Memory usage monitoring
    
    Args:
        job_id: Import job identifier
        processed: Total rows processed
        successful: Successfully imported contacts
        total_rows: Total rows in CSV
        errors: Recent error samples
        error_count: Total error count
        has_critical_errors: Whether processing should stop
        estimated_completion: Optional ETA string
        processing_rate: Optional processing rate
        memory_usage_mb: Optional memory usage
    
    Returns:
        ImportProgressV1: Validated progress event
    """
    # Calculate progress percentage with proper bounds checking
    if total_rows <= 0:
        percent = 0.0
    else:
        percent = round((processed / total_rows) * 100, 2)
        percent = max(0.0, min(100.0, percent))  # Clamp to 0-100 range
    
    return ImportProgressV1(
        type=ImportEventType.PROGRESS,
        job_id=job_id,
        processed=processed,
        successful=successful,
        total_rows=total_rows,
        percent=percent,
        errors=errors,
        error_count=error_count,
        has_critical_errors=has_critical_errors,
        estimated_completion=estimated_completion,
        processing_rate=processing_rate,
        memory_usage_mb=memory_usage_mb,
    )


def create_completion_event(
    job_id: str,
    total_rows: int,
    successful_contacts: int,
    error_count: int,
    processing_time: float,
    average_rate: float,
    peak_memory: float,
    sha256_verified: bool,
    detected_columns: dict,
    error_summary: List[ImportErrorV1],
    started_at: datetime,
    completed_at: datetime,
) -> ImportCompletedV1:
    """
    Create a validated completion event with computed metrics.
    
    Args:
        job_id: Import job identifier
        total_rows: Total rows processed
        successful_contacts: Successful imports
        error_count: Total errors
        processing_time: Total processing time in seconds
        average_rate: Average processing rate
        peak_memory: Peak memory usage in MB
        sha256_verified: File integrity status
        detected_columns: Column detection results
        error_summary: Sample of errors
        started_at: Start timestamp
        completed_at: Completion timestamp
    
    Returns:
        ImportCompletedV1: Validated completion event
    """
    # Calculate success rate and quality score
    success_rate = (successful_contacts / total_rows * 100) if total_rows > 0 else 0.0
    data_quality_score = max(0.0, 100.0 - (error_count / total_rows * 100)) if total_rows > 0 else 100.0
    
    # Determine final status
    if error_count == 0:
        final_status = "success"
    elif successful_contacts > 0:
        final_status = "partial_success"
    else:
        final_status = "failed"
    
    # Extract common error patterns
    error_messages = [error["message"] for error in error_summary]
    common_patterns = list(set(error_messages))[:5]  # Top 5 unique error types
    
    return ImportCompletedV1(
        type=ImportEventType.COMPLETED,
        job_id=job_id,
        total_rows=total_rows,
        successful_contacts=successful_contacts,
        error_count=error_count,
        final_status=final_status,
        success_rate=round(success_rate, 2),
        data_quality_score=round(data_quality_score, 2),
        total_processing_time=processing_time,
        average_processing_rate=average_rate,
        peak_memory_usage_mb=peak_memory,
        sha256_verified=sha256_verified,
        detected_columns=detected_columns,
        error_summary=error_summary,
        common_error_patterns=common_patterns,
        started_at=started_at.isoformat(),
        completed_at=completed_at.isoformat(),
    )


def create_failure_event(
    job_id: str,
    failure_reason: str,
    user_message: str,
    rows_processed: int,
    successful_contacts: int,
    started_at: datetime,
    failed_at: datetime,
    technical_details: Optional[str] = None,
    error_id: Optional[str] = None,
) -> ImportFailedV1:
    """
    Create a validated failure event with diagnostic information.
    
    Args:
        job_id: Import job identifier
        failure_reason: Primary failure classification
        user_message: User-friendly error message
        rows_processed: Rows processed before failure
        successful_contacts: Contacts created before failure
        started_at: Processing start time
        failed_at: Failure occurrence time
        technical_details: Optional technical details
        error_id: Optional error identifier for support
    
    Returns:
        ImportFailedV1: Validated failure event
    """
    # Generate recovery suggestions based on failure reason
    recovery_suggestions = []
    if failure_reason == "validation_error":
        recovery_suggestions = [
            "Check your CSV file format and column headers",
            "Ensure phone numbers are in valid format",
            "Verify file encoding is UTF-8"
        ]
    elif failure_reason == "file_size_error":
        recovery_suggestions = [
            "Split your CSV into smaller files (max 100MB)",
            "Remove unnecessary columns to reduce file size"
        ]
    elif failure_reason == "system_error":
        recovery_suggestions = [
            "Try uploading again in a few minutes",
            "Contact support if the problem persists"
        ]
    
    return ImportFailedV1(
        type=ImportEventType.FAILED,
        job_id=job_id,
        failure_reason=failure_reason,
        user_message=user_message,
        technical_details=technical_details,
        rows_processed_before_failure=rows_processed,
        successful_contacts=successful_contacts,
        failure_point={"timestamp": failed_at.isoformat()},
        system_state={},  # Populated by system monitoring
        recovery_suggestions=recovery_suggestions,
        error_id=error_id,
        support_context=None,  # Populated if needed for support
        started_at=started_at.isoformat(),
        failed_at=failed_at.isoformat(),
    )
</file>

<file path="app/services/imports/parser.py">
"""
Enhanced Streaming CSV parser service for import pipeline.

This module provides memory-efficient CSV parsing with intelligent column detection,
real-time progress tracking, error handling, and bulk database operations for contact imports.

PRODUCTION ENHANCEMENTS:
- Intelligent multi-column phone detection with confidence scoring
- Advanced name detection using linguistic patterns
- User feedback for low-confidence detections
- Enhanced error handling and recovery suggestions
- Optimized progress events with new ImportProgressV1 schema
"""
import csv
import hashlib
import logging
import asyncio
import re
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from pathlib import Path

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.exc import IntegrityError, SQLAlchemyError
from sqlalchemy import select, func

from app.models.import_job import ImportJob, ImportStatus
from app.models.contact import Contact
from app.schemas.contact import ContactCreate
from app.schemas.import_job import ImportError
from app.core.exceptions import ValidationError
from app.utils.phone import validate_phone
from app.db.session import get_repository_context
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.services.imports.events import (
    ImportProgressV1, ImportEventType, ImportErrorV1, 
    create_progress_event, create_completion_event, create_failure_event
)

logger = logging.getLogger("inboxerr.parser")

# Enhanced regex patterns for production
_PHONE_HEADER_RGX = re.compile(
    r"(phone[\s\d\w]*|mobile[\s\d\w]*|cell[\s\d\w]*|tel[\s\d\w]*|"
    r"contact[\s\d\w]*|whatsapp|number[\s\d]*|ph\s*\d*|mob\s*\d*|"
    r"telephone|fone|fon)", 
    re.I
)
_NAME_HEADER_RGX = re.compile(r"(name|contact|person|client|customer|full_name|first_name|last_name)", re.I)
_EMAIL_HEADER_RGX = re.compile(r"(email|mail|e-mail)", re.I)


def _enhanced_phone_column_score(header: str, samples: list[str]) -> float:
    """
    Enhanced phone column scoring with better edge case handling.
    
    Scoring breakdown:
    - Header match: 20 points
    - Data validity: 80 points (with proper empty cell handling)
    - Penalty for mostly empty columns
    
    Args:
        header: Column header name
        samples: Sample values from the column
        
    Returns:
        float: Score from 0-100 indicating how likely this is a phone column
    """
    # Header bonus for phone-related terms
    header_bonus = 20 if _PHONE_HEADER_RGX.search(header) else 0
    
    if not samples:
        return header_bonus * 0.1  # Heavy penalty for completely empty columns
    
    valid_phones = 0
    total_non_empty = 0
    suspicious_patterns = 0
    
    for raw in samples:
        raw = raw.strip()
        if not raw:
            continue  # Skip empty cells entirely
            
        total_non_empty += 1
        
        # Quick rejection for obviously non-phone data
        if len(raw) > 25:  # Too long to be a phone number
            suspicious_patterns += 1
            continue
            
        if len(raw) < 5:  # Too short to be a valid phone
            suspicious_patterns += 1
            continue
            
        # Check for non-phone patterns (emails, URLs, etc.)
        if '@' in raw or 'http' in raw.lower() or raw.isalpha():
            suspicious_patterns += 1
            continue
            
        # Use production phone validation
        is_valid, *_ = validate_phone(raw)
        if is_valid:
            valid_phones += 1
    
    # Avoid division by zero and handle edge cases
    if total_non_empty == 0:
        return header_bonus * 0.1  # Heavily penalize empty columns
    
    # Calculate data score with penalties
    validity_ratio = valid_phones / total_non_empty
    suspicious_ratio = suspicious_patterns / total_non_empty
    
    # Penalize columns with too many suspicious patterns
    if suspicious_ratio > 0.5:
        validity_ratio *= 0.3  # Heavy penalty for mostly non-phone data
    
    data_score = validity_ratio * 80
    
    return header_bonus + data_score


def _enhanced_name_column_score(header: str, samples: list[str]) -> float:
    """
    Enhanced name column detection using linguistic patterns.
    
    Scoring factors:
    - Header match: 30 points
    - Average length (3-50 chars): 20 points  
    - Contains spaces (first/last names): 15 points
    - Mostly alphabetic: 25 points
    - Reasonable name patterns: 10 points
    
    Args:
        header: Column header name
        samples: Sample values from the column
        
    Returns:
        float: Score from 0-100 indicating how likely this is a name column
    """
    score = 0
    
    # Header bonus for name-related terms
    if _NAME_HEADER_RGX.search(header):
        score += 30
    
    if not samples:
        return score * 0.1
    
    # Filter out empty values
    non_empty_values = [v.strip() for v in samples if v and v.strip()]
    
    if not non_empty_values:
        return score * 0.1
    
    # Analyze name-like characteristics
    total_values = len(non_empty_values)
    
    # 1. Average length check (reasonable name length)
    avg_length = sum(len(v) for v in non_empty_values) / total_values
    if 3 <= avg_length <= 50:
        score += 20
    elif avg_length > 50:  # Probably not names if too long
        score -= 10
    
    # 2. Space analysis (first/last name patterns)
    has_spaces = sum(1 for v in non_empty_values if ' ' in v) / total_values
    if has_spaces > 0.3:  # 30%+ have spaces
        score += 15
    elif has_spaces > 0.1:  # Some spaces
        score += 8
    
    # 3. Alphabetic content analysis
    mostly_alpha = sum(1 for v in non_empty_values 
                      if re.match(r"^[a-zA-Z\s\'-\.]+$", v)) / total_values
    if mostly_alpha > 0.7:  # 70%+ alphabetic
        score += 25
    elif mostly_alpha > 0.5:  # 50%+ alphabetic
        score += 15
    
    # 4. Common name patterns
    common_name_patterns = 0
    for value in non_empty_values[:100]:  # Sample first 100 for performance
        # Title case pattern (John Smith)
        if value.istitle():
            common_name_patterns += 1
        # All caps might be names too (JOHN SMITH)
        elif value.isupper() and len(value.split()) <= 3:
            common_name_patterns += 1
    
    if common_name_patterns > 0:
        pattern_ratio = common_name_patterns / min(len(non_empty_values), 100)
        if pattern_ratio > 0.3:
            score += 10
        elif pattern_ratio > 0.1:
            score += 5
    
    # Penalty for obvious non-name patterns
    non_name_patterns = sum(1 for v in non_empty_values[:50] 
                           if '@' in v or v.isdigit() or len(v) < 2)
    if non_name_patterns > 0:
        penalty_ratio = non_name_patterns / min(len(non_empty_values), 50)
        score -= penalty_ratio * 30
    
    return max(0, score)  # Don't return negative scores


class CSVParserConfig:
    """Enhanced configuration for CSV parser behavior."""
    
    # Memory and performance limits
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
    MAX_ROWS = 1_000_000  # 1M rows max
    BULK_INSERT_SIZE = 1000  # Process in chunks of 1000 rows
    
    # CSV parsing settings
    ALLOWED_DELIMITERS = [',', '\t', '|', ';']
    ENCODING_FALLBACKS = ['utf-8', 'latin1', 'cp1252']
    
    # Column detection settings
    COLUMN_SAMPLE_SIZE = 1000  # Rows to sample for column detection
    MIN_PHONE_CONFIDENCE = 30  # Minimum confidence for phone column
    MIN_NAME_CONFIDENCE = 20   # Minimum confidence for name column
    MAX_PHONE_CANDIDATES = 3   # Maximum phone columns to consider
    
    # Error limits
    MAX_ERRORS_PER_JOB = 10000  # Stop processing if too many errors
    ERROR_SAMPLE_SIZE = 100     # Only store first 100 errors
    
    # Progress reporting
    PROGRESS_UPDATE_INTERVAL = 0.5  # Seconds between progress updates


class ColumnDetectionResult:
    """Enhanced result object for column detection with confidence metrics."""
    
    def __init__(self):
        self.primary_phone_column: Optional[str] = None
        self.phone_candidates: List[Dict[str, Any]] = []
        self.name_column: Optional[str] = None
        self.detected_columns: Dict[str, Any] = {}
        self.confidence_scores: Dict[str, float] = {}
        self.detection_quality: str = "unknown"  # high, medium, low
        self.user_guidance: List[str] = []
        
    @property
    def phone_confidence(self) -> float:
        """Get confidence score for primary phone column."""
        return self.confidence_scores.get("phone", 0.0)
    
    @property
    def name_confidence(self) -> float:
        """Get confidence score for name column."""
        return self.confidence_scores.get("name", 0.0)
    
    @property
    def needs_manual_review(self) -> bool:
        """Check if detection confidence is too low for automatic processing."""
        return self.phone_confidence < CSVParserConfig.MIN_PHONE_CONFIDENCE


class CSVParseResult:
    """Enhanced result object for CSV parsing operations."""
    
    def __init__(self):
        self.total_rows = 0
        self.processed_rows = 0
        self.successful_contacts = 0
        self.errors: List[ImportError] = []
        self.status = ImportStatus.PROCESSING
        self.sha256_hash = ""
        self.column_detection: ColumnDetectionResult = ColumnDetectionResult()
        
        # Performance tracking
        self.start_time: Optional[datetime] = None
        self.processing_rate = 0  # rows per second
        self.memory_usage_mb = 0.0
        
    @property
    def progress_percentage(self) -> float:
        """Calculate progress percentage with bounds checking."""
        if self.total_rows == 0:
            return 0.0
        return round(min(100.0, max(0.0, (self.processed_rows / self.total_rows) * 100)), 2)
    
    @property
    def error_count(self) -> int:
        """Get total error count."""
        return len(self.errors)
    
    @property
    def has_critical_errors(self) -> bool:
        """Check if there are too many errors to continue."""
        return self.error_count >= CSVParserConfig.MAX_ERRORS_PER_JOB
    
    @property
    def estimated_completion_time(self) -> str:
        """Calculate estimated completion time based on current processing rate."""
        if self.processing_rate <= 0 or self.processed_rows >= self.total_rows:
            return "Calculating..."
        
        remaining_rows = self.total_rows - self.processed_rows
        estimated_seconds = remaining_rows / self.processing_rate
        
        if estimated_seconds < 60:
            return f"~{int(estimated_seconds)} seconds"
        elif estimated_seconds < 3600:
            return f"~{int(estimated_seconds / 60)} minutes"
        else:
            return f"~{int(estimated_seconds / 3600)} hours"


class StreamingCSVParser:
    """
    Enhanced memory-efficient streaming CSV parser for contact imports.
    
    This parser processes large CSV files in chunks while maintaining constant
    memory usage and providing intelligent column detection with confidence scoring.
    """
    
    def __init__(self, session: AsyncSession):
        """
        Initialize the enhanced CSV parser.
        
        Args:
            session: Async database session for operations
        """
        self.session = session
        self.config = CSVParserConfig()
        self._last_progress_update = 0
        
    async def parse_file(
        self,
        file_path: Path,
        import_job_id: str,
        progress_callback: Optional[callable] = None
    ) -> CSVParseResult:
        """
        Parse a CSV file and import contacts to database with enhanced column detection.
        
        Args:
            file_path: Path to the CSV file to parse
            import_job_id: ID of the import job tracking this operation
            progress_callback: Optional callback for progress updates
            
        Returns:
            CSVParseResult: Enhanced results of the parsing operation
            
        Raises:
            ValidationError: If file validation fails
            FileNotFoundError: If file doesn't exist
            PermissionError: If file can't be read
        """
        logger.info(f"Starting enhanced CSV parse for import job {import_job_id}")
        result = CSVParseResult()
        result.start_time = datetime.now(timezone.utc)
        
        try:
            # Phase 1: File validation and setup
            await self._validate_file(file_path)
            result.sha256_hash = await self._calculate_file_hash(file_path)
            
            # Update import job with initial status
            await self._update_import_job(import_job_id, {
                'status': ImportStatus.PROCESSING,
                'sha256': result.sha256_hash,
                'started_at': result.start_time
            })
            
            # Phase 2: File format detection
            encoding, delimiter = await self._detect_file_format(file_path)
            logger.info(f"Detected encoding: {encoding}, delimiter: '{delimiter}'")
            
            # Phase 3: Enhanced column detection
            headers = await self._parse_headers(file_path, encoding, delimiter)
            result.column_detection = await self._enhanced_column_detection(
                file_path, encoding, delimiter, headers
            )
            
            # Check if manual review is needed
            if result.column_detection.needs_manual_review:
                logger.warning(f"Low confidence column detection for job {import_job_id}")
                # You might want to pause here and ask for user input in production
            
            # Phase 4: Row counting and validation
            result.total_rows = await self._count_csv_rows(file_path, encoding)
            logger.info(f"Total rows to process: {result.total_rows}")
            
            if result.total_rows > self.config.MAX_ROWS:
                raise ValidationError(
                    f"File has {result.total_rows:,} rows, maximum allowed is {self.config.MAX_ROWS:,}"
                )
            
            # Update import job with detection results
            await self._update_import_job(import_job_id, {
                'rows_total': result.total_rows,
                'detected_columns': result.column_detection.detected_columns
            })
            
            # Phase 5: Enhanced streaming processing
            async for chunk_result in self._enhanced_process_csv_chunks(
                file_path, encoding, delimiter, import_job_id, result
            ):
                result.processed_rows += chunk_result['processed']
                result.successful_contacts += chunk_result['successful']
                result.errors.extend(chunk_result['errors'])
                result.processing_rate = chunk_result.get('processing_rate', 0)
                result.memory_usage_mb = chunk_result.get('memory_usage_mb', 0.0)
                
                # Enhanced progress reporting with rate limiting
                current_time = datetime.now(timezone.utc).timestamp()
                if (current_time - self._last_progress_update) >= self.config.PROGRESS_UPDATE_INTERVAL:
                    
                    # Update database
                    await self._update_import_job(import_job_id, {
                        'rows_processed': result.processed_rows,
                        'errors': [error.dict() for error in result.errors[:self.config.ERROR_SAMPLE_SIZE]]
                    })
                    
                    # Enhanced progress callback
                    if progress_callback:
                        # Convert ImportError objects to ImportErrorV1 format
                        error_events = [
                            ImportErrorV1(
                                row=error.row,
                                column=error.column,
                                message=error.message,
                                value=error.value
                            ) for error in chunk_result["errors"]
                        ]
                        
                        progress_event = create_progress_event(
                            job_id=import_job_id,
                            processed=result.processed_rows,
                            successful=result.successful_contacts,
                            total_rows=result.total_rows,
                            errors=error_events,
                            error_count=result.error_count,
                            has_critical_errors=result.has_critical_errors,
                            estimated_completion=result.estimated_completion_time,
                            processing_rate=int(result.processing_rate),
                            memory_usage_mb=result.memory_usage_mb
                        )
                        
                        await progress_callback(progress_event)
                    
                    self._last_progress_update = current_time
                
                # Check for critical errors
                if result.has_critical_errors:
                    logger.warning(f"Too many errors ({result.error_count}), stopping import")
                    result.status = ImportStatus.FAILED
                    break
            
            # Phase 6: Final status determination
            if result.status == ImportStatus.PROCESSING:
                if result.error_count == 0:
                    result.status = ImportStatus.SUCCESS
                elif result.successful_contacts > 0:
                    result.status = ImportStatus.SUCCESS  # Partial success
                else:
                    result.status = ImportStatus.FAILED
            
            # Final import job update
            end_time = datetime.now(timezone.utc)
            total_time = (end_time - result.start_time).total_seconds()
            
            await self._update_import_job(import_job_id, {
                'status': result.status,
                'rows_processed': result.processed_rows,
                'completed_at': end_time,
                'errors': [error.dict() for error in result.errors[:self.config.ERROR_SAMPLE_SIZE]]
            })
            
            logger.info(
                f"Enhanced CSV parse complete for job {import_job_id}: "
                f"{result.successful_contacts} contacts, {result.error_count} errors, "
                f"{total_time:.1f}s total time"
            )
            
        except Exception as e:
            logger.error(f"Enhanced CSV parse failed for job {import_job_id}: {str(e)}")
            result.status = ImportStatus.FAILED
            
            # Update import job with failure
            await self._update_import_job(import_job_id, {
                'status': ImportStatus.FAILED,
                'completed_at': datetime.now(timezone.utc),
                'errors': [{'row': 0, 'column': None, 'message': str(e), 'value': None}]
            })
            
            raise
        
        return result
    

    async def parse_file_with_mapping(
    self,
    file_path: Path,
    import_job_id: str,
    mapping_config: Dict[str, Any],
    progress_callback: Optional[callable] = None
) -> CSVParseResult:
        """
        Parse CSV file with explicit user-provided column mapping.
        
        This method bypasses auto-detection and uses the exact columns specified by the user.
        
        Args:
            file_path: Path to the CSV file to parse
            import_job_id: ID of the import job tracking this operation
            mapping_config: User-provided column mapping configuration
            progress_callback: Optional callback for progress updates
            
        Returns:
            CSVParseResult: Results of the parsing operation
        """
        logger.info(f"Starting mapped CSV parse for import job {import_job_id}")
        result = CSVParseResult()
        result.start_time = datetime.now(timezone.utc)
        
        try:
            # Phase 1: File validation and setup (same as before)
            await self._validate_file(file_path)
            result.sha256_hash = await self._calculate_file_hash(file_path)
            
            await self._update_import_job(import_job_id, {
                'status': ImportStatus.PROCESSING,
                'sha256': result.sha256_hash,
                'started_at': result.start_time
            })
            
            # Phase 2: File format detection
            encoding, delimiter = await self._detect_file_format(file_path)
            logger.info(f"Detected encoding: {encoding}, delimiter: '{delimiter}'")
            
            # Phase 3: Parse headers and validate mapping
            headers = await self._parse_headers(file_path, encoding, delimiter)
            
            # Validate that all mapped columns exist
            all_mapped_columns = (
                mapping_config.get('phone_columns', []) +
                [mapping_config.get('name_column')] +
                mapping_config.get('skip_columns', []) +
                mapping_config.get('tag_columns', [])
            )
            
            for col in all_mapped_columns:
                if col and col not in headers:
                    raise ValidationError(f"Column '{col}' not found in CSV. Available columns: {', '.join(headers)}")
            
            # Phase 4: Create synthetic detection result from mapping
            result.column_detection = self._create_detection_from_mapping(mapping_config, headers)
            
            # Phase 5: Count rows
            result.total_rows = await self._count_csv_rows(file_path, encoding)
            logger.info(f"Total rows to process: {result.total_rows}")
            
            if result.total_rows > self.config.MAX_ROWS:
                raise ValidationError(
                    f"File has {result.total_rows:,} rows, maximum allowed is {self.config.MAX_ROWS:,}"
                )
            
            # Phase 6: Process with mapped columns
            async for chunk_result in self._process_csv_chunks_with_mapping(
                file_path, encoding, delimiter, import_job_id, mapping_config, result
            ):
                result.processed_rows += chunk_result['processed']
                result.successful_contacts += chunk_result['successful']
                result.errors.extend(chunk_result['errors'])
                result.processing_rate = chunk_result.get('processing_rate', 0)
                result.memory_usage_mb = chunk_result.get('memory_usage_mb', 0.0)
                
                # Progress reporting (same as before)
                current_time = datetime.now(timezone.utc).timestamp()
                if (current_time - self._last_progress_update) >= self.config.PROGRESS_UPDATE_INTERVAL:
                    await self._update_import_job(import_job_id, {
                        'rows_processed': result.processed_rows,
                        'errors': [error.dict() for error in result.errors[:self.config.ERROR_SAMPLE_SIZE]]
                    })
                    
                    if progress_callback:
                        # Same progress callback logic as before
                        pass
                    
                    self._last_progress_update = current_time
                
                if result.has_critical_errors:
                    logger.warning(f"Too many errors ({result.error_count}), stopping import")
                    result.status = ImportStatus.FAILED
                    break
            
            # Phase 7: Final status determination (same as before)
            if result.status == ImportStatus.PROCESSING:
                if result.error_count == 0:
                    result.status = ImportStatus.SUCCESS
                elif result.successful_contacts > 0:
                    result.status = ImportStatus.SUCCESS
                else:
                    result.status = ImportStatus.FAILED
            
            # Final update
            end_time = datetime.now(timezone.utc)
            total_time = (end_time - result.start_time).total_seconds()
            
            await self._update_import_job(import_job_id, {
                'status': result.status,
                'rows_processed': result.processed_rows,
                'completed_at': end_time,
                'errors': [error.dict() for error in result.errors[:self.config.ERROR_SAMPLE_SIZE]]
            })
            
            logger.info(
                f"Mapped CSV parse complete for job {import_job_id}: "
                f"{result.successful_contacts} contacts, {result.error_count} errors, "
                f"{total_time:.1f}s total time"
            )
            
        except Exception as e:
            logger.error(f"Mapped CSV parse failed for job {import_job_id}: {str(e)}")
            result.status = ImportStatus.FAILED
            
            await self._update_import_job(import_job_id, {
                'status': ImportStatus.FAILED,
                'completed_at': datetime.now(timezone.utc),
                'errors': [{'row': 0, 'column': None, 'message': str(e), 'value': None}]
            })
            
            raise
        
        return result
    
    async def _enhanced_column_detection(
        self,
        file_path: Path,
        encoding: str,
        delimiter: str,
        headers: List[str]
    ) -> ColumnDetectionResult:
        """
        Enhanced column detection with confidence scoring and user guidance.
        
        Args:
            file_path: Path to CSV file
            encoding: File encoding
            delimiter: CSV delimiter
            headers: Parsed headers
            
        Returns:
            ColumnDetectionResult: Comprehensive detection results with confidence scores
        """
        logger.info("Starting enhanced column detection")
        result = ColumnDetectionResult()
        
        # Sample data for analysis
        buckets = [[] for _ in headers]
        
        with open(file_path, "r", encoding=encoding) as f:
            reader = csv.reader(f, delimiter=delimiter)
            next(reader)  # Skip header
            
            for i, row in enumerate(reader, start=1):
                if i > self.config.COLUMN_SAMPLE_SIZE:
                    break
                for idx, cell in enumerate(row):
                    if idx < len(buckets):  # Safety check
                        buckets[idx].append(cell)
        
        # Enhanced phone column detection
        phone_scores = {}
        for i, header in enumerate(headers):
            score = _enhanced_phone_column_score(header, buckets[i])
            phone_scores[header] = score
            logger.debug(f"Phone score for '{header}': {score:.1f}")
        
        # Sort phone candidates by score
        phone_candidates = sorted(phone_scores.items(), key=lambda x: x[1], reverse=True)
        result.phone_candidates = [
            {"column": col, "score": score, "confidence": self._score_to_confidence(score)}
            for col, score in phone_candidates[:self.config.MAX_PHONE_CANDIDATES]
            if score > 0
        ]
        
        # Select primary phone column
        if phone_candidates and phone_candidates[0][1] >= self.config.MIN_PHONE_CONFIDENCE:
            result.primary_phone_column = phone_candidates[0][0]
            result.confidence_scores["phone"] = phone_candidates[0][1]
        
        # Enhanced name column detection
        name_scores = {}
        for i, header in enumerate(headers):
            score = _enhanced_name_column_score(header, buckets[i])
            name_scores[header] = score
            logger.debug(f"Name score for '{header}': {score:.1f}")
        
        # Select best name column
        best_name_candidate = max(name_scores.items(), key=lambda x: x[1], default=(None, 0))
        if best_name_candidate[1] >= self.config.MIN_NAME_CONFIDENCE:
            result.name_column = best_name_candidate[0]
            result.confidence_scores["name"] = best_name_candidate[1]
        
        # Build detected columns mapping (backwards compatibility)
        phone_cols = [col for col, score in phone_candidates if score > 0]
        result.detected_columns = {
            "phones": phone_cols,
            "primary_phone": result.primary_phone_column,
            "name": result.name_column,
            "confidence_scores": result.confidence_scores
        }
        
        # Determine detection quality and generate user guidance
        result.detection_quality = self._assess_detection_quality(result)
        result.user_guidance = self._generate_user_guidance(result, headers)
        
        logger.info(f"Column detection complete: {result.detection_quality} confidence")
        logger.info(f"Primary phone: {result.primary_phone_column} ({result.phone_confidence:.1f})")
        logger.info(f"Name column: {result.name_column} ({result.name_confidence:.1f})")
        
        return result
    
    def _score_to_confidence(self, score: float) -> str:
        """Convert numeric score to confidence level."""
        if score >= 80:
            return "high"
        elif score >= 50:
            return "medium"
        elif score >= 20:
            return "low"
        else:
            return "very_low"
    
    def _assess_detection_quality(self, detection: ColumnDetectionResult) -> str:
        """Assess overall detection quality for user feedback."""
        phone_conf = detection.phone_confidence
        
        if phone_conf >= 80:
            return "high"
        elif phone_conf >= 50:
            return "medium"
        elif phone_conf >= 20:
            return "low"
        else:
            return "very_low"
    
    def _generate_user_guidance(self, detection: ColumnDetectionResult, headers: List[str]) -> List[str]:
        """Generate user guidance based on detection results."""
        guidance = []
        
        if detection.detection_quality == "very_low":
            guidance.append("❌ Could not reliably detect phone number column. Please verify your CSV format.")
            guidance.append("📋 Available columns: " + ", ".join(headers))
            
        elif detection.detection_quality == "low":
            guidance.append("⚠️ Low confidence in phone number detection. Please review the selected column.")
            if detection.primary_phone_column:
                guidance.append(f"📱 Detected phone column: '{detection.primary_phone_column}'")
                
        elif detection.detection_quality == "medium":
            guidance.append("✅ Phone column detected with medium confidence.")
            if detection.primary_phone_column:
                guidance.append(f"📱 Using column: '{detection.primary_phone_column}'")
                
        else:  # high confidence
            guidance.append("✅ Phone column detected with high confidence.")
            if detection.primary_phone_column:
                guidance.append(f"📱 Using column: '{detection.primary_phone_column}'")
        
        # Name column guidance
        if not detection.name_column:
            guidance.append("ℹ️ No name column detected. Contacts will be created with phone numbers only.")
        else:
            guidance.append(f"👤 Name column: '{detection.name_column}'")
        
        # Additional candidates
        if len(detection.phone_candidates) > 1:
            other_candidates = [c["column"] for c in detection.phone_candidates[1:3]]
            guidance.append(f"🔄 Alternative phone columns: {', '.join(other_candidates)}")
        
        return guidance
    
    async def _enhanced_process_csv_chunks(
        self,
        file_path: Path,
        encoding: str,
        delimiter: str,
        import_job_id: str,
        result: CSVParseResult
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Enhanced CSV processing with performance monitoring."""
        
        with open(file_path, 'r', encoding=encoding) as f:
            reader = csv.DictReader(f, delimiter=delimiter)
            
            chunk_contacts = []
            chunk_errors = []
            row_number = 1
            chunk_start_time = datetime.now(timezone.utc)
            
            for row in reader:
                row_number += 1
                
                try:
                    # Parse contact using enhanced detection results
                    contact_data = await self._enhanced_parse_contact_row(
                        row, row_number, result.column_detection, import_job_id
                    )
                    
                    if contact_data:
                        chunk_contacts.append(contact_data)
                    
                except ValidationError as e:
                    chunk_errors.append(ImportError(
                        row=row_number,
                        column=None,
                        message=str(e),
                        value=str(row)
                    ))
                
                # Process chunk when full
                if (row_number - 1) % self.config.BULK_INSERT_SIZE == 0:
                    successful = await self._bulk_insert_contacts(chunk_contacts)
                    
                    # Calculate performance metrics
                    chunk_end_time = datetime.now(timezone.utc)
                    chunk_duration = (chunk_end_time - chunk_start_time).total_seconds()
                    processing_rate = self.config.BULK_INSERT_SIZE / chunk_duration if chunk_duration > 0 else 0
                    
                    yield {
                        'processed': self.config.BULK_INSERT_SIZE,
                        'successful': successful,
                        'errors': chunk_errors,
                        'processing_rate': processing_rate,
                        'memory_usage_mb': self._get_memory_usage_mb()
                    }
                    
                    # Reset for next chunk
                    chunk_contacts.clear()
                    chunk_errors.clear()
                    chunk_start_time = chunk_end_time
                    
                    # Brief pause to prevent database overwhelming
                    await asyncio.sleep(0.01)
            
            # Process final chunk
            if chunk_contacts or chunk_errors:
                successful = await self._bulk_insert_contacts(chunk_contacts)
                remainder = (row_number - 1) % self.config.BULK_INSERT_SIZE
                
                chunk_end_time = datetime.now(timezone.utc)
                chunk_duration = (chunk_end_time - chunk_start_time).total_seconds()
                processing_rate = remainder / chunk_duration if chunk_duration > 0 else 0
                
                yield {
                    'processed': remainder or self.config.BULK_INSERT_SIZE,
                    'successful': successful,
                    'errors': chunk_errors,
                    'processing_rate': processing_rate,
                    'memory_usage_mb': self._get_memory_usage_mb()
                }
    
    async def _enhanced_parse_contact_row(
        self,
        row: Dict[str, str],
        row_number: int,
        detection: ColumnDetectionResult,
        import_job_id: str
    ) -> Optional[Contact]:
        """Enhanced contact parsing using detection results."""
        
        # Extract phone number using enhanced detection
        phone_raw: Optional[str] = None
        
        # Try primary phone column first
        if detection.primary_phone_column and detection.primary_phone_column in row:
            phone_raw = row[detection.primary_phone_column].strip()
        
        # Fallback to other phone candidates if primary is empty
        if not phone_raw:
            for candidate in detection.phone_candidates:
                col = candidate["column"]
                if col in row:
                    val = row[col].strip()
                    if val:
                        phone_raw = val
                        break
        
        if not phone_raw:
            raise ValidationError("No phone number found in detected phone columns")
        
        # Enhanced phone validation with better error messages
        try:
            is_valid, phone_normalized, error, metadata = validate_phone(phone_raw)
            if not is_valid:
                raise ValidationError(f"Invalid phone number '{phone_raw}': {error or 'Unknown validation error'}")
        except Exception as e:
            raise ValidationError(f"Phone number validation failed for '{phone_raw}': {str(e)}")
        
        # Extract name using enhanced detection
        name = None
        if detection.name_column and detection.name_column in row:
            name_raw = row[detection.name_column].strip()
            if name_raw and len(name_raw) <= 100:  # Reasonable name length limit
                name = name_raw
        
        # Enhanced tag extraction (exclude detected columns)
        tags = []
        exclude_cols = set()
        if detection.primary_phone_column:
            exclude_cols.add(detection.primary_phone_column)
        if detection.name_column:
            exclude_cols.add(detection.name_column)
        
        for key, value in row.items():
            if key not in exclude_cols and value and value.strip():
                # Clean and limit tag length
                clean_value = value.strip()[:50]  # Limit tag value length
                tags.append(f"{key}:{clean_value}")
        
        # Create enhanced contact object
        contact = Contact(
            import_id=import_job_id,
            phone=phone_normalized,
            name=name,
            tags=tags[:20],  # Limit number of tags
            csv_row_number=row_number,
            raw_data=dict(row)  # Store original row data
        )
        
        return contact
    
    def _get_memory_usage_mb(self) -> float:
        """Get current memory usage in MB for monitoring."""
        try:
            import psutil
            import os
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024  # Convert bytes to MB
        except ImportError:
            return 0.0  # psutil not available
        except Exception:
            return 0.0  # Error getting memory info
    
    # Existing methods with minimal changes for compatibility
    async def _validate_file(self, file_path: Path) -> None:
        """Validate file exists and meets size requirements."""
        if not file_path.exists():
            raise FileNotFoundError(f"CSV file not found: {file_path}")
        
        file_size = file_path.stat().st_size
        if file_size > self.config.MAX_FILE_SIZE:
            raise ValidationError(
                f"File size {file_size:,} bytes exceeds maximum allowed size {self.config.MAX_FILE_SIZE:,} bytes"
            )
        
        if file_size == 0:
            raise ValidationError("CSV file is empty")
    
    async def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA-256 hash of file for integrity checking."""
        hash_sha256 = hashlib.sha256()
        
        # Read file in chunks to handle large files
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        
        return hash_sha256.hexdigest()
    
    async def _detect_file_format(self, file_path: Path) -> Tuple[str, str]:
        """Detect file encoding and CSV delimiter."""
        # Try different encodings
        for encoding in self.config.ENCODING_FALLBACKS:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    # Read first few lines to detect delimiter
                    sample = f.read(8192)
                    
                    # Use csv.Sniffer to detect delimiter
                    sniffer = csv.Sniffer()
                    delimiter = sniffer.sniff(sample, delimiters=''.join(self.config.ALLOWED_DELIMITERS)).delimiter
                    
                    return encoding, delimiter
                    
            except (UnicodeDecodeError, csv.Error):
                continue
        
        # Fallback to defaults if detection fails
        logger.warning("Could not detect file format, using defaults")
        return 'utf-8', ','
    
    async def _parse_headers(self, file_path: Path, encoding: str, delimiter: str) -> List[str]:
        """Parse CSV headers and normalize column names."""
        with open(file_path, 'r', encoding=encoding) as f:
            reader = csv.reader(f, delimiter=delimiter)
            headers = next(reader)
            
            # Remove BOM if present
            cleaned_headers = []
            for i, header in enumerate(headers):
                clean_header = header.strip()
                if i == 0 and clean_header.startswith('\ufeff'):  # Remove BOM
                    clean_header = clean_header[1:]
                if clean_header:  # Only add non-empty headers
                    cleaned_headers.append(clean_header)
            
            logger.debug(f"Detected CSV headers: {cleaned_headers}")
            return cleaned_headers
    
    async def _count_csv_rows(self, file_path: Path, encoding: str) -> int:
        """Count total rows in CSV file efficiently."""
        with open(file_path, 'r', encoding=encoding) as f:
            # Skip header row
            next(f)
            row_count = sum(1 for _ in f)
        
        return row_count
    
    async def _bulk_insert_contacts(self, contacts: List[Contact]) -> int:
        """
        Enhanced bulk insert with better error handling and performance monitoring.
        
        Args:
            contacts: List of Contact objects to insert
            
        Returns:
            int: Number of contacts successfully inserted (excludes duplicates)
        """
        if not contacts:
            return 0
        
        insert_start_time = datetime.now(timezone.utc)
        
        try:
            # Use PostgreSQL's efficient UPSERT with VALUES clause
            from sqlalchemy.dialects.postgresql import insert
            from sqlalchemy import select
            
            # Prepare contact data for bulk operation
            contact_values = []
            for contact in contacts:
                # Generate ID if not set
                if not contact.id:
                    contact.id = generate_prefixed_id(IDPrefix.CONTACT)
                
                contact_values.append({
                    'id': contact.id,
                    'import_id': contact.import_id,
                    'phone': contact.phone,
                    'name': contact.name,
                    'tags': contact.tags,
                    'csv_row_number': contact.csv_row_number,
                    'raw_data': contact.raw_data,
                })
            
            # Single UPSERT operation - PostgreSQL optimized
            insert_stmt = insert(Contact.__table__)
            upsert_stmt = insert_stmt.on_conflict_do_nothing(
                index_elements=['import_id', 'phone']  # Uses existing unique constraint
            )
            
            # Execute bulk upsert
            result = await self.session.execute(upsert_stmt, contact_values)
            
            # Count successful insertions
            count_query = select(func.count(Contact.id)).where(
                Contact.id.in_([contact['id'] for contact in contact_values])
            )
            count_result = await self.session.execute(count_query)
            successful_count = count_result.scalar() or 0
            
            # Performance logging
            insert_duration = (datetime.now(timezone.utc) - insert_start_time).total_seconds()
            rate = len(contacts) / insert_duration if insert_duration > 0 else 0
            
            duplicate_count = len(contacts) - successful_count
            if duplicate_count > 0:
                logger.debug(
                    f"Bulk inserted {successful_count}/{len(contacts)} contacts "
                    f"({duplicate_count} duplicates skipped) in {insert_duration:.2f}s ({rate:.1f} contacts/sec)"
                )
            else:
                logger.debug(
                    f"Bulk inserted {successful_count} contacts in {insert_duration:.2f}s ({rate:.1f} contacts/sec)"
                )
            
            return successful_count
            
        except SQLAlchemyError as e:
            logger.error(f"Bulk insert failed: {str(e)}")
            # Fallback to individual inserts for error analysis
            logger.warning("Falling back to individual insert mode for error analysis")
            return await self._bulk_insert_contacts_fallback(contacts)
        
        except Exception as e:
            logger.error(f"Unexpected error in bulk insert: {str(e)}")
            raise

    async def _bulk_insert_contacts_fallback(self, contacts: List[Contact]) -> int:
        """
        Enhanced fallback method with better error tracking.
        
        Args:
            contacts: List of Contact objects to insert
            
        Returns:
            int: Number of contacts successfully inserted
        """
        successful_count = 0
        
        logger.info(f"Processing {len(contacts)} contacts individually for error analysis")
        
        for contact in contacts:
            try:
                # Check if contact already exists
                existing = await self.session.execute(
                    select(Contact).where(
                        Contact.import_id == contact.import_id,
                        Contact.phone == contact.phone
                    )
                )
                
                if existing.scalar_one_or_none():
                    logger.debug(f"Skipping duplicate contact: {contact.phone}")
                    continue
                
                # Generate ID if not set
                if not contact.id:
                    contact.id = generate_prefixed_id(IDPrefix.CONTACT)
                
                # Insert individual contact
                self.session.add(contact)
                successful_count += 1
                
            except SQLAlchemyError as e:
                logger.error(f"Failed to insert contact {contact.phone}: {str(e)}")
            
            except Exception as e:
                logger.error(f"Unexpected error inserting contact {contact.phone}: {str(e)}")
        
        logger.info(f"Fallback processing completed: {successful_count} contacts inserted")
        return successful_count
    
    async def _update_import_job(self, import_job_id: str, updates: Dict[str, Any]) -> None:
        """Enhanced import job updates with better error handling."""
        try:
            # Get import job
            result = await self.session.execute(
                select(ImportJob).where(ImportJob.id == import_job_id)
            )
            import_job = result.scalar_one_or_none()
            
            if not import_job:
                logger.error(f"Import job not found: {import_job_id}")
                return
            
            # Apply updates
            for key, value in updates.items():
                if hasattr(import_job, key):
                    setattr(import_job, key, value)
                else:
                    logger.warning(f"Unknown import job field: {key}")
            
            # Always update the updated_at timestamp
            import_job.updated_at = datetime.now(timezone.utc)
            
        except SQLAlchemyError as e:
            logger.error(f"Failed to update import job {import_job_id}: {str(e)}")
        except Exception as e:
            logger.error(f"Unexpected error updating import job {import_job_id}: {str(e)}")

    def _create_detection_from_mapping(self, mapping_config: Dict[str, Any], headers: List[str]) -> ColumnDetectionResult:
        """
        Create a synthetic detection result from user-provided mapping.
        
        Args:
            mapping_config: User-provided mapping configuration
            headers: CSV headers
            
        Returns:
            ColumnDetectionResult: Detection result based on mapping
        """
        result = ColumnDetectionResult()
        
        # Set phone columns
        phone_columns = mapping_config.get('phone_columns', [])
        if phone_columns:
            result.primary_phone_column = phone_columns[0]
            result.phone_candidates = [
                {"column": col, "score": 100.0, "confidence": "high"}
                for col in phone_columns
            ]
            result.confidence_scores["phone"] = 100.0
        
        # Set name column
        name_column = mapping_config.get('name_column')
        if name_column:
            result.name_column = name_column
            result.confidence_scores["name"] = 100.0
        
        # Set detection quality
        result.detection_quality = "high"  # User-provided mapping is always high confidence
        result.user_guidance = ["✅ Using user-provided column mapping"]
        
        result.detected_columns = {
            "phones": phone_columns,
            "primary_phone": result.primary_phone_column,
            "name": result.name_column,
            "confidence_scores": result.confidence_scores,
            "user_mapped": True  # Flag to indicate this was user-mapped
        }
        
        return result


    async def _process_csv_chunks_with_mapping(
        self,
        file_path: Path,
        encoding: str,
        delimiter: str,
        import_job_id: str,
        mapping_config: Dict[str, Any],
        result: CSVParseResult
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Process CSV chunks with explicit column mapping.
        
        Similar to _enhanced_process_csv_chunks but uses mapping config directly.
        """
        phone_columns = mapping_config.get('phone_columns', [])
        name_column = mapping_config.get('name_column')
        skip_columns = set(mapping_config.get('skip_columns', []))
        tag_columns = mapping_config.get('tag_columns', [])
        skip_invalid_phones = mapping_config.get('skip_invalid_phones', True)
        
        with open(file_path, 'r', encoding=encoding) as f:
            reader = csv.DictReader(f, delimiter=delimiter)
            
            chunk_contacts = []
            chunk_errors = []
            row_number = 1
            chunk_start_time = datetime.now(timezone.utc)
            
            for row in reader:
                row_number += 1
                
                try:
                    # Parse contact with explicit mapping
                    contact_data = await self._parse_contact_row_with_mapping(
                        row, row_number, mapping_config, import_job_id
                    )
                    
                    if contact_data:
                        chunk_contacts.append(contact_data)
                    
                except ValidationError as e:
                    if skip_invalid_phones and "Invalid phone" in str(e):
                        # Skip invalid phones if option is set
                        logger.debug(f"Skipping row {row_number}: {str(e)}")
                    else:
                        chunk_errors.append(ImportError(
                            row=row_number,
                            column=None,
                            message=str(e),
                            value=str(row)
                        ))
                
                # Process chunk when full (same as before)
                if (row_number - 1) % self.config.BULK_INSERT_SIZE == 0:
                    successful = await self._bulk_insert_contacts(chunk_contacts)
                    
                    chunk_end_time = datetime.now(timezone.utc)
                    chunk_duration = (chunk_end_time - chunk_start_time).total_seconds()
                    processing_rate = self.config.BULK_INSERT_SIZE / chunk_duration if chunk_duration > 0 else 0
                    
                    yield {
                        'processed': self.config.BULK_INSERT_SIZE,
                        'successful': successful,
                        'errors': chunk_errors,
                        'processing_rate': processing_rate,
                        'memory_usage_mb': self._get_memory_usage_mb()
                    }
                    
                    chunk_contacts.clear()
                    chunk_errors.clear()
                    chunk_start_time = chunk_end_time
                    await asyncio.sleep(0.01)
            
            # Process final chunk
            if chunk_contacts or chunk_errors:
                successful = await self._bulk_insert_contacts(chunk_contacts)
                remainder = (row_number - 1) % self.config.BULK_INSERT_SIZE
                
                chunk_end_time = datetime.now(timezone.utc)
                chunk_duration = (chunk_end_time - chunk_start_time).total_seconds()
                processing_rate = remainder / chunk_duration if chunk_duration > 0 else 0
                
                yield {
                    'processed': remainder or self.config.BULK_INSERT_SIZE,
                    'successful': successful,
                    'errors': chunk_errors,
                    'processing_rate': processing_rate,
                    'memory_usage_mb': self._get_memory_usage_mb()
                }


    async def _parse_contact_row_with_mapping(
        self,
        row: Dict[str, str],
        row_number: int,
        mapping_config: Dict[str, Any],
        import_job_id: str
    ) -> Optional[Contact]:
        """
        Parse contact row using explicit mapping configuration.
        """
        phone_columns = mapping_config.get('phone_columns', [])
        name_column = mapping_config.get('name_column')
        skip_columns = set(mapping_config.get('skip_columns', []))
        tag_columns = mapping_config.get('tag_columns', [])
        phone_country_default = mapping_config.get('phone_country_default', 'US')
        
        # Try to get phone from mapped columns (try each until we find a valid one)
        phone_raw = None
        phone_normalized = None
        
        for phone_col in phone_columns:
            if phone_col in row:
                candidate = row[phone_col].strip()
                if candidate:
                    try:
                        is_valid, normalized, error, metadata = validate_phone(candidate, default_country=phone_country_default)
                        if is_valid:
                            phone_raw = candidate
                            phone_normalized = normalized
                            break
                    except Exception:
                        continue
        
        if not phone_normalized:
            raise ValidationError(f"No valid phone number found in columns: {', '.join(phone_columns)}")
        
        # Get name from mapped column
        name = None
        if name_column and name_column in row:
            name_raw = row[name_column].strip()
            if name_raw and len(name_raw) <= 100:
                name = name_raw
        
        # Create tags from specified columns
        tags = []
        for tag_col in tag_columns:
            if tag_col in row and row[tag_col].strip():
                clean_value = row[tag_col].strip()[:50]
                tags.append(f"{tag_col}:{clean_value}")
        
        # Create contact
        contact = Contact(
            import_id=import_job_id,
            phone=phone_normalized,
            name=name,
            tags=tags[:20],  # Limit tags
            csv_row_number=row_number,
            raw_data=dict(row)
        )
        
        return contact
# Enhanced utility functions for backward compatibility
def extract_phone_columns(headers: List[str]) -> List[str]:
    """
    Enhanced phone column extraction with better pattern matching.
    
    Args:
        headers: List of CSV headers
        
    Returns:
        List[str]: Potential phone number columns sorted by likelihood
    """
    phone_columns = []
    phone_patterns = [
        'phone', 'mobile', 'cell', 'tel', 'telephone', 'whatsapp', 
        'contact', 'number', 'fone'  # Additional patterns
    ]
    
    for header in headers:
        header_lower = header.lower().replace(' ', '_').replace('-', '_')
        if any(pattern in header_lower for pattern in phone_patterns):
            phone_columns.append(header)
    
    return phone_columns


def extract_name_columns(headers: List[str]) -> List[str]:
    """
    Enhanced name column extraction with better pattern matching.
    
    Args:
        headers: List of CSV headers
        
    Returns:
        List[str]: Potential name columns sorted by likelihood
    """
    name_columns = []
    name_patterns = [
        'name', 'contact', 'person', 'client', 'customer', 
        'full_name', 'first_name', 'last_name', 'fname', 'lname'
    ]
    
    for header in headers:
        header_lower = header.lower().replace(' ', '_').replace('-', '_')
        if any(pattern in header_lower for pattern in name_patterns):
            name_columns.append(header)
    
    return name_columns


# Estimation function for completion time
def estimate_processing_time(remaining_rows: int, processing_rate: float) -> str:
    """
    Estimate processing completion time based on current rate.
    
    Args:
        remaining_rows: Number of rows left to process
        processing_rate: Current processing rate (rows per second)
        
    Returns:
        str: Human-readable time estimate
    """
    if processing_rate <= 0:
        return "Calculating..."
    
    estimated_seconds = remaining_rows / processing_rate
    
    if estimated_seconds < 60:
        return f"~{int(estimated_seconds)} seconds"
    elif estimated_seconds < 3600:
        return f"~{int(estimated_seconds / 60)} minutes"
    else:
        return f"~{int(estimated_seconds / 3600)} hours"
</file>

<file path="alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="alembic/versions/4669eb7a8878_add_user_metrics_table.py">
"""Add user metrics table

Revision ID: 4669eb7a8878
Revises: a5a3f106534b
Create Date: 2025-05-01 22:14:21.434189

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '4669eb7a8878'
down_revision: Union[str, None] = 'a5a3f106534b'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('usermetrics',
    sa.Column('user_id', sa.String(), nullable=False),
    sa.Column('date', sa.Date(), nullable=False),
    sa.Column('messages_sent', sa.Integer(), nullable=False),
    sa.Column('messages_delivered', sa.Integer(), nullable=False),
    sa.Column('messages_failed', sa.Integer(), nullable=False),
    sa.Column('messages_scheduled', sa.Integer(), nullable=False),
    sa.Column('campaigns_created', sa.Integer(), nullable=False),
    sa.Column('campaigns_completed', sa.Integer(), nullable=False),
    sa.Column('campaigns_active', sa.Integer(), nullable=False),
    sa.Column('templates_created', sa.Integer(), nullable=False),
    sa.Column('templates_used', sa.Integer(), nullable=False),
    sa.Column('quota_total', sa.Integer(), nullable=False),
    sa.Column('quota_used', sa.Integer(), nullable=False),
    sa.Column('meta_data', sa.JSON(), nullable=True),
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('user_id', 'date', name='uix_user_date')
    )
    op.create_index(op.f('ix_usermetrics_date'), 'usermetrics', ['date'], unique=False)
    op.create_index(op.f('ix_usermetrics_id'), 'usermetrics', ['id'], unique=False)
    op.create_index(op.f('ix_usermetrics_user_id'), 'usermetrics', ['user_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_usermetrics_user_id'), table_name='usermetrics')
    op.drop_index(op.f('ix_usermetrics_id'), table_name='usermetrics')
    op.drop_index(op.f('ix_usermetrics_date'), table_name='usermetrics')
    op.drop_table('usermetrics')
    # ### end Alembic commands ###
</file>

<file path="alembic/versions/65b12145bf1a_add_message_templates_table.py">
"""Add message templates table

Revision ID: 65b12145bf1a
Revises: 
Create Date: 2025-04-24 15:52:34.696386

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '65b12145bf1a'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###
</file>

<file path="alembic/versions/a5a3f106534b_make_datetime_fields_timezone_aware.py">
"""Make datetime fields timezone-aware

Revision ID: a5a3f106534b
Revises: 65b12145bf1a
Create Date: 2025-04-29 21:44:26.340452

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'a5a3f106534b'
down_revision: Union[str, None] = '65b12145bf1a'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('apikey', 'expires_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('apikey', 'last_used_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('apikey', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('apikey', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('campaign', 'scheduled_start_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'scheduled_end_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'started_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'completed_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('campaign', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('message', 'scheduled_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'sent_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'delivered_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'failed_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('message', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagebatch', 'completed_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('messagebatch', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagebatch', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messageevent', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messageevent', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('user', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('user', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhook', 'last_triggered_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('webhook', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhook', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'next_retry_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('webhookdelivery', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookevent', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookevent', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('webhookevent', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookevent', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'next_retry_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('webhook', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhook', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhook', 'last_triggered_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('user', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('user', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messageevent', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messageevent', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagebatch', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagebatch', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagebatch', 'completed_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('message', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('message', 'failed_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'delivered_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'sent_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'scheduled_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('campaign', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('campaign', 'completed_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'started_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'scheduled_end_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'scheduled_start_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('apikey', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('apikey', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('apikey', 'last_used_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('apikey', 'expires_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    # ### end Alembic commands ###
</file>

<file path="alembic/versions/be43f8bb42d0_add_campaign_personalization_and_import_.py">
"""Add campaign personalization and import pipeline

Revision ID: be43f8bb42d0
Revises: 4669eb7a8878
Create Date: 2025-06-22 18:37:25.178324

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'be43f8bb42d0'
down_revision: Union[str, None] = '4669eb7a8878'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('importjob',
    sa.Column('status', sa.Enum('PROCESSING', 'SUCCESS', 'FAILED', 'CANCELLED', name='importstatus'), nullable=False),
    sa.Column('rows_total', sa.Integer(), nullable=False),
    sa.Column('rows_processed', sa.Integer(), nullable=False),
    sa.Column('errors', sa.JSON(), nullable=True),
    sa.Column('sha256', sa.String(), nullable=True),
    sa.Column('filename', sa.String(), nullable=True),
    sa.Column('file_size', sa.Integer(), nullable=True),
    sa.Column('started_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('completed_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('owner_id', sa.String(), nullable=False),
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['owner_id'], ['user.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_importjob_id'), 'importjob', ['id'], unique=False)
    op.create_index(op.f('ix_importjob_owner_id'), 'importjob', ['owner_id'], unique=False)
    op.create_index(op.f('ix_importjob_sha256'), 'importjob', ['sha256'], unique=False)
    op.create_index(op.f('ix_importjob_status'), 'importjob', ['status'], unique=False)
    op.create_table('contact',
    sa.Column('import_id', sa.String(), nullable=False),
    sa.Column('phone', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=True),
    sa.Column('tags', sa.JSON(), nullable=True),
    sa.Column('csv_row_number', sa.Integer(), nullable=True),
    sa.Column('raw_data', sa.JSON(), nullable=True),
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['import_id'], ['importjob.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('import_id', 'phone', name='uix_import_phone')
    )
    op.create_index(op.f('ix_contact_id'), 'contact', ['id'], unique=False)
    op.create_index(op.f('ix_contact_import_id'), 'contact', ['import_id'], unique=False)
    op.create_index(op.f('ix_contact_phone'), 'contact', ['phone'], unique=False)
    op.add_column('campaign', sa.Column('message_content', sa.Text(), nullable=True))
    op.add_column('campaign', sa.Column('template_id', sa.String(), nullable=True))
    op.create_index(op.f('ix_campaign_template_id'), 'campaign', ['template_id'], unique=False)
    op.create_foreign_key(None, 'campaign', 'messagetemplate', ['template_id'], ['id'])
    op.add_column('message', sa.Column('variables', sa.JSON(), nullable=True))
    op.add_column('message', sa.Column('import_id', sa.String(), nullable=True))
    op.create_index(op.f('ix_message_import_id'), 'message', ['import_id'], unique=False)
    op.create_foreign_key(None, 'message', 'importjob', ['import_id'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, 'message', type_='foreignkey')
    op.drop_index(op.f('ix_message_import_id'), table_name='message')
    op.drop_column('message', 'import_id')
    op.drop_column('message', 'variables')
    op.drop_constraint(None, 'campaign', type_='foreignkey')
    op.drop_index(op.f('ix_campaign_template_id'), table_name='campaign')
    op.drop_column('campaign', 'template_id')
    op.drop_column('campaign', 'message_content')
    op.drop_index(op.f('ix_contact_phone'), table_name='contact')
    op.drop_index(op.f('ix_contact_import_id'), table_name='contact')
    op.drop_index(op.f('ix_contact_id'), table_name='contact')
    op.drop_table('contact')
    op.drop_index(op.f('ix_importjob_status'), table_name='importjob')
    op.drop_index(op.f('ix_importjob_sha256'), table_name='importjob')
    op.drop_index(op.f('ix_importjob_owner_id'), table_name='importjob')
    op.drop_index(op.f('ix_importjob_id'), table_name='importjob')
    op.drop_table('importjob')
    # ### end Alembic commands ###
</file>

<file path="app/api/v1/endpoints/imports.py">
# app/api/v1/endpoints/imports.py
"""
Production-ready CSV import endpoints for Phase 2A.

This module provides streaming CSV upload, background processing, and progress tracking
for contact imports with memory-efficient parsing and robust error handling.

Key Features:
- Streaming file upload with SHA-256 integrity checking
- Memory-efficient CSV parsing with configurable chunk sizes  
- Background processing with real-time progress updates
- Comprehensive error handling and validation
- Rate limiting and concurrent job management
- Automatic file cleanup and security measures

Architecture Compliance:
- Uses proper repository pattern with dependency injection
- Implements context managers for transaction safety
- Background tasks with proper error handling
- Follows REST API conventions with appropriate status codes
"""
import os
import csv
import hashlib
import tempfile
import asyncio
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Dict, Any, List
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Query, BackgroundTasks, status
from fastapi.responses import JSONResponse

# Core dependencies
from app.api.v1.dependencies import get_current_user, get_rate_limiter
from app.core.exceptions import ValidationError, NotFoundError, InboxerrException
from app.schemas.user import User
from app.schemas.import_job import ImportJobResponse, ImportJobSummary, ImportStatus, ImportPreviewResponse, ColumnInfo, MappingSuggestion, ProcessImportRequest, ColumnMapping
from app.schemas.contact import ContactResponse
from app.services.imports.parser import StreamingCSVParser, CSVParseResult, ColumnDetectionResult
from app.services.imports.events import (
    ImportProgressV1, ImportEventType, ImportErrorV1,
    create_progress_event, create_completion_event, create_failure_event
)
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.utils.datetime import utc_now
from app.utils.pagination import PaginationParams, paginate_response, PaginatedResponse
from app.db.session import get_repository_context

# Import proper repository implementations (FIXED: No longer using inline classes)
from app.db.repositories.import_jobs import ImportJobRepository
from app.db.repositories.contacts import ContactRepository

# Setup logging
router = APIRouter()
logger = logging.getLogger("inboxerr.imports")

# Production constants with security considerations
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB - matches Phase 2A spec
MAX_ROW_COUNT = 1_000_000  # 1M rows - matches Phase 2A spec
MAX_CONCURRENT_JOBS = 5  # Limit concurrent processing jobs per user
ALLOWED_EXTENSIONS = {".csv", ".txt"}  # Support TXT for broader compatibility
ALLOWED_MIME_TYPES = {"text/csv", "text/plain", "application/csv", "application/vnd.ms-excel"}
CHUNK_SIZE = 8192  # 8KB chunks for streaming - optimized for memory usage
TEMP_FILE_PREFIX = "inboxerr_import_"  # Secure temp file naming
TEMP_DIR = os.getenv('INBOXERR_TEMP_DIR', tempfile.gettempdir())


@router.post("/upload", status_code=status.HTTP_202_ACCEPTED)
async def upload_csv_file(
    file: UploadFile = File(..., description="CSV file containing contact data"),
    auto_process: Optional[bool] = Query(
        None, 
        description="Whether to auto-process if confidence is high. If not specified, defaults based on detection confidence."
    ),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
) -> JSONResponse:
    """
    Upload a CSV file for streaming processing.
    
    **Phase 2A Implementation Features:**
    - Streams file to secure temporary storage
    - Computes SHA-256 hash for integrity verification
    - Creates ImportJob record with PROCESSING status
    - Starts automatic background processing
    - Returns 202 immediately with job tracking information
    - Implements proper security measures and validation
    
    **Security Features:**
    - File type validation (extension + MIME type)
    - File size limits (100MB max)
    - Row count limits (1M rows max) 
    - Rate limiting per user
    - Concurrent job limits
    - Secure temporary file handling
    
    **Performance Features:**
    - Streaming upload (constant memory usage)
    - Chunked processing for large files
    - Automatic background processing with progress tracking
    - Automatic cleanup of temporary files
    
    Args:
        file: CSV file upload with contact data
        background_tasks: FastAPI background tasks manager
        current_user: Authenticated user from JWT token
        rate_limiter: Rate limiting service dependency
        
    Returns:
        JSONResponse: 202 Accepted with job tracking details
        
    Raises:
        HTTPException: Various validation and processing errors
        ValidationError: File format or content validation failures
        NotFoundError: Resource not found errors
    """
    # Apply rate limiting - prevent abuse
    await rate_limiter.check_rate_limit(current_user.id, "csv_upload")
    
    # Validate file is provided
    if not file or not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="No file provided"
        )
    
    try:
        # Phase 1: File Validation and Security Checks
        await _validate_uploaded_file(file)
        
        # Phase 2: Check Concurrent Job Limits  
        async with get_repository_context(ImportJobRepository) as import_repo:
            active_jobs_count = await import_repo.get_active_jobs_count(current_user.id)

            
            if active_jobs_count >= MAX_CONCURRENT_JOBS:
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=f"Maximum {MAX_CONCURRENT_JOBS} concurrent import jobs allowed. "
                           f"Please wait for existing jobs to complete."
                )
        
        # Phase 3: Streaming File Processing with Memory Efficiency
        import_job_id = generate_prefixed_id(IDPrefix.IMPORT)  # Import job gets "import-xxxx"
        temp_path = None
        file_hash = hashlib.sha256()
        total_size = 0
        row_count = 0
        headers = []
        
        try:
            # Create secure temporary file
            temp_fd, temp_path = tempfile.mkstemp(
                prefix=TEMP_FILE_PREFIX,
                suffix=".csv",
                dir=TEMP_DIR
            )
            
            with os.fdopen(temp_fd, 'wb') as temp_file:
                # Stream file in chunks to prevent memory exhaustion
                while True:
                    chunk = await file.read(CHUNK_SIZE)
                    if not chunk:
                        break
                    
                    total_size += len(chunk)
                    file_hash.update(chunk)
                    temp_file.write(chunk)
                    
                    # Safety check for file size during streaming
                    if total_size > MAX_FILE_SIZE:
                        raise HTTPException(
                            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                            detail=f"File size exceeds {MAX_FILE_SIZE // (1024*1024)}MB limit"
                        )
            
            # Phase 4: Quick CSV Analysis for Row Count and Headers
            try:
                with open(temp_path, 'r', encoding='utf-8') as csv_file:
                    # Detect delimiter using CSV sniffer
                    sample = csv_file.read(8192)
                    csv_file.seek(0)
                    
                    sniffer = csv.Sniffer()
                    try:
                        delimiter = sniffer.sniff(sample, delimiters=',\t|;').delimiter
                    except csv.Error:
                        delimiter = ','  # Default fallback
                    
                    # Read headers
                    reader = csv.reader(csv_file, delimiter=delimiter)
                    headers = next(reader, [])
                    
                    # Count rows efficiently
                    row_count = sum(1 for _ in reader)
                    
                    # Validate row count
                    if row_count > MAX_ROW_COUNT:
                        raise HTTPException(
                            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                            detail=f"File contains {row_count:,} rows. "
                                   f"Maximum: {MAX_ROW_COUNT:,}"
                        )
                        
            except UnicodeDecodeError:
                raise HTTPException(
                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                    detail="File encoding not supported. Please use UTF-8 encoding."
                )
            except Exception as e:
                raise HTTPException(
                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                    detail=f"Error analyzing CSV file: {str(e)}"
                )
            
            # Phase 5: Create ImportJob Record in Database
            async with get_repository_context(ImportJobRepository) as import_repo:
                import_job = await import_repo.create_import_job(
                    id=import_job_id,
                    filename=file.filename,
                    file_size=total_size,
                    sha256=file_hash.hexdigest(),
                    owner_id=current_user.id,
                    rows_total=row_count
                )
            
            # Phase 6: Decide whether to auto-process
            # Quick detection to determine confidence level
            should_auto_process = False
            confidence_level = "unknown"

            if auto_process is None:
                # Run quick detection to determine confidence
                try:
                    from app.services.imports.parser import StreamingCSVParser
                    from app.db.session import get_session
                    
                    async with get_session() as session:
                        parser = StreamingCSVParser(session)
                        detection_result = await parser._enhanced_column_detection(
                            Path(temp_path), 'utf-8', delimiter, headers
                        )
                        confidence_level = detection_result.detection_quality
                        
                        # Auto-process only for high confidence
                        should_auto_process = (confidence_level == "high")
                        
                        # Store detection info for preview
                        await import_repo.update(
                            id=import_job_id,
                            obj_in={
                                "errors": [{
                                    "row": 0,
                                    "column": "_metadata",
                                    "message": "Initial detection",
                                    "value": {
                                        "column_detection": {
                                            "quality": detection_result.detection_quality,
                                            "phone_confidence": detection_result.phone_confidence,
                                            "name_confidence": detection_result.name_confidence,
                                            "primary_phone_column": detection_result.primary_phone_column,
                                            "name_column": detection_result.name_column,
                                            "user_guidance": detection_result.user_guidance
                                        }
                                    }
                                }]
                            }
                        )
                except Exception as e:
                    logger.warning(f"Quick detection failed, defaulting to manual mapping: {str(e)}")
                    should_auto_process = False
                    confidence_level = "low"
            else:
                # Use explicit user preference
                should_auto_process = auto_process

            # Schedule processing if needed
            if should_auto_process:
                background_tasks.add_task(
                    process_csv_background,
                    import_job.id,
                    temp_path
                )
                processing_message = "File uploaded successfully. Processing started automatically due to high confidence detection."
            else:
                processing_message = f"File uploaded successfully. Detection confidence: {confidence_level}. Please review and map columns."
            
            
            # Return 202 with comprehensive tracking information
            return JSONResponse(
                status_code=status.HTTP_202_ACCEPTED,
                content={
                    "job_id": import_job.id,
                    "file_hash": file_hash.hexdigest(),
                    "file_size": total_size,
                    "row_count": row_count,
                    "headers": headers,
                    "detected_delimiter": delimiter if 'delimiter' in locals() else ',',
                    "status": ImportStatus.PROCESSING.value,
                    "message": processing_message,
                    "auto_processing": should_auto_process,
                    "confidence_level": confidence_level,
                    "preview_url": f"/api/v1/imports/jobs/{import_job.id}/preview",
                    "progress_url": f"/api/v1/imports/jobs/{import_job.id}",
                    "estimated_completion_time": _estimate_processing_time(row_count) if should_auto_process else None
                }
            )
            
        except HTTPException:
            # Clean up temp file on HTTP exceptions
            if temp_path and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except OSError as e:
                    logger.warning(f"Failed to cleanup temp file {temp_path}: {e}")
            raise
            
        except Exception as e:
            # Clean up temp file on any other error
            if temp_path and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except OSError as cleanup_error:
                    logger.warning(f"Failed to cleanup temp file {temp_path}: {cleanup_error}")
            
            logger.error(f"Unexpected error during CSV upload: {str(e)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error processing upload: {str(e)}"
            )
        
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        logger.error(f"Critical error in CSV upload endpoint: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during file upload"
        )



@router.get("/jobs/{job_id}", response_model=ImportJobResponse)
async def get_import_job_status(
    job_id: str,
    current_user: User = Depends(get_current_user),
) -> ImportJobResponse:
    """
    Get the status of an import job with comprehensive progress information.
    
    **Real-time Monitoring Features:**
    - Current processing progress (percentage complete)
    - Row-level processing statistics
    - Error details and validation issues
    - Performance metrics and timing
    - File integrity verification
    
    Args:
        job_id: Import job identifier
        current_user: Authenticated user from JWT token
        
    Returns:
        ImportJobResponse: Detailed job status and progress
        
    Raises:
        HTTPException: Authorization or not found errors
        NotFoundError: Import job not found
    """
    try:
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Get import job with error handling
            import_job = await import_repo.get_by_id(job_id)
            if not import_job:
                raise NotFoundError(f"Import job {job_id} not found")
            
            # Check ownership - security critical
            if import_job.owner_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to access this import job"
                )
            
            # Convert to response schema with computed fields
            return ImportJobResponse(
                id=import_job.id,
                status=import_job.status,
                filename=import_job.filename,
                file_size=import_job.file_size,
                rows_total=import_job.rows_total,
                rows_processed=import_job.rows_processed,
                errors=import_job.errors or [],
                sha256=import_job.sha256,
                started_at=import_job.started_at,
                completed_at=import_job.completed_at,
                created_at=import_job.created_at,
                updated_at=import_job.updated_at,
                owner_id=import_job.owner_id,
                progress_percentage=import_job.progress_percentage,
                has_errors=import_job.has_errors,
                error_count=import_job.error_count
            )
            
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        logger.error(f"Error retrieving import job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving import job: {str(e)}"
        )


@router.get("/jobs", response_model=PaginatedResponse[ImportJobSummary])
async def list_import_jobs(
    pagination: PaginationParams = Depends(),
    status_filter: Optional[ImportStatus] = Query(None, description="Filter by job status"),
    current_user: User = Depends(get_current_user),
) -> PaginatedResponse[ImportJobSummary]:
    """
    List import jobs for the current user with filtering and pagination.
    
    **Query Features:**
    - Status-based filtering (processing, success, failed, cancelled)
    - Pagination with configurable page sizes
    - Sorted by creation date (newest first)
    - Summary format for efficient list display
    
    Args:
        pagination: Pagination parameters (page, size, etc.)
        status_filter: Optional status filter
        current_user: Authenticated user from JWT token
        
    Returns:
        PaginatedResponse[ImportJobSummary]: Paginated list of import jobs
        
    Raises:
        HTTPException: Query or processing errors
    """
    try:
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Get paginated jobs with optional status filter
            jobs, total = await import_repo.get_by_owner(
                owner_id=current_user.id,
                status=status_filter,
                skip=pagination.skip,
                limit=pagination.limit
            )
            
            # Convert to summary format for efficient transfer
            job_summaries = [
                ImportJobSummary(
                    id=job.id,
                    filename=job.filename,
                    status=job.status,
                    rows_total=job.rows_total,
                    rows_processed=job.rows_processed,
                    progress_percentage=job.progress_percentage,
                    has_errors=job.has_errors,
                    error_count=job.error_count,
                    created_at=job.created_at,
                    completed_at=job.completed_at
                )
                for job in jobs
            ]
            
            return paginate_response(
                items=job_summaries,
                total=total,
                pagination=pagination
            )
            
    except Exception as e:
        logger.error(f"Error listing import jobs for user {current_user.id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error listing import jobs: {str(e)}"
        )


@router.get("/jobs/{job_id}/contacts", response_model=PaginatedResponse[ContactResponse])
async def get_import_contacts(
    job_id: str,
    pagination: PaginationParams = Depends(),
    current_user: User = Depends(get_current_user),
) -> PaginatedResponse[ContactResponse]:
    """
    Get contacts created from an import job with pagination.
    
    **Preview Features:**
    - Paginated contact listing for large imports
    - Full contact details including validation results
    - Phone number formatting for display
    - Original CSV row data preservation
    
    Args:
        job_id: Import job identifier
        pagination: Pagination parameters
        current_user: Authenticated user from JWT token
        
    Returns:
        PaginatedResponse[ContactResponse]: Paginated contacts from import
        
    Raises:
        HTTPException: Authorization or not found errors
        NotFoundError: Import job not found
    """
    try:
        # Verify import job exists and user has access
        async with get_repository_context(ImportJobRepository) as import_repo:
            import_job = await import_repo.get_by_id(job_id)
            if not import_job:
                raise NotFoundError(f"Import job {job_id} not found")
            
            if import_job.owner_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to access this import job"
                )
        
        # Get contacts from import
        async with get_repository_context(ContactRepository) as contact_repo:
            contacts, total = await contact_repo.get_by_import_id(
                import_id=job_id,
                skip=pagination.skip,
                limit=pagination.limit
            )
            
            # Convert to response format with computed fields
            contact_responses = [
                ContactResponse(
                    id=contact.id,
                    import_id=contact.import_id,
                    phone=contact.phone,
                    name=contact.name,
                    tags=contact.tags or [],
                    csv_row_number=contact.csv_row_number,
                    raw_data=contact.raw_data,
                    created_at=contact.created_at,
                    updated_at=contact.updated_at,
                    display_name=contact.name or contact.phone,
                    formatted_phone=contact.phone  # TODO: Add phone formatting utility
                )
                for contact in contacts
            ]
            
            return paginate_response(
                items=contact_responses,
                total=total,
                pagination=pagination
            )
            
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        logger.error(f"Error getting contacts for import job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error retrieving contacts: {str(e)}"
        )


@router.delete("/jobs/{job_id}", status_code=status.HTTP_204_NO_CONTENT)
async def cancel_import_job(
    job_id: str,
    current_user: User = Depends(get_current_user),
) -> None:
    """
    Cancel an import job and optionally clean up contacts.
    
    **Cancellation Features:**
    - Immediate status update to CANCELLED
    - Stops ongoing processing (if possible)
    - Preserves completed work (contacts already created)
    - Automatic cleanup of temporary files
    
    **Security Notes:**
    - Only job owner can cancel
    - Cannot cancel completed jobs
    - Graceful handling of concurrent cancellation attempts
    
    Args:
        job_id: Import job identifier
        current_user: Authenticated user from JWT token
        
    Returns:
        None: 204 No Content on successful cancellation
        
    Raises:
        HTTPException: Authorization, validation, or processing errors
        NotFoundError: Import job not found
    """
    try:
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Get import job with error handling
            import_job = await import_repo.get_by_id(job_id)
            if not import_job:
                raise NotFoundError(f"Import job {job_id} not found")
            
            # Check ownership - security critical
            if import_job.owner_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to cancel this import job"
                )
            
            # Check if job can be cancelled
            if import_job.status in [ImportStatus.SUCCESS, ImportStatus.CANCELLED]:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Cannot cancel job with status {import_job.status.value}"
                )
            
            # Update job status to cancelled
            await import_repo.update(
                id=job_id,
                obj_in={
                    "status": ImportStatus.CANCELLED,
                    "completed_at": utc_now(),
                    "updated_at": utc_now()
                }
            )
            
            logger.info(f"Import job {job_id} cancelled by user {current_user.id}")
            
            # Note: We don't delete contacts that were already created successfully
            # This preserves user data and allows partial imports to be useful
            
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        logger.error(f"Error cancelling import job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error cancelling import job: {str(e)}"
        )


@router.get("/jobs/{job_id}/preview", response_model=ImportPreviewResponse)
async def get_import_preview(
    job_id: str,
    current_user: User = Depends(get_current_user),
) -> ImportPreviewResponse:
    """
    Get a preview of the CSV file with column analysis and mapping suggestions.
    
    **Preview Features:**
    - First 5 rows of data for visual inspection
    - Column analysis with data type detection
    - Smart mapping suggestions with confidence scores
    - Guidance messages for low-confidence detection
    
    Args:
        job_id: Import job identifier
        current_user: Authenticated user from JWT token
        
    Returns:
        ImportPreviewResponse: Preview data with mapping suggestions
        
    Raises:
        HTTPException: Authorization or processing errors
        NotFoundError: Import job not found
    """
    try:
        # Verify job exists and user has access
        async with get_repository_context(ImportJobRepository) as import_repo:
            import_job = await import_repo.get_by_id(job_id)
            if not import_job:
                raise NotFoundError(f"Import job {job_id} not found")
            
            if import_job.owner_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to access this import job"
                )
            
            # Check if job has already been processed
            if import_job.status != ImportStatus.PROCESSING:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Import job already {import_job.status.value}. Preview not available."
                )
        
        # Get metadata from errors array (stored in _metadata entry)
        metadata = None
        if import_job.errors:
            for error in import_job.errors:
                if error.get("column") == "_metadata":
                    metadata = error.get("value", {})
                    break
        
        # Get column detection info
        column_detection = metadata.get("column_detection", {}) if metadata else {}
        
        #=================
        # Build response
        #=================

        # Get preview data from file
        preview_data = await _get_csv_preview_data(import_job.sha256)
        
        # Build column info
        columns = []
        for i, col_name in enumerate(preview_data.get("headers", [])):
            columns.append(ColumnInfo(
                name=col_name,
                index=i,
                sample_values=preview_data.get("samples", {}).get(col_name, []),
                empty_count=preview_data.get("empty_counts", {}).get(col_name, 0),
                detected_type=_detect_column_type(col_name, preview_data.get("samples", {}).get(col_name, []))
            ))

        # Build suggestions based on detected column types
        suggestions = {
            "phone_columns": [],
            "name_columns": []
        }

        # Add suggestions based on column types detected
        for col in columns:
            if col.detected_type == "phone":
                suggestions["phone_columns"].append(MappingSuggestion(
                    column=col.name,
                    confidence=80.0,  # High confidence since type detection worked
                    reason=f"Column name '{col.name}' and data format match phone pattern"
                ))
            elif col.detected_type == "name":
                suggestions["name_columns"].append(MappingSuggestion(
                    column=col.name,
                    confidence=80.0,
                    reason=f"Column name '{col.name}' contains name data"
                ))
        
        # Determine confidence level
        phone_conf = column_detection.get("phone_confidence", 0)
        if phone_conf >= 80:
            confidence_level = "high"
        elif phone_conf >= 50:
            confidence_level = "medium"
        else:
            confidence_level = "low"
        
        
        # Build messages
        messages = column_detection.get("user_guidance", [])
        if confidence_level == "low":
            messages.append("⚠️ Low confidence detection. Please review and confirm column mappings.")
        
        return ImportPreviewResponse(
            job_id=job_id,
            file_info={
                "filename": import_job.filename,
                "file_size": import_job.file_size,
                "row_count": import_job.rows_total,
                "sha256": import_job.sha256
            },
            columns=columns,
            preview_rows=preview_data.get("rows", []),
            suggestions=suggestions,
            confidence_level=confidence_level,
            auto_process_recommended=(confidence_level == "high"),
            messages=messages
        )
        
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        logger.error(f"Error getting import preview for job {job_id}: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generating preview: {str(e)}"
        )

@router.post("/jobs/{job_id}/process", status_code=status.HTTP_202_ACCEPTED)
async def process_import_with_mapping(
    job_id: str,
    request: ProcessImportRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
) -> JSONResponse:
    """
    Process an import job with explicit column mapping.
    
    **Processing Features:**
    - Uses user-provided column mapping instead of auto-detection
    - Supports multiple phone columns
    - Allows skipping columns and creating tags
    - Configurable processing options
    
    Args:
        job_id: Import job identifier
        request: Column mapping and processing options
        background_tasks: FastAPI background tasks manager
        current_user: Authenticated user from JWT token
        
    Returns:
        JSONResponse: 202 Accepted with processing status
        
    Raises:
        HTTPException: Various validation or processing errors
    """
    try:
        # Verify job exists and user has access
        async with get_repository_context(ImportJobRepository) as import_repo:
            import_job = await import_repo.get_by_id(job_id)
            if not import_job:
                raise NotFoundError(f"Import job {job_id} not found")
            
            if import_job.owner_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to process this import job"
                )
            
            # Check job status - only allow processing of new jobs
            if import_job.status != ImportStatus.PROCESSING:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Import job is {import_job.status.value}. Only PROCESSING jobs can be mapped and processed."
                )
            
            # Validate that we have the file
            temp_file_path = _get_temp_file_path(import_job.sha256)
            if not temp_file_path or not os.path.exists(temp_file_path):
                raise HTTPException(
                    status_code=status.HTTP_410_GONE,
                    detail="Import file no longer available. Please upload again."
                )
            
            # Store mapping in job metadata
            mapping_metadata = {
                "column_mapping": request.column_mapping.dict(),
                "options": request.options,
                "mapped_at": datetime.now(timezone.utc).isoformat()
            }
            
            # Update job with mapping info
            await import_repo.update(
                id=job_id,
                obj_in={
                    "updated_at": datetime.now(timezone.utc),
                    "errors": [{
                        "row": 0,
                        "column": "_mapping",
                        "message": "User-provided column mapping",
                        "value": mapping_metadata
                    }]
                }
            )
        
        # Schedule processing with mapping
        background_tasks.add_task(
            process_csv_with_mapping_background,
            job_id,
            temp_file_path,
            request.column_mapping,
            request.options
        )
        
        logger.info(
            f"Import job {job_id} scheduled for processing with manual mapping. "
            f"Phone columns: {request.column_mapping.phone_columns}"
        )
        
        return JSONResponse(
            status_code=status.HTTP_202_ACCEPTED,
            content={
                "job_id": job_id,
                "status": "processing",
                "message": "Import job processing started with your column mapping.",
                "progress_url": f"/api/v1/imports/jobs/{job_id}",
                "mapping": {
                    "phone_columns": request.column_mapping.phone_columns,
                    "name_column": request.column_mapping.name_column,
                    "tag_columns": request.column_mapping.tag_columns,
                    "skip_columns": request.column_mapping.skip_columns
                }
            }
        )
        
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except Exception as e:
        logger.error(f"Error processing import job {job_id} with mapping: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error starting import processing: {str(e)}"
        )



# ============================================================================
# BACKGROUND PROCESSING IMPLEMENTATION
# ============================================================================
async def process_csv_background(job_id: str, temp_file_path: str) -> None:
    """
    Enhanced background task to process CSV file using the new StreamingCSVParser.
    
    **PRODUCTION ENHANCEMENTS:**
    - Uses enhanced column detection with confidence scoring
    - Provides detailed progress updates with performance metrics
    - Better error handling with user-friendly messages
    - Memory monitoring for large file processing
    - Comprehensive completion events
    
    Args:
        job_id: Import job identifier for tracking
        temp_file_path: Path to temporary CSV file to process
    """
    logger.info(f"Starting enhanced background processing for import job {job_id}")
    
    try:
        # PRODUCTION: Use direct session management for background tasks
        from app.db.session import get_session
        
        # Process CSV with enhanced parser
        result = None
        async with get_session() as session:
            # Create enhanced parser with session
            parser = StreamingCSVParser(session)
            
            # Enhanced progress callback with structured events
            async def enhanced_progress_callback(progress_event: ImportProgressV1):
                """Enhanced progress callback with detailed logging and potential WebSocket broadcasting."""
                try:
                    # Log progress with enhanced details
                    logger.info(
                        f"Import {job_id}: {progress_event['percent']:.1f}% "
                        f"({progress_event['processed']}/{progress_event['total_rows']}) "
                        f"✓{progress_event['successful']} ✗{progress_event['error_count']} "
                        f"Rate: {progress_event.get('processing_rate', 0)} rows/sec "
                        f"ETA: {progress_event.get('estimated_completion', 'Unknown')}"
                    )
                    
                    # Here you could broadcast to WebSockets for real-time frontend updates
                    # await websocket_manager.broadcast_to_user(user_id, progress_event)
                    
                    # Store detailed progress in cache for API polling
                    # await redis_client.setex(f"import_progress:{job_id}", 300, json.dumps(progress_event))
                    
                except Exception as e:
                    logger.warning(f"Progress callback error for job {job_id}: {str(e)}")
            
            # Process the CSV file with enhanced progress tracking
            result = await parser.parse_file(
                file_path=Path(temp_file_path),
                import_job_id=job_id,
                progress_callback=enhanced_progress_callback
            )
            
            logger.info(f"Enhanced CSV parsing completed for job {job_id}")
        
        # Enhanced completion handling using repository pattern
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Determine final status with enhanced logic
            if result.status == ImportStatus.PROCESSING:
                if result.error_count == 0:
                    final_status = ImportStatus.SUCCESS
                elif result.has_critical_errors:
                    final_status = ImportStatus.FAILED
                else:
                    final_status = ImportStatus.SUCCESS  # Partial success
            else:
                final_status = result.status
            
            # Enhanced completion data
            # Create enhanced error list with additional metadata
            enhanced_errors = []
            for error in result.errors[:100]:  # Limit stored errors
                enhanced_error = {
                    "row": error.row,
                    "column": error.column,
                    "message": error.message,
                    "value": error.value
                }
                enhanced_errors.append(enhanced_error)
            
            # Add summary information to the first error entry for metadata storage
            if enhanced_errors or final_status == ImportStatus.SUCCESS:
                summary_error = {
                    "row": 0,
                    "column": "_metadata",
                    "message": "Import summary",
                    "value": {
                        "successful_contacts": result.successful_contacts,
                        "total_errors": result.error_count,
                        "processing_rate": result.processing_rate,
                        "memory_peak_mb": result.memory_usage_mb,
                        "column_detection": {
                            "quality": result.column_detection.detection_quality,
                            "phone_confidence": result.column_detection.phone_confidence,
                            "name_confidence": result.column_detection.name_confidence,
                            "primary_phone_column": result.column_detection.primary_phone_column,
                            "name_column": result.column_detection.name_column,
                            "user_guidance": result.column_detection.user_guidance
                        }
                    }
                }
                enhanced_errors.insert(0, summary_error)
            
            # Update job with correct parameters
            await import_repo.complete_job(
                job_id=job_id,
                status=final_status,
                rows_processed=result.processed_rows,
                errors=enhanced_errors
            )
            
            
            # Send completion event (for WebSocket broadcasting)
            if result.start_time:
                total_time = (datetime.now(timezone.utc) - result.start_time).total_seconds()
                completion_event = create_completion_event(
                    job_id=job_id,
                    total_rows=result.total_rows,
                    successful_contacts=result.successful_contacts,
                    error_count=result.error_count,
                    processing_time=total_time,
                    average_rate=result.processing_rate,
                    peak_memory=result.memory_usage_mb,
                    sha256_verified=True,  # You have the hash from result
                    detected_columns=result.column_detection.detected_columns,
                    error_summary=[
                        ImportErrorV1(
                            row=error.row,
                            column=error.column,
                            message=error.message,
                            value=error.value
                        ) for error in result.errors[:10]  # Sample for event
                    ],
                    started_at=result.start_time,
                    completed_at=datetime.now(timezone.utc)
                )
                
                # Broadcast completion event
                # await websocket_manager.broadcast_completion(completion_event)
            
            # Enhanced logging with performance metrics
            logger.info(
                f"Enhanced processing completed for import job {job_id}: "
                f"Status={final_status.value}, "
                f"Contacts={result.successful_contacts}/{result.total_rows}, "
                f"Errors={result.error_count}, "
                f"Detection={result.column_detection.detection_quality}, "
                f"Rate={result.processing_rate:.1f} rows/sec, "
                f"Memory={result.memory_usage_mb:.1f}MB"
            )
            
    except FileNotFoundError:
        logger.error(f"Temporary file not found for import job {job_id}: {temp_file_path}")
        await _handle_enhanced_processing_error(
            job_id, 
            "file_not_found",
            "Temporary file not found - upload may have expired",
            temp_file_path
        )
        
    except ValidationError as e:
        logger.error(f"Validation error processing CSV for import job {job_id}: {str(e)}")
        await _handle_enhanced_processing_error(
            job_id,
            "validation_error", 
            f"CSV validation failed: {str(e)}",
            temp_file_path
        )
        
    except PermissionError:
        logger.error(f"Permission denied accessing file for import job {job_id}: {temp_file_path}")
        await _handle_enhanced_processing_error(
            job_id,
            "permission_error",
            "Permission denied accessing temporary file",
            temp_file_path
        )
        
    except Exception as e:
        logger.error(f"Unexpected error processing CSV for import job {job_id}: {str(e)}")
        await _handle_enhanced_processing_error(
            job_id,
            "system_error",
            f"Processing failed due to unexpected error: {str(e)}",
            temp_file_path
        )
    
    finally:
        # CRITICAL: Always clean up temporary file
        await _cleanup_temp_file(temp_file_path)


async def process_csv_with_mapping_background(
    job_id: str, 
    temp_file_path: str,
    column_mapping: ColumnMapping,
    options: Dict[str, Any]
) -> None:
    """
    Process CSV file with explicit column mapping provided by user.
    
    Args:
        job_id: Import job identifier
        temp_file_path: Path to temporary CSV file
        column_mapping: User-provided column mapping
        options: Processing options
    """
    logger.info(f"Starting mapped processing for import job {job_id}")
    
    try:
        from app.db.session import get_session
        
        async with get_session() as session:
            # Create parser with explicit mapping
            parser = StreamingCSVParser(session)
            
            # Create mapping config for parser
            mapping_config = {
                "phone_columns": column_mapping.phone_columns,
                "name_column": column_mapping.name_column,
                "skip_columns": column_mapping.skip_columns,
                "tag_columns": column_mapping.tag_columns,
                "skip_invalid_phones": options.get("skip_invalid_phones", True),
                "phone_country_default": options.get("phone_country_default", "US")
            }
            
            # Process with explicit mapping
            result = await parser.parse_file_with_mapping(
                file_path=Path(temp_file_path),
                import_job_id=job_id,
                mapping_config=mapping_config,
                progress_callback=None  # You can add progress callback here
            )
            
            # Rest of processing is same as original...
            # (Copy the completion handling from process_csv_background)
            
    except Exception as e:
        logger.error(f"Error in mapped processing for job {job_id}: {str(e)}")
        await _handle_enhanced_processing_error(
            job_id,
            "processing_error",
            f"Processing failed: {str(e)}",
            temp_file_path
        )
    finally:
        await _cleanup_temp_file(temp_file_path)

async def _handle_enhanced_processing_error(
    job_id: str, 
    error_type: str,
    error_message: str, 
    temp_file_path: str
) -> None:
    """
    Enhanced error handling with structured error types and recovery suggestions.
    
    Args:
        job_id: Import job identifier
        error_type: Structured error type for frontend handling
        error_message: Error description for user
        temp_file_path: Path to temporary file for cleanup
    """
    try:
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Create failure event with recovery suggestions
            failure_event = create_failure_event(
                job_id=job_id,
                failure_reason=error_type,
                user_message=error_message,
                rows_processed=0,
                successful_contacts=0,
                started_at=datetime.now(timezone.utc),
                failed_at=datetime.now(timezone.utc),
                technical_details=None,  # Don't expose technical details to users
                error_id=f"err_{job_id}_{int(datetime.now().timestamp())}"
            )
            
            # Update job with structured error data
            await import_repo.complete_job(
                job_id=job_id,
                status=ImportStatus.FAILED,
                rows_processed=0,
                errors=[{
                    "row": 0, 
                    "column": None, 
                    "message": error_message,
                    "value": {
                        "error_type": error_type,
                        "recovery_suggestions": failure_event["recovery_suggestions"]
                    }
                }]
            )
            
            # Broadcast failure event for real-time updates
            # await websocket_manager.broadcast_failure(failure_event)
            
    except Exception as update_error:
        logger.error(f"Failed to update import job status for {job_id}: {str(update_error)}")


# Enhanced estimation function that works with the new parser
def _enhanced_estimate_processing_time(row_count: int, processing_rate: float = None) -> str:
    """
    Enhanced processing time estimation with dynamic rate calculation.
    
    Args:
        row_count: Number of rows to process
        processing_rate: Current processing rate (optional)
        
    Returns:
        str: Human-readable time estimate
    """
    if processing_rate and processing_rate > 0:
        # Use actual processing rate if available
        estimated_seconds = row_count / processing_rate
    else:
        # Fallback to benchmark-based estimates
        if row_count <= 1000:
            estimated_seconds = 30
        elif row_count <= 10000:
            estimated_seconds = 120  # 2 minutes
        elif row_count <= 100000:
            estimated_seconds = 180  # 3 minutes
        elif row_count <= 500000:
            estimated_seconds = 900  # 15 minutes
        else:
            estimated_seconds = 1800  # 30 minutes
    
    # Format the estimate
    if estimated_seconds < 60:
        return f"~{int(estimated_seconds)} seconds"
    elif estimated_seconds < 3600:
        return f"~{int(estimated_seconds / 60)} minutes"
    else:
        return f"~{int(estimated_seconds / 3600)} hours"


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

async def _validate_uploaded_file(file: UploadFile) -> None:
    """
    Comprehensive file validation for security and format compliance.
    
    **Security Validations:**
    - File extension whitelist checking
    - MIME type validation
    - File size limits
    - Filename sanitization
    
    **Format Validations:**
    - CSV header detection
    - Basic structure validation
    - Encoding compatibility check
    
    Args:
        file: Uploaded file to validate
        
    Raises:
        HTTPException: Various validation failures with specific error messages
    """
    # Validate file extension
    if not file.filename:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Filename is required"
        )
    
    file_extension = Path(file.filename).suffix.lower()
    if file_extension not in ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"File type '{file_extension}' not supported. "
                   f"Allowed types: {', '.join(ALLOWED_EXTENSIONS)}"
        )
    
    # Validate MIME type
    if file.content_type and file.content_type not in ALLOWED_MIME_TYPES:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail=f"MIME type '{file.content_type}' not supported. "
                   f"Please upload a CSV file."
        )
    
    # Validate filename contains no dangerous characters
    if any(char in file.filename for char in ['..', '/', '\\', '\0']):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Filename contains invalid characters"
        )


async def _handle_processing_error(job_id: str, error_message: str, temp_file_path: str) -> None:
    """
    Handle processing errors with proper cleanup and status updates.
    
    Args:
        job_id: Import job identifier
        error_message: Error description for user
        temp_file_path: Path to temporary file for cleanup
    """
    try:
        async with get_repository_context(ImportJobRepository) as import_repo:
            await import_repo.complete_job(
                job_id=job_id,
                status=ImportStatus.FAILED,
                rows_processed=0,
                errors=[{"message": error_message, "row": 0, "column": None}]
            )
    except Exception as update_error:
        logger.error(f"Failed to update import job status for {job_id}: {str(update_error)}")


async def _cleanup_temp_file(temp_file_path: str) -> None:
    """
    Safely clean up temporary files with proper error handling.
    
    Args:
        temp_file_path: Path to temporary file to delete
    """
    try:
        if temp_file_path and os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
            logger.info(f"Cleaned up temporary file: {temp_file_path}")
    except OSError as e:
        logger.warning(f"Failed to clean up temp file {temp_file_path}: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error cleaning up temp file {temp_file_path}: {str(e)}")


def _estimate_processing_time(row_count: int) -> str:
    """
    Estimate processing completion time based on row count.
    
    **Performance Estimates (based on Phase 2A benchmarks):**
    - 1,000 rows: ~30 seconds
    - 10,000 rows: ~2 minutes  
    - 100,000 rows: ~3 minutes
    - 1,000,000 rows: ~30 minutes
    
    Args:
        row_count: Number of rows to process
        
    Returns:
        str: Human-readable time estimate
    """
    if row_count <= 1000:
        return "~30 seconds"
    elif row_count <= 10000:
        return "~2 minutes"
    elif row_count <= 100000:
        return "~3 minutes"
    elif row_count <= 500000:
        return "~15 minutes"
    else:
        return "~30 minutes"


def _log_progress(job_id: str, progress: Dict[str, Any]) -> None:
    """
    Log processing progress for monitoring and debugging.
    
    Args:
        job_id: Import job identifier
        progress: Progress information from parser
    """
    try:
        if progress["type"] is not ImportEventType.PROGRESS:
            return

        logger.info(
            "Import %s: %5.1f%%  processed=%s  ok=%s  errors=%s",
            job_id,
            progress["percent"],
            progress["processed"],
            progress["successful"],
            len(progress["errors"]),
        )
    except Exception as e:                # noqa: BLE001
        logger.warning("Progress logger swallowed error: %s", e)


# Enhanced progress logging function
def _enhanced_log_progress(job_id: str, progress: ImportProgressV1) -> None:
    """
    Enhanced progress logging with structured data and performance metrics.
    
    Args:
        job_id: Import job identifier
        progress: Enhanced progress information
    """
    try:
        if progress["type"] != ImportEventType.PROGRESS:
            return

        # Enhanced logging with performance metrics
        logger.info(
            "Import %s: %5.1f%% | %s/%s rows | ✓%s contacts | ✗%s errors | %s rows/sec | %sMB | ETA: %s",
            job_id,
            progress["percent"],
            progress["processed"],
            progress["total_rows"],
            progress["successful"],
            progress["error_count"],
            progress.get("processing_rate", 0),
            progress.get("memory_usage_mb", 0),
            progress.get("estimated_completion", "Unknown")
        )
        
        # Log any critical issues
        if progress["has_critical_errors"]:
            logger.warning(f"Import {job_id}: Critical error threshold reached!")
        
        # Log recent errors for debugging
        if progress["errors"]:
            logger.debug(f"Import {job_id}: Recent errors: {len(progress['errors'])}")
            for error in progress["errors"][:3]:  # Log first 3 errors
                logger.debug(f"  Row {error['row']}: {error['message']}")
                
    except Exception as e:
        logger.warning(f"Enhanced progress logger error for {job_id}: {str(e)}")


async def _get_csv_preview_data(sha256: str, preview_rows: int = 5) -> Dict[str, Any]:
    """
    Get preview data from CSV file using SHA256 hash.
    
    Args:
        sha256: File hash to locate the file
        preview_rows: Number of rows to preview
        
    Returns:
        Dict with headers, sample rows, and column analysis
    """
    try:
        # Find the temp file by hash
        temp_file_path = _get_temp_file_path(sha256)
        if not temp_file_path:
            logger.warning(f"Temp file not found for hash {sha256}")
            return {
                "headers": [],
                "rows": [],
                "samples": {},
                "empty_counts": {}
            }
        
        headers = []
        rows = []
        samples = {}
        empty_counts = {}
        
        with open(temp_file_path, 'r', encoding='utf-8') as f:
            # Detect delimiter
            sample = f.read(8192)
            f.seek(0)
            
            sniffer = csv.Sniffer()
            try:
                delimiter = sniffer.sniff(sample, delimiters=',\t|;').delimiter
            except csv.Error:
                delimiter = ','
            
            # Read CSV
            reader = csv.DictReader(f, delimiter=delimiter)
            headers = reader.fieldnames or []
            
            # Initialize data structures
            for header in headers:
                samples[header] = []
                empty_counts[header] = 0
            
            # Read preview rows and collect samples
            row_count = 0
            for row in reader:
                if row_count < preview_rows:
                    rows.append(row)
                
                # Collect samples for column analysis (up to 100 rows)
                if row_count < 100:
                    for header in headers:
                        value = row.get(header, '').strip()
                        if value:
                            samples[header].append(value)
                        else:
                            empty_counts[header] += 1
                
                row_count += 1
                if row_count >= 100:  # Stop after 100 rows for sampling
                    break
        
        # Limit samples to 10 per column for response size
        for header in headers:
            if len(samples[header]) > 10:
                samples[header] = samples[header][:10]
        
        return {
            "headers": headers,
            "rows": rows,
            "samples": samples,
            "empty_counts": empty_counts
        }
        
    except Exception as e:
        logger.error(f"Error reading CSV preview data: {str(e)}")
        return {
            "headers": [],
            "rows": [],
            "samples": {},
            "empty_counts": {}
        }

def _detect_column_type(column_name: str, sample_values: List[str]) -> str:
    """
    Detect the likely data type of a column based on name and sample values.
    
    Args:
        column_name: Name of the column
        sample_values: Sample values from the column
        
    Returns:
        str: Detected type (phone, name, email, text, number)
    """
    col_lower = column_name.lower()
    
    # Check column name patterns
    if any(x in col_lower for x in ['phone', 'mobile', 'cell', 'tel']):
        return "phone"
    elif any(x in col_lower for x in ['name', 'contact', 'person']):
        return "name"
    elif any(x in col_lower for x in ['email', 'mail']):
        return "email"
    
    # Check sample values
    if sample_values:
        # Check if mostly numbers
        numeric_count = sum(1 for v in sample_values if v.replace('.', '').replace('-', '').isdigit())
        if numeric_count > len(sample_values) * 0.8:
            return "number"
    
    return "text"


def _get_temp_file_path(sha256: str) -> Optional[str]:
    """
    Get temp file path from SHA256 hash.
    
    Args:
        sha256: File hash
        
    Returns:
        Path to temp file or None if not found
    """
    # Look for file in temp directory
    # This is a simplified version - in production you might use a cache
    temp_dir = Path(TEMP_DIR)
    for file_path in temp_dir.glob(f"{TEMP_FILE_PREFIX}*.csv"):
        # Calculate hash of this file
        file_hash = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b""):
                file_hash.update(chunk)
        
        if file_hash.hexdigest() == sha256:
            return str(file_path)
    
    return None
</file>

<file path="app/db/repositories/metrics.py">
"""
Repository for metrics operations.
"""
from datetime import datetime, timezone, date, timedelta
from typing import List, Optional, Dict, Any, Tuple

from sqlalchemy import select, update, and_, func, desc
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.dialects.postgresql import insert

from app.db.repositories.base import BaseRepository
from app.models.metrics import UserMetrics


class MetricsRepository(BaseRepository[UserMetrics, Dict[str, Any], Dict[str, Any]]):
    """Repository for metrics operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and UserMetrics model."""
        super().__init__(session=session, model=UserMetrics)
    
    async def get_or_create_for_day(
        self,
        *,
        user_id: str,
        day: date
    ) -> UserMetrics:
        """
        Get or create metrics for a specific user and day.
        
        Args:
            user_id: User ID
            day: Date for metrics
            
        Returns:
            UserMetrics: Metrics for the specified day
        """
        # Check if metrics exist for this user and day
        query = select(UserMetrics).where(
            and_(
                UserMetrics.user_id == user_id,
                UserMetrics.date == day
            )
        )
        result = await self.session.execute(query)
        metrics = result.scalar_one_or_none()
        
        if metrics:
            return metrics
        
        # Create new metrics if not found
        metrics = UserMetrics(
            user_id=user_id,
            date=day,
            messages_sent=0,
            messages_delivered=0,
            messages_failed=0,
            messages_scheduled=0,
            campaigns_created=0,
            campaigns_completed=0,
            campaigns_active=0,
            templates_created=0,
            templates_used=0,
            quota_total=1000,  # Default quota
            quota_used=0
        )
        
        self.session.add(metrics)
        
        return metrics
    
    async def increment_metric(
        self,
        *,
        user_id: str,
        date: date,
        metric_name: str,
        increment: int = 1
    ) -> Optional[UserMetrics]:
        """
        Increment a specific metric for a user on a specific day.
        
        Args:
            user_id: User ID
            date: Date for metrics
            metric_name: Name of metric to increment
            increment: Amount to increment by
            
        Returns:
            UserMetrics: Updated metrics or None if error
        """
        # Get or create metrics for this day
        metrics = await self.get_or_create_for_day(user_id=user_id, day=date)
        
        # Update specific metric
        if hasattr(metrics, metric_name):
            current_value = getattr(metrics, metric_name)
            setattr(metrics, metric_name, current_value + increment)
            
            # Also update quota_used if incrementing sent messages
            if metric_name == "messages_sent":
                metrics.quota_used += increment
            
            self.session.add(metrics)
            
            return metrics
        
        return None
    
    async def get_metrics_for_day(
        self,
        *,
        user_id: str,
        day: date
    ) -> Optional[UserMetrics]:
        """
        Get metrics for a specific user and day.
        
        Args:
            user_id: User ID
            day: Date for metrics
            
        Returns:
            UserMetrics: Metrics for the specified day or None if not found
        """
        query = select(UserMetrics).where(
            and_(
                UserMetrics.user_id == user_id,
                UserMetrics.date == day
            )
        )
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def get_metrics_range(
        self,
        *,
        user_id: str,
        start_date: date,
        end_date: date
    ) -> List[UserMetrics]:
        """
        Get metrics for a specific user over a date range.
        
        Args:
            user_id: User ID
            start_date: Start date (inclusive)
            end_date: End date (inclusive)
            
        Returns:
            List[UserMetrics]: List of metrics for the date range
        """
        query = select(UserMetrics).where(
            and_(
                UserMetrics.user_id == user_id,
                UserMetrics.date >= start_date,
                UserMetrics.date <= end_date
            )
        ).order_by(UserMetrics.date)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def get_monthly_metrics(
        self,
        *,
        user_id: str,
        year: int,
        month: int
    ) -> List[UserMetrics]:
        """
        Get metrics for a specific user for a full month.
        
        Args:
            user_id: User ID
            year: Year
            month: Month (1-12)
            
        Returns:
            List[UserMetrics]: List of metrics for the month
        """
        # Calculate start and end date for the month
        start_date = date(year, month, 1)
        if month == 12:
            end_date = date(year + 1, 1, 1) - timedelta(days=1)
        else:
            end_date = date(year, month + 1, 1) - timedelta(days=1)
        
        return await self.get_metrics_range(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
    
    async def get_summary_metrics(
        self,
        *,
        user_id: str,
        start_date: date,
        end_date: date
    ) -> Dict[str, Any]:
        """
        Get summarized metrics for a specific user over a date range.
        
        Args:
            user_id: User ID
            start_date: Start date (inclusive)
            end_date: End date (inclusive)
            
        Returns:
            Dict[str, Any]: Summarized metrics
        """
        # Get metrics for the date range
        metrics_list = await self.get_metrics_range(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
        
        # Calculate summary metrics
        total_sent = sum(m.messages_sent for m in metrics_list)
        total_delivered = sum(m.messages_delivered for m in metrics_list)
        total_failed = sum(m.messages_failed for m in metrics_list)
        
        # Calculate delivery rate
        delivery_rate = 0
        if total_sent > 0:
            delivery_rate = (total_delivered / total_sent) * 100
        
        # Get latest quota information
        quota_used = metrics_list[-1].quota_used if metrics_list else 0
        quota_total = metrics_list[-1].quota_total if metrics_list else 1000
        
        # Create summary
        return {
            "period": {
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat()
            },
            "messages": {
                "sent": total_sent,
                "delivered": total_delivered,
                "failed": total_failed,
                "delivery_rate": round(delivery_rate, 1)
            },
            "campaigns": {
                "created": sum(m.campaigns_created for m in metrics_list),
                "completed": sum(m.campaigns_completed for m in metrics_list),
                "active": metrics_list[-1].campaigns_active if metrics_list else 0
            },
            "templates": {
                "created": sum(m.templates_created for m in metrics_list),
                "used": sum(m.templates_used for m in metrics_list)
            },
            "quota": {
                "used": quota_used,
                "total": quota_total,
                "percent": round((quota_used / quota_total) * 100, 1) if quota_total > 0 else 0
            }
        }
    
    async def update_daily_metrics(self, day: Optional[date] = None) -> int:
        """
        Update metrics for all users for a specific day based on actual data.
        This is useful for background jobs that rebuild metrics.
        
        Args:
            day: Date to update metrics for (defaults to yesterday)
            
        Returns:
            int: Number of users updated
        """
        from app.db.repositories.messages import MessageRepository
        from app.db.repositories.campaigns import CampaignRepository
        from app.db.repositories.templates import TemplateRepository
        from app.db.repositories.users import UserRepository
        
        # Default to yesterday if no day provided
        if day is None:
            day = date.today() - timedelta(days=1)
        
        # Get all active users
        async with self.session.begin_nested():
            # Get all users
            query = select("*").select_from(UserRepository.model)
            result = await self.session.execute(query)
            users = result.fetchall()
            
            updates_count = 0
            
            # For each user, calculate and store metrics
            for user_row in users:
                user_id = user_row[0]  # Assuming id is the first column
                
                # Calculate message metrics
                msg_query = select(
                    MessageRepository.model.status,
                    func.count(MessageRepository.model.id)
                ).where(
                    and_(
                        MessageRepository.model.user_id == user_id,
                        MessageRepository.model.created_at >= datetime.combine(day, datetime.min.time()),
                        MessageRepository.model.created_at < datetime.combine(day + timedelta(days=1), datetime.min.time())
                    )
                ).group_by(MessageRepository.model.status)
                
                msg_result = await self.session.execute(msg_query)
                msg_counts = dict(msg_result.fetchall())
                
                # Calculate campaign metrics
                campaign_query = select(
                    CampaignRepository.model.status,
                    func.count(CampaignRepository.model.id)
                ).where(
                    CampaignRepository.model.user_id == user_id
                ).group_by(CampaignRepository.model.status)
                
                campaign_result = await self.session.execute(campaign_query)
                campaign_counts = dict(campaign_result.fetchall())
                
                # Calculate template metrics for the day
                template_created_query = select(
                    func.count(TemplateRepository.model.id)
                ).where(
                    and_(
                        TemplateRepository.model.user_id == user_id,
                        TemplateRepository.model.created_at >= datetime.combine(day, datetime.min.time()),
                        TemplateRepository.model.created_at < datetime.combine(day + timedelta(days=1), datetime.min.time())
                    )
                )
                
                template_created_result = await self.session.execute(template_created_query)
                templates_created = template_created_result.scalar_one_or_none() or 0
                
                # Get or create metrics for this day
                metrics = await self.get_or_create_for_day(user_id=user_id, day=day)
                
                # Update metrics with actual values
                metrics.messages_sent = msg_counts.get('sent', 0)
                metrics.messages_delivered = msg_counts.get('delivered', 0)
                metrics.messages_failed = msg_counts.get('failed', 0)
                metrics.messages_scheduled = msg_counts.get('scheduled', 0)
                
                metrics.campaigns_created = sum(1 for status, count in campaign_counts.items() 
                                            if campaign_counts.get('created_at') and campaign_counts['created_at'] >= day)
                metrics.campaigns_completed = campaign_counts.get('completed', 0) + campaign_counts.get('cancelled', 0)
                metrics.campaigns_active = campaign_counts.get('active', 0)
                
                metrics.templates_created = templates_created
                
                # Calculate quota used based on messages sent
                metrics.quota_used = msg_counts.get('sent', 0) + msg_counts.get('delivered', 0) + msg_counts.get('failed', 0)
                
                # Save updated metrics
                self.session.add(metrics)
                updates_count += 1
        
        
        return updates_count
</file>

<file path="app/main.py">
"""
Main FastAPI application entry point for Inboxerr Backend.
"""
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging

from app.api.router import api_router
from app.core.config import settings
from app.core.exceptions import InboxerrException
from app.core.events import startup_event_handler, shutdown_event_handler

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("inboxerr")

# Create FastAPI app
app = FastAPI(
    title=settings.PROJECT_NAME,
    description=settings.PROJECT_DESCRIPTION,
    version=settings.VERSION,
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json",
)

# Set up CORS middleware
if settings.BACKEND_CORS_ORIGINS:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Register event handlers
app.add_event_handler("startup", startup_event_handler)
app.add_event_handler("shutdown", shutdown_event_handler)

# Register exception handlers
@app.exception_handler(InboxerrException)
async def inboxerr_exception_handler(request: Request, exc: InboxerrException):
    """Custom exception handler for InboxerrException."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "status": "error",
            "code": exc.code,
            "message": exc.message,
            "details": exc.details,
        },
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Enhanced HTTP exception handler with consistent format."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "status": "error",
            "code": f"HTTP_{exc.status_code}",
            "message": exc.detail,
            "details": None,
        },
    )

# Register routers
app.include_router(api_router, prefix=settings.API_PREFIX)

# Root endpoint
@app.get("/", tags=["Health"])
async def root():
    """Root endpoint for health checks."""
    return {
        "status": "ok",
        "service": settings.PROJECT_NAME,
        "version": settings.VERSION,
    }

if __name__ == "__main__":
    # For debugging only - use uvicorn for production
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="app/models/contact.py">
"""
Database model for contacts imported from CSV files.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text, UniqueConstraint
from sqlalchemy.orm import relationship

from app.models.base import Base


class Contact(Base):
    """Model for storing contacts imported from CSV files."""
    
    # Import tracking
    import_id = Column(String, ForeignKey("importjob.id"), nullable=False, index=True)
    
    # Contact information
    phone = Column(String, nullable=False, index=True)  # Phone number in E.164 format
    name = Column(String, nullable=True)  # Contact name
    
    # Additional contact data
    tags = Column(JSON, nullable=True, default=list)  # Array of tags for categorization
    
    # CSV row metadata for debugging
    csv_row_number = Column(Integer, nullable=True)  # Original row number in CSV
    raw_data = Column(JSON, nullable=True)  # Store original CSV row data
    
    # Relationships
    import_job = relationship("ImportJob", back_populates="contacts")
    
    # Constraints - ensure unique phone per import
    __table_args__ = (
        UniqueConstraint('import_id', 'phone', name='uix_import_phone'),
    )
    
    # Helper properties
    @property
    def display_name(self) -> str:
        """Get display name, falling back to phone if name is empty."""
        return self.name if self.name else self.phone
    
    @property
    def formatted_phone(self) -> str:
        """Get formatted phone number for display."""
        # Basic formatting - can be enhanced later
        if self.phone.startswith('+1') and len(self.phone) == 12:
            # US number formatting: +1 (555) 123-4567
            return f"+1 ({self.phone[2:5]}) {self.phone[5:8]}-{self.phone[8:]}"
        return self.phone
    
    def add_tag(self, tag: str) -> None:
        """Add a tag to the contact."""
        if self.tags is None:
            self.tags = []
        if tag not in self.tags:
            self.tags.append(tag)
    
    def remove_tag(self, tag: str) -> None:
        """Remove a tag from the contact."""
        if self.tags and tag in self.tags:
            self.tags.remove(tag)
</file>

<file path="app/models/import_job.py">
"""
Database model for import job tracking.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any
from enum import Enum

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text, Enum as SQLEnum
from sqlalchemy.orm import relationship

from app.models.base import Base


class ImportStatus(str, Enum):
    """Import job status enum."""
    PROCESSING = "processing"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ImportJob(Base):
    """Model for tracking CSV import jobs and their progress."""
    
    # Job identification and status
    status = Column(SQLEnum(ImportStatus), nullable=False, default=ImportStatus.PROCESSING, index=True)
    
    # Progress tracking
    rows_total = Column(Integer, default=0, nullable=False)
    rows_processed = Column(Integer, default=0, nullable=False)
    
    # Error tracking - JSONB with {row, column, message} objects
    errors = Column(JSON, nullable=True, default=list)
    
    # File integrity and metadata
    sha256 = Column(String, nullable=True, index=True)  # SHA-256 hash of uploaded file
    filename = Column(String, nullable=True)  # Original filename
    file_size = Column(Integer, nullable=True)  # File size in bytes
    
    # Processing metadata
    started_at = Column(DateTime(timezone=True), nullable=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Ownership
    owner_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    
    # Relationships
    owner = relationship("User")
    contacts = relationship("Contact", back_populates="import_job", cascade="all, delete-orphan")
    
    # Helper properties
    @property
    def progress_percentage(self) -> float:
        """Calculate the import progress percentage."""
        if self.rows_total == 0:
            return 0
        return round((self.rows_processed / self.rows_total) * 100, 2)
    
    @property
    def has_errors(self) -> bool:
        """Check if the import has any errors."""
        return self.errors is not None and len(self.errors) > 0
    
    @property
    def error_count(self) -> int:
        """Get the total number of errors."""
        return len(self.errors) if self.errors else 0
</file>

<file path="app/schemas/contact.py">
"""
Pydantic schemas for contact-related API operations.
"""
from typing import List, Optional, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field, validator


class ContactBase(BaseModel):
    """Base schema for contact data."""
    phone: str = Field(..., description="Phone number in E.164 format")
    name: Optional[str] = Field(None, description="Contact name")
    tags: Optional[List[str]] = Field(default=[], description="List of tags for categorization")


class ContactCreate(ContactBase):
    """Schema for creating a new contact."""
    import_id: str = Field(..., description="Import job ID this contact belongs to")
    csv_row_number: Optional[int] = Field(None, description="Original row number in CSV")
    raw_data: Optional[Dict[str, Any]] = Field(None, description="Original CSV row data")
    
    @validator("phone")
    def validate_phone_number(cls, v):
        """Validate phone number format."""
        if not v or not (v.startswith("+") and len(v) >= 8):
            raise ValueError("Phone number must be in E.164 format (e.g. +1234567890)")
        # Additional validation can be added here
        if len(v) > 20:  # Reasonable max length for international numbers
            raise ValueError("Phone number is too long")
        return v
    
    @validator("name")
    def validate_name(cls, v):
        """Validate contact name."""
        if v is not None:
            v = v.strip()
            if len(v) == 0:
                return None  # Convert empty string to None
            if len(v) > 100:
                raise ValueError("Contact name is too long (max 100 characters)")
        return v
    
    @validator("tags")
    def validate_tags(cls, v):
        """Validate tags list."""
        if v is not None:
            # Remove empty tags and duplicates
            v = list(set([tag.strip() for tag in v if tag.strip()]))
            if len(v) > 20:  # Reasonable limit
                raise ValueError("Too many tags (max 20)")
        return v or []


class ContactUpdate(BaseModel):
    """Schema for updating a contact."""
    name: Optional[str] = Field(None, description="Contact name")
    tags: Optional[List[str]] = Field(None, description="List of tags for categorization")
    
    @validator("name")
    def validate_name(cls, v):
        """Validate contact name."""
        if v is not None:
            v = v.strip()
            if len(v) == 0:
                return None
            if len(v) > 100:
                raise ValueError("Contact name is too long (max 100 characters)")
        return v
    
    @validator("tags")
    def validate_tags(cls, v):
        """Validate tags list."""
        if v is not None:
            v = list(set([tag.strip() for tag in v if tag.strip()]))
            if len(v) > 20:
                raise ValueError("Too many tags (max 20)")
        return v


class ContactResponse(ContactBase):
    """Schema for contact response."""
    id: str = Field(..., description="Contact ID")
    import_id: str = Field(..., description="Import job ID this contact belongs to")
    csv_row_number: Optional[int] = Field(None, description="Original row number in CSV")
    raw_data: Optional[Dict[str, Any]] = Field(None, description="Original CSV row data")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    
    # Computed fields
    display_name: str = Field(..., description="Display name (name or phone)")
    formatted_phone: str = Field(..., description="Formatted phone number for display")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class ContactSummary(BaseModel):
    """Schema for contact summary (lightweight response)."""
    id: str = Field(..., description="Contact ID")
    phone: str = Field(..., description="Phone number")
    name: Optional[str] = Field(None, description="Contact name")
    display_name: str = Field(..., description="Display name")
    tags: List[str] = Field(..., description="Contact tags")
    created_at: datetime = Field(..., description="Creation timestamp")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class ContactBulkCreate(BaseModel):
    """Schema for bulk contact creation."""
    import_id: str = Field(..., description="Import job ID")
    contacts: List[ContactCreate] = Field(..., description="List of contacts to create")
    
    @validator("contacts")
    def validate_contacts(cls, v):
        """Validate contacts list."""
        if not v:
            raise ValueError("Contacts list cannot be empty")
        if len(v) > 10000:  # Reasonable batch limit
            raise ValueError("Too many contacts in single batch (max 10,000)")
        
        # Check for duplicate phone numbers within the batch
        phones = [contact.phone for contact in v]
        if len(phones) != len(set(phones)):
            raise ValueError("Duplicate phone numbers found in batch")
        
        return v


class ContactBulkResponse(BaseModel):
    """Schema for bulk contact creation response."""
    import_id: str = Field(..., description="Import job ID")
    total: int = Field(..., description="Total contacts processed")
    created: int = Field(..., description="Number of contacts created")
    skipped: int = Field(..., description="Number of contacts skipped (duplicates)")
    errors: int = Field(..., description="Number of contacts with errors")
    error_details: List[Dict[str, Any]] = Field(default=[], description="Details of any errors")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class ContactSearchFilter(BaseModel):
    """Schema for contact search filters."""
    import_id: Optional[str] = Field(None, description="Filter by import job ID")
    name: Optional[str] = Field(None, description="Search by name (partial match)")
    phone: Optional[str] = Field(None, description="Search by phone number (partial match)")
    tags: Optional[List[str]] = Field(None, description="Filter by tags (OR operation)")
    created_after: Optional[datetime] = Field(None, description="Filter contacts created after this date")
    created_before: Optional[datetime] = Field(None, description="Filter contacts created before this date")
    
    class Config:
        """Pydantic config."""
        schema_extra = {
            "example": {
                "import_id": "import_123",
                "name": "John",
                "tags": ["vip", "customer"],
                "created_after": "2024-01-01T00:00:00Z"
            }
        }
</file>

<file path="app/schemas/import_job.py">
"""
Pydantic schemas for import job-related API operations.
"""
from typing import List, Optional, Dict, Any, Literal
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field, validator


class ImportStatus(str, Enum):
    """Import job status enum."""
    PROCESSING = "processing"
    SUCCESS = "success"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ImportJobBase(BaseModel):
    """Base schema for import job data."""
    filename: Optional[str] = Field(None, description="Original filename")
    file_size: Optional[int] = Field(None, description="File size in bytes")


class ImportJobCreate(ImportJobBase):
    """Schema for creating a new import job."""
    sha256: Optional[str] = Field(None, description="SHA-256 hash of uploaded file")
    
    @validator("file_size")
    def validate_file_size(cls, v):
        """Validate file size limits."""
        if v is not None:
            if v <= 0:
                raise ValueError("File size must be greater than 0")
            if v > 100 * 1024 * 1024:  # 100MB limit
                raise ValueError("File size exceeds 100MB limit")
        return v


class ImportJobUpdate(BaseModel):
    """Schema for updating an import job."""
    status: Optional[ImportStatus] = Field(None, description="Import job status")
    rows_total: Optional[int] = Field(None, description="Total number of rows to process")
    rows_processed: Optional[int] = Field(None, description="Number of rows processed")
    errors: Optional[List[Dict[str, Any]]] = Field(None, description="List of error objects")
    started_at: Optional[datetime] = Field(None, description="Processing start time")
    completed_at: Optional[datetime] = Field(None, description="Processing completion time")
    
    @validator("rows_total", "rows_processed")
    def validate_rows(cls, v):
        """Validate row counts."""
        if v is not None and v < 0:
            raise ValueError("Row count cannot be negative")
        return v


class ImportJobResponse(ImportJobBase):
    """Schema for import job response."""
    id: str = Field(..., description="Import job ID")
    status: ImportStatus = Field(..., description="Import job status")
    rows_total: int = Field(..., description="Total number of rows to process")
    rows_processed: int = Field(..., description="Number of rows processed")
    errors: Optional[List[Dict[str, Any]]] = Field(default=[], description="List of error objects")
    sha256: Optional[str] = Field(None, description="SHA-256 hash of uploaded file")
    started_at: Optional[datetime] = Field(None, description="Processing start time")
    completed_at: Optional[datetime] = Field(None, description="Processing completion time")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    owner_id: str = Field(..., description="User who created the import job")
    
    # Computed fields
    progress_percentage: float = Field(0, description="Import progress percentage")
    has_errors: bool = Field(False, description="Whether the import has errors")
    error_count: int = Field(0, description="Total number of errors")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class ImportJobProgress(BaseModel):
    """Schema for import job progress tracking."""
    id: str = Field(..., description="Import job ID")
    status: ImportStatus = Field(..., description="Current status")
    progress_percentage: float = Field(..., description="Progress percentage (0-100)")
    rows_processed: int = Field(..., description="Number of rows processed")
    rows_total: int = Field(..., description="Total number of rows")
    error_count: int = Field(..., description="Number of errors encountered")
    estimated_completion: Optional[datetime] = Field(None, description="Estimated completion time")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class ImportJobSummary(BaseModel):
    """Schema for import job summary (lightweight response)."""
    id: str = Field(..., description="Import job ID")
    filename: Optional[str] = Field(None, description="Original filename")
    status: ImportStatus = Field(..., description="Import job status")
    progress_percentage: float = Field(..., description="Progress percentage")
    created_at: datetime = Field(..., description="Creation timestamp")
    rows_total: int = Field(..., description="Total rows")
    rows_processed: int = Field(..., description="Processed rows")
    error_count: int = Field(..., description="Error count")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class ImportError(BaseModel):
    """Schema for individual import errors."""
    row: int = Field(..., description="Row number where error occurred")
    column: Optional[str] = Field(None, description="Column name where error occurred")
    message: str = Field(..., description="Error message")
    value: Optional[str] = Field(None, description="The value that caused the error")
    
    class Config:
        """Pydantic config."""
        schema_extra = {
            "example": {
                "row": 25,
                "column": "phone_number",
                "message": "Invalid phone number format",
                "value": "123-456-7890"
            }
        }


class ColumnInfo(BaseModel):
    """Information about a CSV column for preview."""
    name: str = Field(..., description="Column name from CSV header")
    index: int = Field(..., description="Column index (0-based)")
    sample_values: List[str] = Field(..., description="Sample non-empty values from this column")
    empty_count: int = Field(..., description="Number of empty cells in sample")
    detected_type: str = Field(..., description="Detected data type (phone, name, email, text, number)")
    
class MappingSuggestion(BaseModel):
    """Suggestion for column mapping."""
    column: str = Field(..., description="Column name")
    confidence: float = Field(..., description="Confidence score (0-100)")
    reason: str = Field(..., description="Why this column was suggested")

class ImportPreviewResponse(BaseModel):
    """Response schema for import job preview."""
    job_id: str = Field(..., description="Import job ID")
    file_info: Dict[str, Any] = Field(..., description="File metadata")
    columns: List[ColumnInfo] = Field(..., description="Column information")
    preview_rows: List[Dict[str, str]] = Field(..., description="First 5 rows of data")
    suggestions: Dict[str, List[MappingSuggestion]] = Field(
        ..., 
        description="Mapping suggestions for phone, name, etc."
    )
    confidence_level: Literal["high", "medium", "low"] = Field(
        ..., 
        description="Overall confidence in auto-detection"
    )
    auto_process_recommended: bool = Field(
        ...,
        description="Whether auto-processing is recommended based on confidence"
    )
    messages: List[str] = Field(
        default=[],
        description="User guidance messages"
    )

class ColumnMapping(BaseModel):
    """Schema for explicit column mapping."""
    phone_columns: List[str] = Field(
        ..., 
        description="Column names containing phone numbers",
        min_items=1
    )
    name_column: Optional[str] = Field(
        None,
        description="Column name containing contact names"
    )
    skip_columns: List[str] = Field(
        default=[],
        description="Columns to ignore during import"
    )
    tag_columns: List[str] = Field(
        default=[],
        description="Columns to import as tags"
    )

class ProcessImportRequest(BaseModel):
    """Request schema for processing import with mapping."""
    column_mapping: ColumnMapping = Field(
        ...,
        description="How to map CSV columns to contact fields"
    )
    options: Dict[str, Any] = Field(
        default={
            "skip_invalid_phones": True,
            "merge_duplicate_phones": True,
            "phone_country_default": "US"
        },
        description="Processing options"
    )
    
    class Config:
        """Pydantic config."""
        schema_extra = {
            "example": {
                "column_mapping": {
                    "phone_columns": ["Phone 1", "Phone 2"],
                    "name_column": "First Name",
                    "skip_columns": ["Index"],
                    "tag_columns": ["Company", "Source"]
                },
                "options": {
                    "skip_invalid_phones": True,
                    "phone_country_default": "US"
                }
            }
        }
</file>

<file path="app/schemas/template.py">
# app/schemas/template.py
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from pydantic import BaseModel, Field, validator


class MessageTemplateBase(BaseModel):
    """Base schema for message templates."""
    name: str = Field(..., description="Template name")
    content: str = Field(..., description="Template content with placeholders")
    description: Optional[str] = Field(None, description="Template description")
    is_active: bool = Field(True, description="Whether the template is active")


class MessageTemplateCreate(MessageTemplateBase):
    """Schema for creating a new message template."""
    variables: Optional[List[str]] = Field(default=[], description="List of variables in the template")
    
    @validator("content")
    def validate_content(cls, v):
        """Validate template content."""
        if not v or len(v.strip()) == 0:
            raise ValueError("Template content cannot be empty")
        if len(v) > 1600:  # Max length for multi-part SMS
            raise ValueError("Template exceeds maximum length of 1600 characters")
        return v
    
    @validator("variables", pre=True)
    def validate_variables(cls, v, values):
        """Extract variables from content if not provided."""
        import re
        
        if not v and "content" in values:
            # Extract variables like {{variable_name}} from content
            pattern = r"{{([a-zA-Z0-9_]+)}}"
            matches = re.findall(pattern, values["content"])
            if matches:
                return list(set(matches))  # Return unique variables
        return v or []


class MessageTemplateUpdate(BaseModel):
    """Schema for updating a message template."""
    name: Optional[str] = Field(None, description="Template name")
    content: Optional[str] = Field(None, description="Template content with placeholders")
    description: Optional[str] = Field(None, description="Template description")
    is_active: Optional[bool] = Field(None, description="Whether the template is active")
    variables: Optional[List[str]] = Field(None, description="List of variables in the template")
    
    @validator("content")
    def validate_content(cls, v):
        """Validate template content if provided."""
        if v is not None:
            if len(v.strip()) == 0:
                raise ValueError("Template content cannot be empty")
            if len(v) > 1600:
                raise ValueError("Template exceeds maximum length of 1600 characters")
        return v


class MessageTemplateResponse(MessageTemplateBase):
    """Schema for message template response."""
    id: str = Field(..., description="Template ID")
    variables: List[str] = Field(..., description="List of variables in the template")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    user_id: str = Field(..., description="User who created the template")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class MessageWithTemplate(BaseModel):
    """Schema for sending a message using a template."""
    template_id: str = Field(..., description="Template ID")
    phone_number: str = Field(..., description="Recipient phone number in E.164 format")
    variables: Dict[str, str] = Field(..., description="Values for template variables")
    scheduled_at: Optional[datetime] = Field(None, description="Schedule message for future delivery")
    custom_id: Optional[str] = Field(None, description="Custom ID for tracking")
    
    @validator("phone_number")
    def validate_phone_number(cls, v):
        """Validate phone number format."""
        if not v or not (v.startswith("+") and len(v) >= 8):
            raise ValueError("Phone number must be in E.164 format (e.g. +1234567890)")
        return v
</file>

<file path="app/scripts/create_admin.py">
# scripts/create_admin.py
import asyncio
import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from app.db.session import async_session_factory, initialize_database
from app.db.repositories.users import UserRepository
from app.core.security import get_password_hash

async def create_admin_user():
    """Create an admin user if none exists."""
    # Initialize database
    await initialize_database()
    
    # Use session properly with context manager
    async with async_session_factory() as session:
        # Create repository with session
        user_repo = UserRepository(session)
        
        # Check if admin exists
        admin = await user_repo.get_by_email("admin@inboxerr.com")
        
        if admin:
            print("Admin user already exists")
            return
        
        # Create admin user
        password = "Admin123!"
        hashed_password = get_password_hash(password)
        
        admin = await user_repo.create(
            email="admin@inboxerr.com",
            hashed_password=hashed_password,
            full_name="Admin User",
            is_active=True,
            role="admin"
        )
        
        print(f"Admin user created with ID: {admin.id}")
        print(f"Email: admin@inboxerr.com")
        print(f"Password: {password}")

if __name__ == "__main__":
    asyncio.run(create_admin_user())
</file>

<file path="app/utils/datetime.py">
"""
Utilities for standardized datetime handling.
"""
from datetime import datetime, timedelta, timezone
from typing import Optional, Union
import re

def utc_now() -> datetime:
    """
    Get current UTC time as timezone-aware datetime.
    
    Returns:
        datetime: Current UTC time
    """
    return datetime.now(timezone.utc)

def format_datetime(dt: Optional[datetime] = None) -> str:
    """
    Format datetime as ISO 8601 string.
    
    Args:
        dt: Datetime to format (defaults to current UTC time)
        
    Returns:
        str: ISO 8601 formatted string
    """
    if dt is None:
        dt = utc_now()
    
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt.isoformat()

def parse_datetime(date_string: str) -> Optional[datetime]:
    """
    Parse datetime from string.
    
    Args:
        date_string: Datetime string to parse
        
    Returns:
        datetime: Parsed datetime or None if invalid
    """
    try:
        # Handle common formats
        if 'T' in date_string:
            # ISO format
            if date_string.endswith('Z'):
                date_string = date_string[:-1] + '+00:00'
            return datetime.fromisoformat(date_string)
        else:
            # Try common date formats
            date_patterns = [
                # YYYY-MM-DD
                r'^(\d{4})-(\d{2})-(\d{2})$',
                # MM/DD/YYYY
                r'^(\d{1,2})/(\d{1,2})/(\d{4})$',
                # DD/MM/YYYY
                r'^(\d{1,2})-(\d{1,2})-(\d{4})$',
            ]
            
            for pattern in date_patterns:
                match = re.match(pattern, date_string)
                if match:
                    if pattern == date_patterns[0]:
                        year, month, day = match.groups()
                    else:
                        if pattern == date_patterns[1]:
                            month, day, year = match.groups()
                        else:
                            day, month, year = match.groups()
                    
                    return datetime(int(year), int(month), int(day), tzinfo=timezone.utc)
            
            # If all patterns fail, try direct parsing
            return datetime.fromisoformat(date_string)
            
    except (ValueError, TypeError):
        return None

def add_time(dt: datetime, *, 
            days: int = 0, 
            hours: int = 0, 
            minutes: int = 0, 
            seconds: int = 0) -> datetime:
    """
    Add time to datetime.
    
    Args:
        dt: Base datetime
        days: Days to add
        hours: Hours to add
        minutes: Minutes to add
        seconds: Seconds to add
        
    Returns:
        datetime: New datetime
    """
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt + timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds)

def subtract_time(dt: datetime, *, 
                days: int = 0, 
                hours: int = 0, 
                minutes: int = 0, 
                seconds: int = 0) -> datetime:
    """
    Subtract time from datetime.
    
    Args:
        dt: Base datetime
        days: Days to subtract
        hours: Hours to subtract
        minutes: Minutes to subtract
        seconds: Seconds to subtract
        
    Returns:
        datetime: New datetime
    """
    return add_time(dt, days=-days, hours=-hours, minutes=-minutes, seconds=-seconds)

def is_future(dt: datetime) -> bool:
    """
    Check if datetime is in the future.
    
    Args:
        dt: Datetime to check
        
    Returns:
        bool: True if datetime is in the future
    """
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt > utc_now()

def is_past(dt: datetime) -> bool:
    """
    Check if datetime is in the past.
    
    Args:
        dt: Datetime to check
        
    Returns:
        bool: True if datetime is in the past
    """
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt < utc_now()

def ensure_utc(dt: datetime) -> datetime:
    """
    Ensure datetime is UTC timezone-aware.
    
    Args:
        dt: Datetime to process
        
    Returns:
        datetime: UTC timezone-aware datetime
    """
    # If timezone-naive, assume it's already UTC and add timezone
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    
    # If it has a different timezone, convert to UTC
    return dt.astimezone(timezone.utc)
</file>

<file path="app/utils/error_handling.py">
"""
Utilities for standardized error handling across API endpoints.
"""
from typing import Any, Dict, Optional, Type, Union, List
import logging
from fastapi import HTTPException, status
from pydantic import ValidationError as PydanticValidationError

from app.core.exceptions import (
    InboxerrException, 
    ValidationError, 
    NotFoundError, 
    AuthenticationError,
    AuthorizationError,
    SMSGatewayError,
    RetryableError,
    WebhookError
)

logger = logging.getLogger("inboxerr.errors")

class ErrorResponse:
    """Standard error response format."""
    
    @staticmethod
    def model(
        status_code: int,
        code: str,
        message: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a standardized error response model.
        
        Args:
            status_code: HTTP status code
            code: Error code
            message: Error message
            details: Additional error details
            
        Returns:
            Dict: Standardized error response
        """
        return {
            "status": "error",
            "code": code,
            "message": message,
            "details": details or {}
        }
    
    @staticmethod
    def from_exception(exception: Union[Exception, InboxerrException]) -> Dict[str, Any]:
        """
        Create error response from exception.
        
        Args:
            exception: Exception to process
            
        Returns:
            Dict: Standardized error response
        """
        if isinstance(exception, InboxerrException):
            # Use attributes from custom exception
            return ErrorResponse.model(
                status_code=exception.status_code,
                code=exception.code,
                message=exception.message,
                details=exception.details
            )
        elif isinstance(exception, HTTPException):
            # Convert FastAPI HTTPException
            return ErrorResponse.model(
                status_code=exception.status_code,
                code=f"HTTP_{exception.status_code}",
                message=exception.detail,
                details=getattr(exception, "details", None)
            )
        elif isinstance(exception, PydanticValidationError):
            # Convert Pydantic validation error
            return ErrorResponse.model(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                code="VALIDATION_ERROR",
                message="Validation error",
                details={"errors": exception.errors()}
            )
        else:
            # Generic exception
            return ErrorResponse.model(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                code="INTERNAL_ERROR",
                message=str(exception),
                details={"type": type(exception).__name__}
            )


def handle_exception(exception: Exception) -> HTTPException:
    """
    Convert any exception to appropriate HTTPException.
    
    Args:
        exception: Exception to handle
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    # Log all exceptions
    if isinstance(exception, (ValidationError, NotFoundError)):
        logger.info(f"Expected exception: {exception}")
    else:
        logger.error(f"Exception: {exception}", exc_info=True)
    
    # Map custom exceptions to status codes
    if isinstance(exception, ValidationError):
        status_code = status.HTTP_422_UNPROCESSABLE_ENTITY
    elif isinstance(exception, NotFoundError):
        status_code = status.HTTP_404_NOT_FOUND
    elif isinstance(exception, AuthenticationError):
        status_code = status.HTTP_401_UNAUTHORIZED
    elif isinstance(exception, AuthorizationError):
        status_code = status.HTTP_403_FORBIDDEN
    elif isinstance(exception, SMSGatewayError):
        status_code = status.HTTP_502_BAD_GATEWAY
    elif isinstance(exception, RetryableError):
        status_code = status.HTTP_503_SERVICE_UNAVAILABLE
    elif isinstance(exception, WebhookError):
        status_code = status.HTTP_400_BAD_REQUEST
    elif isinstance(exception, PydanticValidationError):
        status_code = status.HTTP_422_UNPROCESSABLE_ENTITY
    elif isinstance(exception, HTTPException):
        # Already a FastAPI HTTPException, just return it
        return exception
    else:
        # Default to internal server error
        status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    
    # Get error response
    error_response = ErrorResponse.from_exception(exception)
    
    # Create FastAPI HTTPException
    http_exception = HTTPException(
        status_code=status_code,
        detail=error_response
    )
    
    # Add authentication headers if needed
    if isinstance(exception, AuthenticationError):
        http_exception.headers = {"WWW-Authenticate": "Bearer"}
    
    # Add retry headers if needed
    if isinstance(exception, RetryableError):
        retry_after = getattr(exception, "details", {}).get("retry_after", 60)
        http_exception.headers = {"Retry-After": str(retry_after)}
    
    return http_exception


def validation_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create a validation error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(ValidationError(message=message, details=details))


def not_found_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create a not found error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(NotFoundError(message=message, details=details))


def auth_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create an authentication error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(AuthenticationError(message=message, details=details))


def permission_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create an authorization error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(AuthorizationError(message=message, details=details))


def server_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create a server error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return HTTPException(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        detail=ErrorResponse.model(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            code="INTERNAL_ERROR",
            message=message,
            details=details
        )
    )
</file>

<file path="app/utils/pagination.py">
"""
Utilities for API pagination.
"""
from typing import List, Dict, Any, TypeVar, Generic, Optional
from fastapi import Query, Depends
from pydantic import BaseModel


class PaginationParams:
    """
    Pagination parameters for API endpoints.
    
    This class is used as a FastAPI dependency to extract pagination parameters
    from query parameters.
    """
    
    def __init__(
        self,
        page: int = Query(1, ge=1, description="Page number"),
        limit: int = Query(20, ge=1, le=100, description="Items per page"),
        sort: Optional[str] = Query(None, description="Sort field"),
        order: Optional[str] = Query("asc", description="Sort order (asc or desc)")
    ):
        """
        Initialize pagination parameters.
        
        Args:
            page: Page number (1-based)
            limit: Items per page
            sort: Field to sort by
            order: Sort order (asc or desc)
        """
        self.page = page
        self.limit = limit
        self.sort = sort
        self.order = order
        
        # Calculate skip value for database queries
        self.skip = (page - 1) * limit


class PageInfo(BaseModel):
    """
    Page information for paginated responses.
    """
    current_page: int
    total_pages: int
    page_size: int
    total_items: int
    has_previous: bool
    has_next: bool


T = TypeVar('T')

class PaginatedResponse(BaseModel, Generic[T]):
    """
    Generic paginated response model.
    
    This class is used to standardize the format of paginated responses
    across all API endpoints.
    """
    items: List[T]
    page_info: PageInfo
    
    class Config:
        """Pydantic config."""
        arbitrary_types_allowed = True


def paginate_response(
    items: List[Any],
    total: int,
    pagination: PaginationParams
) -> Dict[str, Any]:
    """
    Create a standardized paginated response.
    
    Args:
        items: List of items for the current page
        total: Total number of items across all pages
        pagination: Pagination parameters
        
    Returns:
        Dict: Standardized response with items and pagination info
    """
    # Calculate pagination values
    total_pages = (total + pagination.limit - 1) // pagination.limit
    
    # Create page info
    page_info = PageInfo(
        current_page=pagination.page,
        total_pages=total_pages,
        page_size=pagination.limit,
        total_items=total,
        has_previous=pagination.page > 1,
        has_next=pagination.page < total_pages
    )
    
    # Create response
    return {
        "items": items,
        "page_info": page_info
    }


def get_pagination_links(
    path: str,
    pagination: PaginationParams,
    total: int,
    query_params: Optional[Dict[str, Any]] = None
) -> Dict[str, Optional[str]]:
    """
    Generate pagination links for HATEOAS.
    
    Args:
        path: Base path for links
        pagination: Pagination parameters
        total: Total number of items
        query_params: Additional query parameters
        
    Returns:
        Dict: Links for first, prev, next, and last pages
    """
    # Calculate pagination values
    total_pages = (total + pagination.limit - 1) // pagination.limit
    
    # Initialize query params
    params = query_params.copy() if query_params else {}
    
    # Helper to create URL with query params
    def create_url(page: int) -> str:
        page_params = {**params, "page": page, "limit": pagination.limit}
        
        if pagination.sort:
            page_params["sort"] = pagination.sort
            page_params["order"] = pagination.order
            
        query_string = "&".join(f"{key}={value}" for key, value in page_params.items())
        return f"{path}?{query_string}"
    
    # Create links
    links = {
        "first": create_url(1),
        "last": create_url(total_pages) if total_pages > 0 else None,
        "prev": create_url(pagination.page - 1) if pagination.page > 1 else None,
        "next": create_url(pagination.page + 1) if pagination.page < total_pages else None
    }
    
    return links
</file>

<file path="c.xml">
This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
alembic.ini
alembic/env.py
alembic/README
alembic/script.py.mako
alembic/versions/4669eb7a8878_add_user_metrics_table.py
alembic/versions/65b12145bf1a_add_message_templates_table.py
alembic/versions/a5a3f106534b_make_datetime_fields_timezone_aware.py
app/api/router.py
app/api/v1/dependencies.py
app/api/v1/endpoints/auth.py
app/api/v1/endpoints/campaigns.py
app/api/v1/endpoints/messages.py
app/api/v1/endpoints/metrics.py
app/api/v1/endpoints/templates.py
app/api/v1/endpoints/webhooks.py
app/core/config.py
app/core/events.py
app/core/exceptions.py
app/core/security.py
app/db/base.py
app/db/repositories/base.py
app/db/repositories/campaigns.py
app/db/repositories/messages.py
app/db/repositories/metrics.py
app/db/repositories/templates.py
app/db/repositories/users.py
app/db/repositories/webhooks.py
app/db/session.py
app/main.py
app/models/base.py
app/models/campaign.py
app/models/message.py
app/models/metrics.py
app/models/user.py
app/models/webhook.py
app/schemas/campaign.py
app/schemas/message.py
app/schemas/metrics.py
app/schemas/template.py
app/schemas/user.py
app/scripts/create_admin.py
app/services/campaigns/processor.py
app/services/event_bus/bus.py
app/services/event_bus/events.py
app/services/metrics/collector.py
app/services/rate_limiter.py
app/services/sms/retry_engine.py
app/services/sms/sender.py
app/services/webhooks/manager.py
app/services/webhooks/models.py
app/utils/datetime.py
app/utils/error_handling.py
app/utils/ids.py
app/utils/pagination.py
app/utils/phone.py
database-schema.md
docker-compose.yml
Dockerfile
docs/FRONTEND_API_SETUP.md
docs/FRONTEND_DEVELOPER_GUIDE.md
docs/Message Template System - User Guide.md
docs/tofix.md
project_structure.md
README.md
requirements.txt
scripts/generate_migration.py
scripts/mvp.sh
scripts/reset_db.py
scripts/run_tests.py
scripts/seed_db.py
scripts/seed_frontend_data.py
scripts/setup_test_db.py
tests/conftest.py
tests/core_functionality_test.py
tests/unit/api/messages/test_messages_endpoints.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="alembic/versions/4669eb7a8878_add_user_metrics_table.py">
"""Add user metrics table

Revision ID: 4669eb7a8878
Revises: a5a3f106534b
Create Date: 2025-05-01 22:14:21.434189

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '4669eb7a8878'
down_revision: Union[str, None] = 'a5a3f106534b'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('usermetrics',
    sa.Column('user_id', sa.String(), nullable=False),
    sa.Column('date', sa.Date(), nullable=False),
    sa.Column('messages_sent', sa.Integer(), nullable=False),
    sa.Column('messages_delivered', sa.Integer(), nullable=False),
    sa.Column('messages_failed', sa.Integer(), nullable=False),
    sa.Column('messages_scheduled', sa.Integer(), nullable=False),
    sa.Column('campaigns_created', sa.Integer(), nullable=False),
    sa.Column('campaigns_completed', sa.Integer(), nullable=False),
    sa.Column('campaigns_active', sa.Integer(), nullable=False),
    sa.Column('templates_created', sa.Integer(), nullable=False),
    sa.Column('templates_used', sa.Integer(), nullable=False),
    sa.Column('quota_total', sa.Integer(), nullable=False),
    sa.Column('quota_used', sa.Integer(), nullable=False),
    sa.Column('meta_data', sa.JSON(), nullable=True),
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['user.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('user_id', 'date', name='uix_user_date')
    )
    op.create_index(op.f('ix_usermetrics_date'), 'usermetrics', ['date'], unique=False)
    op.create_index(op.f('ix_usermetrics_id'), 'usermetrics', ['id'], unique=False)
    op.create_index(op.f('ix_usermetrics_user_id'), 'usermetrics', ['user_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_usermetrics_user_id'), table_name='usermetrics')
    op.drop_index(op.f('ix_usermetrics_id'), table_name='usermetrics')
    op.drop_index(op.f('ix_usermetrics_date'), table_name='usermetrics')
    op.drop_table('usermetrics')
    # ### end Alembic commands ###
</file>

<file path="alembic/versions/65b12145bf1a_add_message_templates_table.py">
"""Add message templates table

Revision ID: 65b12145bf1a
Revises: 
Create Date: 2025-04-24 15:52:34.696386

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '65b12145bf1a'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###
</file>

<file path="alembic/versions/a5a3f106534b_make_datetime_fields_timezone_aware.py">
"""Make datetime fields timezone-aware

Revision ID: a5a3f106534b
Revises: 65b12145bf1a
Create Date: 2025-04-29 21:44:26.340452

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'a5a3f106534b'
down_revision: Union[str, None] = '65b12145bf1a'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('apikey', 'expires_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('apikey', 'last_used_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('apikey', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('apikey', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('campaign', 'scheduled_start_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'scheduled_end_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'started_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'completed_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('campaign', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('campaign', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('message', 'scheduled_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'sent_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'delivered_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'failed_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('message', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('message', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagebatch', 'completed_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('messagebatch', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagebatch', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messageevent', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messageevent', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('user', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('user', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhook', 'last_triggered_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('webhook', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhook', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'next_retry_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
    op.alter_column('webhookdelivery', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookevent', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    op.alter_column('webhookevent', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.alter_column('webhookevent', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookevent', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhookdelivery', 'next_retry_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('webhook', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhook', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('webhook', 'last_triggered_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('user', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('user', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagetemplate', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messageevent', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messageevent', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagebatch', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagebatch', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('messagebatch', 'completed_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('message', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('message', 'failed_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'delivered_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'sent_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('message', 'scheduled_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('campaign', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('campaign', 'completed_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'started_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'scheduled_end_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('campaign', 'scheduled_start_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('apikey', 'updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('apikey', 'created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
    op.alter_column('apikey', 'last_used_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    op.alter_column('apikey', 'expires_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
    # ### end Alembic commands ###
</file>

<file path="app/db/repositories/metrics.py">
"""
Repository for metrics operations.
"""
from datetime import datetime, timezone, date, timedelta
from typing import List, Optional, Dict, Any, Tuple

from sqlalchemy import select, update, and_, func, desc
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.dialects.postgresql import insert

from app.db.repositories.base import BaseRepository
from app.models.metrics import UserMetrics


class MetricsRepository(BaseRepository[UserMetrics, Dict[str, Any], Dict[str, Any]]):
    """Repository for metrics operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and UserMetrics model."""
        super().__init__(session=session, model=UserMetrics)
    
    async def get_or_create_for_day(
        self,
        *,
        user_id: str,
        day: date
    ) -> UserMetrics:
        """
        Get or create metrics for a specific user and day.
        
        Args:
            user_id: User ID
            day: Date for metrics
            
        Returns:
            UserMetrics: Metrics for the specified day
        """
        # Check if metrics exist for this user and day
        query = select(UserMetrics).where(
            and_(
                UserMetrics.user_id == user_id,
                UserMetrics.date == day
            )
        )
        result = await self.session.execute(query)
        metrics = result.scalar_one_or_none()
        
        if metrics:
            return metrics
        
        # Create new metrics if not found
        metrics = UserMetrics(
            user_id=user_id,
            date=day,
            messages_sent=0,
            messages_delivered=0,
            messages_failed=0,
            messages_scheduled=0,
            campaigns_created=0,
            campaigns_completed=0,
            campaigns_active=0,
            templates_created=0,
            templates_used=0,
            quota_total=1000,  # Default quota
            quota_used=0
        )
        
        self.session.add(metrics)
        await self.session.commit()
        await self.session.refresh(metrics)
        
        return metrics
    
    async def increment_metric(
        self,
        *,
        user_id: str,
        date: date,
        metric_name: str,
        increment: int = 1
    ) -> Optional[UserMetrics]:
        """
        Increment a specific metric for a user on a specific day.
        
        Args:
            user_id: User ID
            date: Date for metrics
            metric_name: Name of metric to increment
            increment: Amount to increment by
            
        Returns:
            UserMetrics: Updated metrics or None if error
        """
        # Get or create metrics for this day
        metrics = await self.get_or_create_for_day(user_id=user_id, day=date)
        
        # Update specific metric
        if hasattr(metrics, metric_name):
            current_value = getattr(metrics, metric_name)
            setattr(metrics, metric_name, current_value + increment)
            
            # Also update quota_used if incrementing sent messages
            if metric_name == "messages_sent":
                metrics.quota_used += increment
            
            self.session.add(metrics)
            await self.session.commit()
            await self.session.refresh(metrics)
            
            return metrics
        
        return None
    
    async def get_metrics_for_day(
        self,
        *,
        user_id: str,
        day: date
    ) -> Optional[UserMetrics]:
        """
        Get metrics for a specific user and day.
        
        Args:
            user_id: User ID
            day: Date for metrics
            
        Returns:
            UserMetrics: Metrics for the specified day or None if not found
        """
        query = select(UserMetrics).where(
            and_(
                UserMetrics.user_id == user_id,
                UserMetrics.date == day
            )
        )
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def get_metrics_range(
        self,
        *,
        user_id: str,
        start_date: date,
        end_date: date
    ) -> List[UserMetrics]:
        """
        Get metrics for a specific user over a date range.
        
        Args:
            user_id: User ID
            start_date: Start date (inclusive)
            end_date: End date (inclusive)
            
        Returns:
            List[UserMetrics]: List of metrics for the date range
        """
        query = select(UserMetrics).where(
            and_(
                UserMetrics.user_id == user_id,
                UserMetrics.date >= start_date,
                UserMetrics.date <= end_date
            )
        ).order_by(UserMetrics.date)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def get_monthly_metrics(
        self,
        *,
        user_id: str,
        year: int,
        month: int
    ) -> List[UserMetrics]:
        """
        Get metrics for a specific user for a full month.
        
        Args:
            user_id: User ID
            year: Year
            month: Month (1-12)
            
        Returns:
            List[UserMetrics]: List of metrics for the month
        """
        # Calculate start and end date for the month
        start_date = date(year, month, 1)
        if month == 12:
            end_date = date(year + 1, 1, 1) - timedelta(days=1)
        else:
            end_date = date(year, month + 1, 1) - timedelta(days=1)
        
        return await self.get_metrics_range(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
    
    async def get_summary_metrics(
        self,
        *,
        user_id: str,
        start_date: date,
        end_date: date
    ) -> Dict[str, Any]:
        """
        Get summarized metrics for a specific user over a date range.
        
        Args:
            user_id: User ID
            start_date: Start date (inclusive)
            end_date: End date (inclusive)
            
        Returns:
            Dict[str, Any]: Summarized metrics
        """
        # Get metrics for the date range
        metrics_list = await self.get_metrics_range(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
        
        # Calculate summary metrics
        total_sent = sum(m.messages_sent for m in metrics_list)
        total_delivered = sum(m.messages_delivered for m in metrics_list)
        total_failed = sum(m.messages_failed for m in metrics_list)
        
        # Calculate delivery rate
        delivery_rate = 0
        if total_sent > 0:
            delivery_rate = (total_delivered / total_sent) * 100
        
        # Get latest quota information
        quota_used = metrics_list[-1].quota_used if metrics_list else 0
        quota_total = metrics_list[-1].quota_total if metrics_list else 1000
        
        # Create summary
        return {
            "period": {
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat()
            },
            "messages": {
                "sent": total_sent,
                "delivered": total_delivered,
                "failed": total_failed,
                "delivery_rate": round(delivery_rate, 1)
            },
            "campaigns": {
                "created": sum(m.campaigns_created for m in metrics_list),
                "completed": sum(m.campaigns_completed for m in metrics_list),
                "active": metrics_list[-1].campaigns_active if metrics_list else 0
            },
            "templates": {
                "created": sum(m.templates_created for m in metrics_list),
                "used": sum(m.templates_used for m in metrics_list)
            },
            "quota": {
                "used": quota_used,
                "total": quota_total,
                "percent": round((quota_used / quota_total) * 100, 1) if quota_total > 0 else 0
            }
        }
    
    async def update_daily_metrics(self, day: Optional[date] = None) -> int:
        """
        Update metrics for all users for a specific day based on actual data.
        This is useful for background jobs that rebuild metrics.
        
        Args:
            day: Date to update metrics for (defaults to yesterday)
            
        Returns:
            int: Number of users updated
        """
        from app.db.repositories.messages import MessageRepository
        from app.db.repositories.campaigns import CampaignRepository
        from app.db.repositories.templates import TemplateRepository
        from app.db.repositories.users import UserRepository
        
        # Default to yesterday if no day provided
        if day is None:
            day = date.today() - timedelta(days=1)
        
        # Get all active users
        async with self.session.begin_nested():
            # Get all users
            query = select("*").select_from(UserRepository.model)
            result = await self.session.execute(query)
            users = result.fetchall()
            
            updates_count = 0
            
            # For each user, calculate and store metrics
            for user_row in users:
                user_id = user_row[0]  # Assuming id is the first column
                
                # Calculate message metrics
                msg_query = select(
                    MessageRepository.model.status,
                    func.count(MessageRepository.model.id)
                ).where(
                    and_(
                        MessageRepository.model.user_id == user_id,
                        MessageRepository.model.created_at >= datetime.combine(day, datetime.min.time()),
                        MessageRepository.model.created_at < datetime.combine(day + timedelta(days=1), datetime.min.time())
                    )
                ).group_by(MessageRepository.model.status)
                
                msg_result = await self.session.execute(msg_query)
                msg_counts = dict(msg_result.fetchall())
                
                # Calculate campaign metrics
                campaign_query = select(
                    CampaignRepository.model.status,
                    func.count(CampaignRepository.model.id)
                ).where(
                    CampaignRepository.model.user_id == user_id
                ).group_by(CampaignRepository.model.status)
                
                campaign_result = await self.session.execute(campaign_query)
                campaign_counts = dict(campaign_result.fetchall())
                
                # Calculate template metrics for the day
                template_created_query = select(
                    func.count(TemplateRepository.model.id)
                ).where(
                    and_(
                        TemplateRepository.model.user_id == user_id,
                        TemplateRepository.model.created_at >= datetime.combine(day, datetime.min.time()),
                        TemplateRepository.model.created_at < datetime.combine(day + timedelta(days=1), datetime.min.time())
                    )
                )
                
                template_created_result = await self.session.execute(template_created_query)
                templates_created = template_created_result.scalar_one_or_none() or 0
                
                # Get or create metrics for this day
                metrics = await self.get_or_create_for_day(user_id=user_id, day=day)
                
                # Update metrics with actual values
                metrics.messages_sent = msg_counts.get('sent', 0)
                metrics.messages_delivered = msg_counts.get('delivered', 0)
                metrics.messages_failed = msg_counts.get('failed', 0)
                metrics.messages_scheduled = msg_counts.get('scheduled', 0)
                
                metrics.campaigns_created = sum(1 for status, count in campaign_counts.items() 
                                            if campaign_counts.get('created_at') and campaign_counts['created_at'] >= day)
                metrics.campaigns_completed = campaign_counts.get('completed', 0) + campaign_counts.get('cancelled', 0)
                metrics.campaigns_active = campaign_counts.get('active', 0)
                
                metrics.templates_created = templates_created
                
                # Calculate quota used based on messages sent
                metrics.quota_used = msg_counts.get('sent', 0) + msg_counts.get('delivered', 0) + msg_counts.get('failed', 0)
                
                # Save updated metrics
                self.session.add(metrics)
                updates_count += 1
        
        # Commit all changes
        await self.session.commit()
        
        return updates_count
</file>

<file path="app/main.py">
"""
Main FastAPI application entry point for Inboxerr Backend.
"""
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging

from app.api.router import api_router
from app.core.config import settings
from app.core.exceptions import InboxerrException
from app.core.events import startup_event_handler, shutdown_event_handler

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("inboxerr")

# Create FastAPI app
app = FastAPI(
    title=settings.PROJECT_NAME,
    description=settings.PROJECT_DESCRIPTION,
    version=settings.VERSION,
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json",
)

# Set up CORS middleware
if settings.BACKEND_CORS_ORIGINS:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Register event handlers
app.add_event_handler("startup", startup_event_handler)
app.add_event_handler("shutdown", shutdown_event_handler)

# Register exception handlers
@app.exception_handler(InboxerrException)
async def inboxerr_exception_handler(request: Request, exc: InboxerrException):
    """Custom exception handler for InboxerrException."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "status": "error",
            "code": exc.code,
            "message": exc.message,
            "details": exc.details,
        },
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Enhanced HTTP exception handler with consistent format."""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "status": "error",
            "code": f"HTTP_{exc.status_code}",
            "message": exc.detail,
            "details": None,
        },
    )

# Register routers
app.include_router(api_router, prefix=settings.API_PREFIX)

# Root endpoint
@app.get("/", tags=["Health"])
async def root():
    """Root endpoint for health checks."""
    return {
        "status": "ok",
        "service": settings.PROJECT_NAME,
        "version": settings.VERSION,
    }

if __name__ == "__main__":
    # For debugging only - use uvicorn for production
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="app/schemas/template.py">
# app/schemas/template.py
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from pydantic import BaseModel, Field, validator


class MessageTemplateBase(BaseModel):
    """Base schema for message templates."""
    name: str = Field(..., description="Template name")
    content: str = Field(..., description="Template content with placeholders")
    description: Optional[str] = Field(None, description="Template description")
    is_active: bool = Field(True, description="Whether the template is active")


class MessageTemplateCreate(MessageTemplateBase):
    """Schema for creating a new message template."""
    variables: Optional[List[str]] = Field(default=[], description="List of variables in the template")
    
    @validator("content")
    def validate_content(cls, v):
        """Validate template content."""
        if not v or len(v.strip()) == 0:
            raise ValueError("Template content cannot be empty")
        if len(v) > 1600:  # Max length for multi-part SMS
            raise ValueError("Template exceeds maximum length of 1600 characters")
        return v
    
    @validator("variables", pre=True)
    def validate_variables(cls, v, values):
        """Extract variables from content if not provided."""
        import re
        
        if not v and "content" in values:
            # Extract variables like {{variable_name}} from content
            pattern = r"{{([a-zA-Z0-9_]+)}}"
            matches = re.findall(pattern, values["content"])
            if matches:
                return list(set(matches))  # Return unique variables
        return v or []


class MessageTemplateUpdate(BaseModel):
    """Schema for updating a message template."""
    name: Optional[str] = Field(None, description="Template name")
    content: Optional[str] = Field(None, description="Template content with placeholders")
    description: Optional[str] = Field(None, description="Template description")
    is_active: Optional[bool] = Field(None, description="Whether the template is active")
    variables: Optional[List[str]] = Field(None, description="List of variables in the template")
    
    @validator("content")
    def validate_content(cls, v):
        """Validate template content if provided."""
        if v is not None:
            if len(v.strip()) == 0:
                raise ValueError("Template content cannot be empty")
            if len(v) > 1600:
                raise ValueError("Template exceeds maximum length of 1600 characters")
        return v


class MessageTemplateResponse(MessageTemplateBase):
    """Schema for message template response."""
    id: str = Field(..., description="Template ID")
    variables: List[str] = Field(..., description="List of variables in the template")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    user_id: str = Field(..., description="User who created the template")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class MessageWithTemplate(BaseModel):
    """Schema for sending a message using a template."""
    template_id: str = Field(..., description="Template ID")
    phone_number: str = Field(..., description="Recipient phone number in E.164 format")
    variables: Dict[str, str] = Field(..., description="Values for template variables")
    scheduled_at: Optional[datetime] = Field(None, description="Schedule message for future delivery")
    custom_id: Optional[str] = Field(None, description="Custom ID for tracking")
    
    @validator("phone_number")
    def validate_phone_number(cls, v):
        """Validate phone number format."""
        if not v or not (v.startswith("+") and len(v) >= 8):
            raise ValueError("Phone number must be in E.164 format (e.g. +1234567890)")
        return v
</file>

<file path="app/scripts/create_admin.py">
# scripts/create_admin.py
import asyncio
import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from app.db.session import async_session_factory, initialize_database
from app.db.repositories.users import UserRepository
from app.core.security import get_password_hash

async def create_admin_user():
    """Create an admin user if none exists."""
    # Initialize database
    await initialize_database()
    
    # Use session properly with context manager
    async with async_session_factory() as session:
        # Create repository with session
        user_repo = UserRepository(session)
        
        # Check if admin exists
        admin = await user_repo.get_by_email("admin@inboxerr.com")
        
        if admin:
            print("Admin user already exists")
            return
        
        # Create admin user
        password = "Admin123!"
        hashed_password = get_password_hash(password)
        
        admin = await user_repo.create(
            email="admin@inboxerr.com",
            hashed_password=hashed_password,
            full_name="Admin User",
            is_active=True,
            role="admin"
        )
        
        print(f"Admin user created with ID: {admin.id}")
        print(f"Email: admin@inboxerr.com")
        print(f"Password: {password}")

if __name__ == "__main__":
    asyncio.run(create_admin_user())
</file>

<file path="app/utils/datetime.py">
"""
Utilities for standardized datetime handling.
"""
from datetime import datetime, timedelta, timezone
from typing import Optional, Union
import re

def utc_now() -> datetime:
    """
    Get current UTC time as timezone-aware datetime.
    
    Returns:
        datetime: Current UTC time
    """
    return datetime.now(timezone.utc)

def format_datetime(dt: Optional[datetime] = None) -> str:
    """
    Format datetime as ISO 8601 string.
    
    Args:
        dt: Datetime to format (defaults to current UTC time)
        
    Returns:
        str: ISO 8601 formatted string
    """
    if dt is None:
        dt = utc_now()
    
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt.isoformat()

def parse_datetime(date_string: str) -> Optional[datetime]:
    """
    Parse datetime from string.
    
    Args:
        date_string: Datetime string to parse
        
    Returns:
        datetime: Parsed datetime or None if invalid
    """
    try:
        # Handle common formats
        if 'T' in date_string:
            # ISO format
            if date_string.endswith('Z'):
                date_string = date_string[:-1] + '+00:00'
            return datetime.fromisoformat(date_string)
        else:
            # Try common date formats
            date_patterns = [
                # YYYY-MM-DD
                r'^(\d{4})-(\d{2})-(\d{2})$',
                # MM/DD/YYYY
                r'^(\d{1,2})/(\d{1,2})/(\d{4})$',
                # DD/MM/YYYY
                r'^(\d{1,2})-(\d{1,2})-(\d{4})$',
            ]
            
            for pattern in date_patterns:
                match = re.match(pattern, date_string)
                if match:
                    if pattern == date_patterns[0]:
                        year, month, day = match.groups()
                    else:
                        if pattern == date_patterns[1]:
                            month, day, year = match.groups()
                        else:
                            day, month, year = match.groups()
                    
                    return datetime(int(year), int(month), int(day), tzinfo=timezone.utc)
            
            # If all patterns fail, try direct parsing
            return datetime.fromisoformat(date_string)
            
    except (ValueError, TypeError):
        return None

def add_time(dt: datetime, *, 
            days: int = 0, 
            hours: int = 0, 
            minutes: int = 0, 
            seconds: int = 0) -> datetime:
    """
    Add time to datetime.
    
    Args:
        dt: Base datetime
        days: Days to add
        hours: Hours to add
        minutes: Minutes to add
        seconds: Seconds to add
        
    Returns:
        datetime: New datetime
    """
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt + timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds)

def subtract_time(dt: datetime, *, 
                days: int = 0, 
                hours: int = 0, 
                minutes: int = 0, 
                seconds: int = 0) -> datetime:
    """
    Subtract time from datetime.
    
    Args:
        dt: Base datetime
        days: Days to subtract
        hours: Hours to subtract
        minutes: Minutes to subtract
        seconds: Seconds to subtract
        
    Returns:
        datetime: New datetime
    """
    return add_time(dt, days=-days, hours=-hours, minutes=-minutes, seconds=-seconds)

def is_future(dt: datetime) -> bool:
    """
    Check if datetime is in the future.
    
    Args:
        dt: Datetime to check
        
    Returns:
        bool: True if datetime is in the future
    """
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt > utc_now()

def is_past(dt: datetime) -> bool:
    """
    Check if datetime is in the past.
    
    Args:
        dt: Datetime to check
        
    Returns:
        bool: True if datetime is in the past
    """
    # Ensure datetime is timezone-aware
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
        
    return dt < utc_now()

def ensure_utc(dt: datetime) -> datetime:
    """
    Ensure datetime is UTC timezone-aware.
    
    Args:
        dt: Datetime to process
        
    Returns:
        datetime: UTC timezone-aware datetime
    """
    # If timezone-naive, assume it's already UTC and add timezone
    if dt.tzinfo is None:
        return dt.replace(tzinfo=timezone.utc)
    
    # If it has a different timezone, convert to UTC
    return dt.astimezone(timezone.utc)
</file>

<file path="app/utils/error_handling.py">
"""
Utilities for standardized error handling across API endpoints.
"""
from typing import Any, Dict, Optional, Type, Union, List
import logging
from fastapi import HTTPException, status
from pydantic import ValidationError as PydanticValidationError

from app.core.exceptions import (
    InboxerrException, 
    ValidationError, 
    NotFoundError, 
    AuthenticationError,
    AuthorizationError,
    SMSGatewayError,
    RetryableError,
    WebhookError
)

logger = logging.getLogger("inboxerr.errors")

class ErrorResponse:
    """Standard error response format."""
    
    @staticmethod
    def model(
        status_code: int,
        code: str,
        message: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a standardized error response model.
        
        Args:
            status_code: HTTP status code
            code: Error code
            message: Error message
            details: Additional error details
            
        Returns:
            Dict: Standardized error response
        """
        return {
            "status": "error",
            "code": code,
            "message": message,
            "details": details or {}
        }
    
    @staticmethod
    def from_exception(exception: Union[Exception, InboxerrException]) -> Dict[str, Any]:
        """
        Create error response from exception.
        
        Args:
            exception: Exception to process
            
        Returns:
            Dict: Standardized error response
        """
        if isinstance(exception, InboxerrException):
            # Use attributes from custom exception
            return ErrorResponse.model(
                status_code=exception.status_code,
                code=exception.code,
                message=exception.message,
                details=exception.details
            )
        elif isinstance(exception, HTTPException):
            # Convert FastAPI HTTPException
            return ErrorResponse.model(
                status_code=exception.status_code,
                code=f"HTTP_{exception.status_code}",
                message=exception.detail,
                details=getattr(exception, "details", None)
            )
        elif isinstance(exception, PydanticValidationError):
            # Convert Pydantic validation error
            return ErrorResponse.model(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                code="VALIDATION_ERROR",
                message="Validation error",
                details={"errors": exception.errors()}
            )
        else:
            # Generic exception
            return ErrorResponse.model(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                code="INTERNAL_ERROR",
                message=str(exception),
                details={"type": type(exception).__name__}
            )


def handle_exception(exception: Exception) -> HTTPException:
    """
    Convert any exception to appropriate HTTPException.
    
    Args:
        exception: Exception to handle
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    # Log all exceptions
    if isinstance(exception, (ValidationError, NotFoundError)):
        logger.info(f"Expected exception: {exception}")
    else:
        logger.error(f"Exception: {exception}", exc_info=True)
    
    # Map custom exceptions to status codes
    if isinstance(exception, ValidationError):
        status_code = status.HTTP_422_UNPROCESSABLE_ENTITY
    elif isinstance(exception, NotFoundError):
        status_code = status.HTTP_404_NOT_FOUND
    elif isinstance(exception, AuthenticationError):
        status_code = status.HTTP_401_UNAUTHORIZED
    elif isinstance(exception, AuthorizationError):
        status_code = status.HTTP_403_FORBIDDEN
    elif isinstance(exception, SMSGatewayError):
        status_code = status.HTTP_502_BAD_GATEWAY
    elif isinstance(exception, RetryableError):
        status_code = status.HTTP_503_SERVICE_UNAVAILABLE
    elif isinstance(exception, WebhookError):
        status_code = status.HTTP_400_BAD_REQUEST
    elif isinstance(exception, PydanticValidationError):
        status_code = status.HTTP_422_UNPROCESSABLE_ENTITY
    elif isinstance(exception, HTTPException):
        # Already a FastAPI HTTPException, just return it
        return exception
    else:
        # Default to internal server error
        status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    
    # Get error response
    error_response = ErrorResponse.from_exception(exception)
    
    # Create FastAPI HTTPException
    http_exception = HTTPException(
        status_code=status_code,
        detail=error_response
    )
    
    # Add authentication headers if needed
    if isinstance(exception, AuthenticationError):
        http_exception.headers = {"WWW-Authenticate": "Bearer"}
    
    # Add retry headers if needed
    if isinstance(exception, RetryableError):
        retry_after = getattr(exception, "details", {}).get("retry_after", 60)
        http_exception.headers = {"Retry-After": str(retry_after)}
    
    return http_exception


def validation_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create a validation error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(ValidationError(message=message, details=details))


def not_found_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create a not found error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(NotFoundError(message=message, details=details))


def auth_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create an authentication error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(AuthenticationError(message=message, details=details))


def permission_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create an authorization error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return handle_exception(AuthorizationError(message=message, details=details))


def server_error(message: str, details: Optional[Dict[str, Any]] = None) -> HTTPException:
    """
    Create a server error response.
    
    Args:
        message: Error message
        details: Additional error details
        
    Returns:
        HTTPException: FastAPI HTTP exception
    """
    return HTTPException(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        detail=ErrorResponse.model(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            code="INTERNAL_ERROR",
            message=message,
            details=details
        )
    )
</file>

<file path="app/utils/ids.py">
# app/utils/ids.py

from enum import Enum
from uuid import uuid4

class IDPrefix(str, Enum):
    MESSAGE = "msg"
    EVENT = "event"
    BATCH = "batch"
    USER = "user"
    CAMPAIGN = "campaign"
    TEMPLATE = "template"
    WEBHOOK = "webhook"

def generate_prefixed_id(prefix: IDPrefix) -> str:
    """
    Generate a UUID string with a prefix.
    
    Args:
        prefix (IDPrefix): The entity prefix (e.g., MESSAGE, EVENT).
        
    Returns:
        str: A prefixed UUID string like 'msg-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'
    """
    return f"{prefix.value}-{uuid4()}"
</file>

<file path="app/utils/pagination.py">
"""
Utilities for API pagination.
"""
from typing import List, Dict, Any, TypeVar, Generic, Optional
from fastapi import Query, Depends
from pydantic import BaseModel


class PaginationParams:
    """
    Pagination parameters for API endpoints.
    
    This class is used as a FastAPI dependency to extract pagination parameters
    from query parameters.
    """
    
    def __init__(
        self,
        page: int = Query(1, ge=1, description="Page number"),
        limit: int = Query(20, ge=1, le=100, description="Items per page"),
        sort: Optional[str] = Query(None, description="Sort field"),
        order: Optional[str] = Query("asc", description="Sort order (asc or desc)")
    ):
        """
        Initialize pagination parameters.
        
        Args:
            page: Page number (1-based)
            limit: Items per page
            sort: Field to sort by
            order: Sort order (asc or desc)
        """
        self.page = page
        self.limit = limit
        self.sort = sort
        self.order = order
        
        # Calculate skip value for database queries
        self.skip = (page - 1) * limit


class PageInfo(BaseModel):
    """
    Page information for paginated responses.
    """
    current_page: int
    total_pages: int
    page_size: int
    total_items: int
    has_previous: bool
    has_next: bool


T = TypeVar('T')

class PaginatedResponse(BaseModel, Generic[T]):
    """
    Generic paginated response model.
    
    This class is used to standardize the format of paginated responses
    across all API endpoints.
    """
    items: List[T]
    page_info: PageInfo
    
    class Config:
        """Pydantic config."""
        arbitrary_types_allowed = True


def paginate_response(
    items: List[Any],
    total: int,
    pagination: PaginationParams
) -> Dict[str, Any]:
    """
    Create a standardized paginated response.
    
    Args:
        items: List of items for the current page
        total: Total number of items across all pages
        pagination: Pagination parameters
        
    Returns:
        Dict: Standardized response with items and pagination info
    """
    # Calculate pagination values
    total_pages = (total + pagination.limit - 1) // pagination.limit
    
    # Create page info
    page_info = PageInfo(
        current_page=pagination.page,
        total_pages=total_pages,
        page_size=pagination.limit,
        total_items=total,
        has_previous=pagination.page > 1,
        has_next=pagination.page < total_pages
    )
    
    # Create response
    return {
        "items": items,
        "page_info": page_info
    }


def get_pagination_links(
    path: str,
    pagination: PaginationParams,
    total: int,
    query_params: Optional[Dict[str, Any]] = None
) -> Dict[str, Optional[str]]:
    """
    Generate pagination links for HATEOAS.
    
    Args:
        path: Base path for links
        pagination: Pagination parameters
        total: Total number of items
        query_params: Additional query parameters
        
    Returns:
        Dict: Links for first, prev, next, and last pages
    """
    # Calculate pagination values
    total_pages = (total + pagination.limit - 1) // pagination.limit
    
    # Initialize query params
    params = query_params.copy() if query_params else {}
    
    # Helper to create URL with query params
    def create_url(page: int) -> str:
        page_params = {**params, "page": page, "limit": pagination.limit}
        
        if pagination.sort:
            page_params["sort"] = pagination.sort
            page_params["order"] = pagination.order
            
        query_string = "&".join(f"{key}={value}" for key, value in page_params.items())
        return f"{path}?{query_string}"
    
    # Create links
    links = {
        "first": create_url(1),
        "last": create_url(total_pages) if total_pages > 0 else None,
        "prev": create_url(pagination.page - 1) if pagination.page > 1 else None,
        "next": create_url(pagination.page + 1) if pagination.page < total_pages else None
    }
    
    return links
</file>

<file path="database-schema.md">
# Inboxerr Database Schema Overview

This document outlines the structure of the PostgreSQL database used by the Inboxerr backend service. It includes all tables, relationships, and index/foreign key mappings, aligned with the SQLAlchemy models defined in the codebase.

---

## 🔢 Tables & Relationships

| **Table**           | **Primary Keys** | **Foreign Keys**                             | **Relationships**                             |
|---------------------|------------------|-----------------------------------------------|------------------------------------------------|
| `user`              | `id`             | –                                             | Referenced by many tables                      |
| `apikey`            | `id`             | `user_id` → `user(id)`                     | Each API key belongs to a user                 |
| `campaign`          | `id`             | `user_id` → `user(id)`                     | One user owns many campaigns                   |
| `message`           | `id`             | `user_id` → `user(id)`<br>`campaign_id` → `campaign(id)`<br>`batch_id` → `messagebatch(id)` | Messages belong to a campaign and batch        |
| `messagebatch`      | `id`             | `user_id` → `user(id)`                     | Groups messages sent together                  |
| `messageevent`      | `id`             | `message_id` → `message(id)`              | Tracks status updates for a message            |
| `messagetemplate`   | `id`             | `user_id` → `user(id)`                     | Message content templates                      |
| `webhook`           | `id`             | `user_id` → `user(id)`                     | Defines external callbacks                     |
| `webhookdelivery`   | `id`             | `webhook_id` → `webhook(id)`<br>`message_id` → `message(id)` | Stores actual webhook attempts                 |
| `webhookevent`      | `id`             | –                                             | Events that can trigger webhooks               |
| `alembic_version`   | –                | –                                             | Managed by Alembic for schema migrations       |

---

## 📊 Indexes & Performance

Each table includes relevant indexes, such as:
- `id` (primary key, indexed by default)
- Frequently queried fields like `user_id`, `campaign_id`, `status`, and `scheduled_at`

---

## 🔒 Data Types Overview

| **Field**            | **Type**                       |
|----------------------|---------------------------------|
| `id`                 | `character varying` (UUIDs)     |
| `user_id`            | `character varying` (FK)        |
| `campaign_id`        | `character varying` (FK)        |
| `message`            | `text`                          |
| `status`             | `character varying`             |
| `scheduled_at`       | `timestamp without time zone`   |
| `settings`, `data`   | `json`                          |

---

## 📄 How to Inspect the Schema

From inside `psql`:
```bash
\c inboxerr        -- Connect to DB
\dt                 -- List tables
\d tablename       -- Describe table structure
SELECT * FROM tablename LIMIT 5;  -- Preview data
```

To see all foreign keys:
```sql
SELECT conname AS constraint_name, conrelid::regclass AS table,
       a.attname AS column, confrelid::regclass AS referenced_table
FROM pg_constraint
JOIN pg_class ON conrelid = pg_class.oid
JOIN pg_attribute a ON a.attrelid = conrelid AND a.attnum = ANY(conkey)
WHERE contype = 'f';
```
</file>

<file path="Dockerfile">
# Use Python 3.10 slim as base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        build-essential \
        libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN adduser --disabled-password --gecos "" appuser
RUN chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Start application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path="docs/FRONTEND_API_SETUP.md">
# Inboxerr Backend - Frontend Developer Setup

This guide will help frontend developers set up and interact with the Inboxerr backend API.

## Quick Start

1. Clone the repository
```bash
git clone https://github.com/your-org/inboxerr-backend.git
cd inboxerr-backend
```

2. Set up a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

4. Create a local config file
```bash
cp .env.example .env
```

5. Set up the database
```bash
# Make sure PostgreSQL is running on your system
# Create database
psql -U postgres -c "CREATE DATABASE inboxerr;"

# Run migrations
alembic upgrade head
```

6. Seed sample data for frontend development
```bash
python scripts/seed_frontend_data.py
```

7. Start the server
```bash
uvicorn app.main:app --reload
```

8. Access the API at http://localhost:8000/api/docs

## Sample Account

After running the seed script, you can use these credentials:
- Email: `test@example.com`
- Password: `Test1234!`

## Key Features Ready for Frontend Integration

- ✅ User authentication (JWT)
- ✅ Send individual and batch SMS messages
- ✅ Message templates with variable substitution
- ✅ Campaign management
- ✅ Message status tracking
- ✅ Webhook handling for status updates

## API Documentation

See the [Inboxerr API Frontend Developer Guide](FRONTEND_API_GUIDE.md) for complete documentation of all available endpoints.

## Mock SMS Gateway

For frontend development, the backend can operate without real SMS Gateway credentials. Messages will be processed normally but not actually sent:

1. In development mode, the backend will simulate sending messages
2. All webhook events can be manually triggered for testing
3. All message statuses can be updated through the API

## Using with Docker (Alternative)

If you prefer using Docker:

```bash
# Start all services
docker-compose up -d

# Seed sample data
docker-compose exec api python scripts/seed_frontend_data.py
```

## Testing Webhooks

To test webhook events for message status updates:

```bash
# Replace EVENT_TYPE with: sms:sent, sms:delivered, or sms:failed
curl -X POST http://localhost:8000/api/v1/webhooks/test/{EVENT_TYPE} \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message_id": "YOUR_MESSAGE_ID"}'
```

## Troubleshooting

- **Database connection issues**: Ensure PostgreSQL is running and credentials are correct in `.env`
- **Authentication errors**: Check that you're using the correct bearer token format
- **CORS errors**: Add your frontend URL to `BACKEND_CORS_ORIGINS` in `.env`

## Need Help?

Contact the backend team via:
- Slack: #inboxerr-backend
- Email: backend@inboxerr.com
</file>

<file path="docs/FRONTEND_DEVELOPER_GUIDE.md">
# Inboxerr API - Frontend Developer Guide

This document provides frontend developers with essential information for integrating with the Inboxerr backend API.

## Base URL

```
http://localhost:8000/api/v1
```

For production, this will be replaced with the actual deployment URL.

## Authentication

### Getting a Token

```
POST /auth/token
```

**Request Body:**
```json
{
  "username": "your-email@example.com",
  "password": "your-password"
}
```

**Response:**
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_at": "2025-04-25T20:04:04.790Z"
}
```

### Using Authentication

Include the token in all subsequent requests:

```
Authorization: Bearer {access_token}
```

### Test User Credentials

For development, use the following credentials:

- Email: `test@example.com`
- Password: `Test1234!`

## Key Endpoints

### 1. Send a Single SMS

```
POST /messages/send
```

**Request Body:**
```json
{
  "phone_number": "+1234567890",
  "message": "Your message content",
  "scheduled_at": null,
  "custom_id": "optional-tracking-id"
}
```

### 2. Send Batch Messages

```
POST /messages/batch
```

**Request Body:**
```json
{
  "messages": [
    {
      "phone_number": "+1234567890",
      "message": "Message for recipient 1",
      "scheduled_at": null
    },
    {
      "phone_number": "+9876543210",
      "message": "Message for recipient 2",
      "scheduled_at": null
    }
  ],
  "options": {
    "delay_between_messages": 0.3,
    "fail_on_first_error": false
  }
}
```

### 3. List Messages

```
GET /messages?skip=0&limit=20
```

Optional query parameters:
- `status` - Filter by message status (pending, sent, delivered, failed)
- `phone_number` - Filter by phone number
- `from_date` - Filter by date (ISO format)
- `to_date` - Filter by date (ISO format)

### 4. Message Templates

#### Create Template

```
POST /templates
```

**Request Body:**
```json
{
  "name": "Welcome Template",
  "content": "Hello {{name}}, welcome to our service!",
  "description": "Welcome message for new users"
}
```

#### Send Using Template

```
POST /templates/send
```

**Request Body:**
```json
{
  "template_id": "template-uuid",
  "phone_number": "+1234567890",
  "variables": {
    "name": "John"
  }
}
```

### 5. Campaigns

#### Create Campaign

```
POST /campaigns
```

**Request Body:**
```json
{
  "name": "Marketing Campaign",
  "description": "Product launch campaign",
  "scheduled_start_at": "2025-05-01T09:00:00Z",
  "scheduled_end_at": "2025-05-01T18:00:00Z"
}
```

#### Start Campaign

```
POST /campaigns/{campaign_id}/start
```

## Status Codes

- `200` - Success
- `201` - Created
- `202` - Accepted (for async processing)
- `400` - Bad request
- `401` - Unauthorized
- `403` - Forbidden
- `404` - Not found
- `422` - Validation error
- `429` - Rate limit exceeded
- `500` - Server error

## Error Format

All API errors follow this format:

```json
{
  "status": "error",
  "code": "ERROR_CODE",
  "message": "Human-readable error message",
  "details": {}
}
```

## Pagination

Endpoints that return lists support pagination:

```
GET /messages?page=1&limit=20
```

Response includes pagination info:

```json
{
  "items": [...],
  "page_info": {
    "current_page": 1,
    "total_pages": 5,
    "page_size": 20,
    "total_items": 100,
    "has_previous": false,
    "has_next": true
  }
}
```

## Message Status Flow

Messages follow this status flow:

1. `pending` - Initial state when created
2. `scheduled` - For future delivery
3. `processed` - Submitted to SMS gateway
4. `sent` - Accepted by the carrier
5. `delivered` - Confirmed delivery to recipient
6. `failed` - Failed to deliver

## Webhooks

For development, you can test webhook events using:

```
GET /webhooks/test/{event_type}
```

Where `event_type` can be:
- `sms:sent`
- `sms:delivered`
- `sms:failed`

## Rate Limits

- Message sending: 60 requests per minute
- Batch operations: 10 requests per minute
- Template operations: 100 requests per minute

## Development Notes

- Phone numbers should be in E.164 format (e.g., +1234567890)
- Messages longer than 160 characters will be sent as multi-part SMS
- SMS templates support variable substitution using `{{variable_name}}` syntax
</file>

<file path="docs/Message Template System - User Guide.md">
# Message Template System - User Guide

## Overview

The message template system allows you to create reusable templates for your SMS messages. This is particularly useful when you need to send similar messages to multiple recipients with personalized content.

## Key Features

- Create and manage reusable message templates
- Support for variables using the `{{variable_name}}` syntax
- Preview how templates will look with specific variable values
- Send messages using templates with just a phone number and variable values
- Send batch messages using the same template with different variables for each recipient

## Creating Templates

### Via API

```http
POST /api/v1/templates
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "name": "OTP Notification",
  "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
  "description": "Template for sending OTP codes",
  "is_active": true
}
```

The system will automatically detect variables in the format `{{variable_name}}` from your template content.

### Variable Format

Variables should be enclosed in double curly braces like `{{variable_name}}`. Variable names can contain letters, numbers, and underscores.

Examples:
- `{{code}}`
- `{{user_name}}`
- `{{order_123}}`

## Using Templates

### Previewing a Template

Before sending, you can preview how your template will look with specific variables:

```http
POST /api/v1/templates/apply?template_id=TEMPLATE_ID
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "variables": {
    "code": "123456",
    "minutes": "15"
  }
}
```

Response:
```json
{
  "result": "Your verification code is 123456. It will expire in 15 minutes.",
  "missing_variables": []
}
```

### Sending a Message with a Template

```http
POST /api/v1/templates/send
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "template_id": "TEMPLATE_ID",
  "phone_number": "+1234567890",
  "variables": {
    "code": "123456",
    "minutes": "15"
  },
  "scheduled_at": null,
  "custom_id": "otp-1234"
}
```

This will apply the variables to your template and send the resulting message.

### Batch Sending with Templates

For sending to multiple recipients with different variables:

```http
POST /api/v1/messages/batch
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "messages": [
    {
      "phone_number": "+1234567890",
      "message": "Your custom message using {{variable}} syntax",
      "custom_id": "batch-1"
    },
    {
      "phone_number": "+0987654321",
      "message": "Another message with {{different}} variable",
      "custom_id": "batch-2"
    }
  ],
  "options": {
    "delay_between_messages": 0.3,
    "fail_on_first_error": false
  }
}
```

## Managing Templates

### Listing Templates

```http
GET /api/v1/templates?active_only=true
Authorization: Bearer YOUR_TOKEN
```

### Getting a Specific Template

```http
GET /api/v1/templates/{template_id}
Authorization: Bearer YOUR_TOKEN
```

### Updating a Template

```http
PUT /api/v1/templates/{template_id}
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "name": "Updated OTP Template",
  "content": "Your code is {{code}}. Valid for {{minutes}} minutes.",
  "is_active": true
}
```

### Deleting a Template

```http
DELETE /api/v1/templates/{template_id}
Authorization: Bearer YOUR_TOKEN
```

## Best Practices

1. **Descriptive Variable Names**: Use clear, descriptive variable names that indicate what data should be inserted.

2. **Test Before Sending**: Always use the `/templates/apply` endpoint to test how your template will look with real data before sending messages.

3. **Handle Missing Variables**: Check the `missing_variables` field in responses to ensure all required variables are provided.

4. **Version Your Templates**: If you need to make significant changes to a template that's in use, consider creating a new version instead of updating the existing one.

5. **Keep Templates Simple**: Avoid complex formatting that might not render well on all mobile devices.

6. **Include Message Signature**: Consider including your company name or service identifier at the end of templates to help recipients identify the sender.

## Examples

### Appointment Reminder

```
Template Content:
"Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule."

Applied with:
{
  "name": "John",
  "date": "May 5, 2025",
  "time": "2:30 PM",
  "phone": "555-123-4567"
}

Result:
"Hi John, this is a reminder for your appointment on May 5, 2025 at 2:30 PM. Reply YES to confirm or call 555-123-4567 to reschedule."
```

### Order Confirmation

```
Template Content:
"Your order #{{order_id}} has been confirmed! Estimated delivery: {{delivery_date}}. Track your package at {{tracking_url}}. Thanks for shopping with {{company_name}}!"

Applied with:
{
  "order_id": "1234567",
  "delivery_date": "Apr 28-30, 2025",
  "tracking_url": "https://track.example.com/1234567",
  "company_name": "Example Shop"
}

Result:
"Your order #1234567 has been confirmed! Estimated delivery: Apr 28-30, 2025. Track your package at https://track.example.com/1234567. Thanks for shopping with Example Shop!"
```
</file>

<file path="docs/tofix.md">
CSV import can exhaust memory on large files
await file.read() loads the entire upload into RAM. For big CSVs (> ~50 MB) this will block the event-loop and may kill small instances.



2.


1. Problem Statement
All three “send” endpoints in app/api/v1/endpoints/messages.py accept a BackgroundTasks object but never enqueue tasks with it.

py
Copy
Edit
async def send_message(..., background_tasks: BackgroundTasks, ...)
    ...
    result = await sms_sender.send_message(...)   # ← blocks request
As a result, every request waits for the SMS gateway (or bulk CSV parsing) before returning, contradicting the 202 “accepted / processing” contract and limiting throughput.

2. Impact
Area	Consequence
Latency / UX	Client sits idle for seconds on large batches; 202 is misleading.
Throughput	Long-running awaits tie up the event loop → fewer concurrent requests.
Scalability	Memory spike when importing large CSVs (await file.read() loads whole file in RAM).
Task progress	/tasks/{id} endpoint often returns 404 because no task records are created.
...








important for mvp:


Inboxerr – Campaign Message Personalization & Integration Update
1. Objective Overview
Enforce every campaign includes message content (raw text or template) at creation.

Enable dynamic, per-recipient message personalization (“merge fields”) via CSV import (e.g., {{name}}, {{order_no}}, etc.).

Flow: Create Campaign (w/ message/template) → Attach contacts (with variables) → Start → Generate/send personalized Message records.

2. Schema Changes (Pydantic & DB)
CampaignCreate, CampaignCreateFromCSV:

Add: message_content: Optional[str]

Add: template_id: Optional[str]

Enforce at least one required (via root validator).

CSV Import: Accept extra columns as variables; for each contact, store variables JSON.

Message Model:

Add: variables: JSON column per message for per-recipient substitution.

CampaignResponse:

Return message_content, template_id (whichever used).

DB:

Migrate Campaign and Message tables as above.

3. Endpoint Behavior Updates
POST /api/v1/campaigns

Accepts message_content or template_id (reject if neither).

Stores accordingly.

POST /api/v1/campaigns/from-csv

Accept campaign data (with content or template), plus CSV with extra columns mapped as variables per row.

For each recipient: create Message with correct variables.

POST /api/v1/campaigns/{id}/start

For each Message:

If template_id: render message using stored variables JSON per recipient.

If message_content: use as-is (no variables).

4. Edge Cases & Validations
Both fields provided: template takes precedence or return error.

Only template_id: validate existence, ownership, and activeness.

Only message_content: validate length, reject if empty.

CSV upload: error if required columns (e.g., phone) missing, or variable columns don’t match template variables.

5. Security & Permissions
User can only reference their own template_id.

Only owner can create/edit campaigns/messages.

6. Testing & Docs
Cover:

Campaign creation failure (no content/template).

Success with either.

Multiple message records, correct variable mapping.

Template variable substitution logic.

Update API docs/OpenAPI for new fields and logic.

7. Sample Flow
Campaign create (template_id or content required).

CSV upload:

Phone, name, order_no columns (for example).

Store:

phone: "+15555555555"

variables: { "name": "Bob", "order_no": "12345" }

Start campaign:

For each message, render template with per-recipient variables, send.

Inboxerr – Campaign Message Personalization & Integration Update
1. Objective Overview
Enforce every campaign includes message content (raw text or template) at creation.

Enable dynamic, per-recipient message personalization (“merge fields”) via CSV import (e.g., {{name}}, {{order_no}}, etc.).

Flow: Create Campaign (w/ message/template) → Attach contacts (with variables) → Start → Generate/send personalized Message records.

2. Schema Changes (Pydantic & DB)
CampaignCreate, CampaignCreateFromCSV:

Add: message_content: Optional[str]

Add: template_id: Optional[str]

Enforce at least one required (via root validator).

CSV Import: Accept extra columns as variables; for each contact, store variables JSON.

Message Model:

Add: variables: JSON column per message for per-recipient substitution.

CampaignResponse:

Return message_content, template_id (whichever used).

DB:

Migrate Campaign and Message tables as above.

3. Endpoint Behavior Updates
POST /api/v1/campaigns

Accepts message_content or template_id (reject if neither).

Stores accordingly.

POST /api/v1/campaigns/from-csv

Accept campaign data (with content or template), plus CSV with extra columns mapped as variables per row.

For each recipient: create Message with correct variables.

POST /api/v1/campaigns/{id}/start

For each Message:

If template_id: render message using stored variables JSON per recipient.

If message_content: use as-is (no variables).

4. Edge Cases & Validations
Both fields provided: template takes precedence or return error.

Only template_id: validate existence, ownership, and activeness.

Only message_content: validate length, reject if empty.

CSV upload: error if required columns (e.g., phone) missing, or variable columns don’t match template variables.

5. Security & Permissions
User can only reference their own template_id.

Only owner can create/edit campaigns/messages.

6. Testing & Docs
Cover:

Campaign creation failure (no content/template).

Success with either.

Multiple message records, correct variable mapping.

Template variable substitution logic.

Update API docs/OpenAPI for new fields and logic.

7. Sample Flow
Campaign create (template_id or content required).

CSV upload:

Phone, name, order_no columns (for example).

Store:

phone: "+15555555555"

variables: { "name": "Bob", "order_no": "12345" }

Start campaign:

For each message, render template with per-recipient variables, send.





------------CSSVV-------


CSV → Contacts → Queued Messages — Backend Architecture Specification (FINAL)
(Share this with the backend team; it contains every decision agreed so far, including the library pick.)

1 — Objectives
Goal	Success Metric
MVP CSV upload	100 k-row file parses & queues in < 3 min on a single mid-tier VM
Zero RAM spikes	RSS remains < 300 MB during import
No raw-file retention	Temp file deleted immediately after parse; only SHA-256 stored
Future-proof queue layer	Parsing & dispatch logic unchanged when moving from in-process tasks to Celery/RQ

2 — End-to-End Flow
POST /imports/csv
Streams file to /tmp/{import_id}.csv, computes SHA-256, creates import_jobs row (status=processing), returns 202.

BackgroundTasks schedules process_csv(import_id, path) immediately.

Parser (process_csv)
Reads with csv.DictReader, validates, bulk-inserts Contacts every 1 000 rows, updates counters.

Deletes temp file, updates import_jobs → status=success | error.

Campaign creation accepts import_id; a single INSERT … SELECT pre-creates Message rows (status=queued).

Dispatcher loop (async task in app-lifespan) dequeues 50 queued messages at a time using FOR UPDATE SKIP LOCKED, calls sms_sender, updates status, repeats.

(Later Celery/RQ upgrade: only the runner that invokes process_csv and the dispatcher registration change; the inner logic stays identical.)

3 — Data Model Additions
Table	Key Columns	Notes
import_jobs	id UUID, filename, sha256, `status enum(processing	success
contacts	id UUID, import_id FK, phone, name, tags[], created_at; unique (import_id, phone)	
(optional Phase-2) campaign_batches	id UUID, campaign_id FK, range_start, range_end, status	

messages already exists; add an optional import_id FK for lineage.

4 — API Surface
Method	Path	Description
POST	/api/v1/imports/csv	Multipart CSV upload → {import_id} & 202
GET	/api/v1/imports/{id}	Progress (status, row counts, sha256)
GET	/api/v1/imports/{id}/contacts	Paginated preview (first 100 rows)
POST	/api/v1/campaigns	Existing payload + import_id (alternative to contact list)

5 — Responsibilities & File Map
Component	File (suggested)	Notes
Upload handler	app/api/v1/endpoints/imports.py	Streams, hashes, enqueues parser
Parser	app/services/imports/parser.py	Validates rows, bulk inserts, deletes temp file
ImportJob model	app/models/import_job.py	Enum ImportStatus shared in constants
Dispatcher loop	app/services/sms/dispatcher.py	Async loop, FOR UPDATE SKIP LOCKED, asyncio.Semaphore
Alembic migration	alembic/versions/<ts>_add_import_tables.py	Adds tables & FKs

6 — Library Decision (MVP)
Library	Pros	Cons	Decision
Std-lib csv.DictReader	Zero new deps, true streaming, constant memory, parses 100 k rows ≈ 2 s	Pure-Python (slower for > 1 M rows)	✅ Use now
pandas read_csv(chunksize)	Fast C engine, familiar API	Heavy wheel (~35 MB), higher RAM, unnecessary for current scale	Park for analytics phase
Polars Arrow read_csv(streaming)	Extremely fast, out-of-core	New binary dep, team unfamiliar	Evaluate in Phase-2 (millions of rows)

→ Implement parser with csv.DictReader; swapping to Pandas/Polars later is a one-liner.

7 — Scaling & Switch-over Plan
Stage	Trigger	Action
Phase 0 — BackgroundTasks	Works to ~200 k rows/day on one VM	Monitor p95 latency, CPU
Phase 1 — Celery/RQ	p95 > 5 min or CPU > 75 % sustained	Add Redis/Rabbit, 2-4 worker containers
Phase 2 — Horizontal scale	> 1 M rows/day	Partition messages, adopt campaign_batches, more workers

8 — Security & Compliance
Raw CSV deleted post-parse; only SHA-256 retained.

/tmp mounted with noexec,nosuid; daily cron cleans orphan files.

Future audit need → stream to S3 instead of /tmp (30-day lifecycle), same parser.

9 — Engineering Task List
DB migrations — import_jobs, contacts, add import_id FK to messages.

Add ImportStatus enum in shared constants.

Implement upload endpoint with streaming + SHA-256.

Build parser service with bulk inserts (1 000 rows/commit) and file deletion.

Extend campaign creation to accept import_id and pre-create queued messages.

Implement dispatcher loop with FOR UPDATE SKIP LOCKED, asyncio.Semaphore.

Unit & load tests:

100 k-row synthetic CSV completes < 3 min.

Dispatcher sends 10 k messages, no duplicates/deadlocks.

Add Prometheus counters: import_rows_processed_total, messages_sent_total, dispatcher_loop_duration_seconds.
</file>

<file path="scripts/generate_migration.py">
#!/usr/bin/env python
"""
Generate Alembic migrations.

This script parses database credentials from settings and runs alembic to generate migrations.

Usage:
    python scripts/generate_migration.py "Add message templates"
"""
import sys
import subprocess
import re
from pathlib import Path

# Add parent directory to path to allow importing from app
sys.path.append(str(Path(__file__).parent.parent))

# Import after adding to path
from app.core.config import settings

def generate_migration(message):
    """Generate an Alembic migration with the given message."""
    try:
        # Set the Alembic database URL from settings
        # Replace asyncpg with standard psycopg2 for Alembic
        db_url = settings.DATABASE_URL.replace("postgresql+asyncpg", "postgresql")
        
        # Set environment variable for Alembic
        import os
        os.environ["ALEMBIC_DB_URL"] = db_url
        
        # Run alembic command
        print(f"Generating migration: {message}")
        result = subprocess.run(
            ["alembic", "revision", "--autogenerate", "-m", message], 
            check=True,
            capture_output=True,
            text=True
        )
        
        # Parse output to find migration file path
        output = result.stdout
        file_pattern = r"Generating .*\\(.*?\.py)"
        match = re.search(file_pattern, output)
        
        if match:
            migration_file = match.group(1)
            print(f"✅ Successfully generated migration: {migration_file}")
        else:
            print(f"✅ Successfully generated migration")
            
        print("⚠️ Please review the generated migration file to ensure it's correct.")
        print("💡 To apply the migration, run: alembic upgrade head")
        
    except subprocess.CalledProcessError as e:
        print(f"❌ Error generating migration: {e}")
        print("Error output:")
        print(e.stderr)
        print("💡 Make sure alembic is installed and your database is running.")
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("❌ Error: Please provide a migration message.")
        print("💡 Example: python scripts/generate_migration.py 'Add user table'")
        sys.exit(1)
    
    message = sys.argv[1]
    generate_migration(message)
</file>

<file path="scripts/mvp.sh">
#!/bin/bash
# MVP Setup and Run Script
# This script helps set up and run all components needed for the MVP

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}=== Inboxerr MVP Setup and Run ===${NC}"
echo "This script will help you set up and run all MVP components"

# Extract database connection details from settings
get_db_info() {
  echo "Extracting database connection details..."
  
  # Run a Python script to get DB connection info from settings
  python -c "
from app.core.config import settings
import re

# Parse the connection URL
url = settings.DATABASE_URL
pattern = r'postgresql(?:\+asyncpg)?://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)'
match = re.match(pattern, url)

if match:
    print(f'DB_USER={match.group(1)}')
    print(f'DB_PASS={match.group(2)}')
    print(f'DB_HOST={match.group(3)}')
    print(f'DB_PORT={match.group(4)}')
    print(f'DB_NAME={match.group(5)}')
else:
    print('Could not parse database URL')
  "
}

# Source the DB info (if Python script outputs variables)
eval "$(get_db_info)"

# Check if PostgreSQL is running using the extracted credentials
check_postgres() {
  if [ -z "$DB_HOST" ] || [ -z "$DB_PORT" ] || [ -z "$DB_USER" ]; then
    echo -e "${RED}Could not extract database connection details from settings.${NC}"
    return 1
  fi
  
  export PGPASSWORD=$DB_PASS
  pg_status=$(psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d postgres -c "SELECT 1;" 2>/dev/null)
  
  if [ $? -ne 0 ]; then
    echo -e "${RED}PostgreSQL is not running or connection failed.${NC}"
    echo "Please make sure PostgreSQL is running and connection details are correct:"
    echo "Host: $DB_HOST"
    echo "Port: $DB_PORT"
    echo "User: $DB_USER"
    echo "Database: $DB_NAME"
    return 1
  else
    echo -e "${GREEN}PostgreSQL connection successful.${NC}"
    return 0
  fi
}

# Check PostgreSQL connection
if ! check_postgres; then
  echo -e "${YELLOW}Would you like to continue anyway? (y/N)${NC}"
  read continue_anyway
  if [[ ! "$continue_anyway" =~ ^[Yy]$ ]]; then
    echo "Exiting."
    exit 1
  fi
fi

# Create directories if needed
if [ ! -d "tests" ]; then
  echo -e "${YELLOW}Creating test directories...${NC}"
  mkdir -p tests/unit
  mkdir -p tests/integration
  mkdir -p tests/unit/services
  mkdir -p tests/unit/repositories
  mkdir -p tests/unit/api
  mkdir -p tests/integration/api
  
  # Create empty __init__.py files
  touch tests/__init__.py
  touch tests/unit/__init__.py
  touch tests/integration/__init__.py
  touch tests/unit/services/__init__.py
  touch tests/unit/repositories/__init__.py
  touch tests/unit/api/__init__.py
  touch tests/integration/api/__init__.py
  
  echo -e "${GREEN}Test directory structure created!${NC}"
fi

# Function to show menu
show_menu() {
  echo ""
  echo -e "${BLUE}Available actions:${NC}"
  echo "1) Setup development database"
  echo "2) Generate database migrations"
  echo "3) Run migrations"
  echo "4) Run tests"
  echo "5) Start API server"
  echo "6) View API documentation"
  echo "q) Quit"
  echo ""
  echo -n "Enter your choice: "
}

# Main menu loop
while true; do
  show_menu
  read choice

  case $choice in
    1)
      echo -e "${YELLOW}Setting up development database...${NC}"
      python scripts/setup_test_db.py
      ;;
    2)
      echo -e "${YELLOW}Generating database migrations...${NC}"
      read -p "Enter migration description: " desc
      python scripts/generate_migration.py "$desc"
      ;;
    3)
      echo -e "${YELLOW}Running database migrations...${NC}"
      alembic upgrade head
      ;;
    4)
      echo -e "${YELLOW}Running tests...${NC}"
      python scripts/run_tests.py
      ;;
    5)
      echo -e "${YELLOW}Starting API server...${NC}"
      echo -e "${GREEN}API will be available at: http://localhost:8000${NC}"
      echo -e "${GREEN}API Docs URL: http://localhost:8000/api/docs${NC}"
      echo -e "${YELLOW}Press CTRL+C to stop the server${NC}"
      uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
      ;;
    6)
      echo -e "${YELLOW}Opening API documentation...${NC}"
      if command -v xdg-open &> /dev/null; then
        xdg-open http://localhost:8000/api/docs
      elif command -v open &> /dev/null; then
        open http://localhost:8000/api/docs
      elif command -v start &> /dev/null; then
        start http://localhost:8000/api/docs
      else
        echo -e "${RED}Cannot open browser automatically.${NC}"
        echo -e "${GREEN}Please visit: http://localhost:8000/api/docs${NC}"
      fi
      ;;
    q|Q)
      echo -e "${GREEN}Goodbye!${NC}"
      exit 0
      ;;
    *)
      echo -e "${RED}Invalid choice. Please try again.${NC}"
      ;;
  esac
done
</file>

<file path="scripts/reset_db.py">
"""
Reset the development database, run Alembic migrations, and seed initial data.
Works with PostgreSQL database.
"""
import sys
import os
import subprocess
from pathlib import Path



# Resolve project root dynamically
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def reset_database() -> None:
    """
    Reset the development database, run Alembic migrations, and seed initial data.
    This should only be used in development environments.
    """
    # Reset PostgreSQL database (drop and recreate)
    print("🗄️ Resetting PostgreSQL database...")
    try:
        # Connect to default postgres database to drop/create our database
        subprocess.run(
            ["psql", "-U", "postgres", "-c", "DROP DATABASE IF EXISTS inboxerr;"],
            check=True
        )
        subprocess.run(
            ["psql", "-U", "postgres", "-c", "CREATE DATABASE inboxerr;"],
            check=True
        )
        print("✅ Database reset successfully")
    except subprocess.CalledProcessError as e:
        print(f"❌ Error resetting database: {e}")
        print("💡 Make sure PostgreSQL is running and you have permissions")
        sys.exit(1)

    print("🚀 Running Alembic migrations...")
    subprocess.run(["alembic", "upgrade", "head"], check=True, cwd=PROJECT_ROOT)

    print("🌱 Seeding initial data...")
    subprocess.run(
        [sys.executable, "scripts/seed_db.py"], 
        check=True, 
        cwd=PROJECT_ROOT,
        env={**os.environ, "PYTHONPATH": str(PROJECT_ROOT)}
    )

    print("✅ Database reset and seeded successfully!")

if __name__ == "__main__":
    reset_database()
</file>

<file path="scripts/run_tests.py">
#!/usr/bin/env python
"""
Run application tests with pytest.

This script:
1. Creates a test database if it doesn't exist
2. Runs pytest with specified options
3. Generates a coverage report

Usage:
    python scripts/run_tests.py [pytest_args]
    
Examples:
    python scripts/run_tests.py                             # Run all tests
    python scripts/run_tests.py tests/integration           # Run integration tests
    python scripts/run_tests.py -v tests/unit/test_users.py # Run specific test with verbose output
"""
import sys
import os
import subprocess
import re
from pathlib import Path

# Add parent directory to sys.path
sys.path.append(str(Path(__file__).parent.parent))

# Import settings after adding to path
from app.core.config import settings

# Extract database connection info from the URL
def parse_db_url(url):
    """Parse database URL to extract connection information."""
    # PostgreSQL URL format: postgresql+asyncpg://user:password@host:port/dbname
    pattern = r"postgresql(?:\+asyncpg)?://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)"
    match = re.match(pattern, url)
    
    if match:
        return {
            "user": match.group(1),
            "password": match.group(2),
            "host": match.group(3),
            "port": match.group(4),
            "dbname": match.group(5)
        }
    return None

def run_tests():
    """Run tests with pytest."""
    # Get pytest arguments from command line
    pytest_args = sys.argv[1:] if len(sys.argv) > 1 else []
    
    # Create test database name
    db_info = parse_db_url(settings.DATABASE_URL)
    if not db_info:
        print("❌ Could not parse database URL. Please check the format.")
        return False
    
    test_db_name = f"{db_info['dbname']}_test"
    
    # Set up test database environment variable
    os.environ["DATABASE_URL"] = f"postgresql+asyncpg://{db_info['user']}:{db_info['password']}@{db_info['host']}:{db_info['port']}/{test_db_name}"
    os.environ["TESTING"] = "1"
    
    # Print the test database URL
    print(f"Using test database: {test_db_name}")
    
    # Get environment variables for PGPASSWORD to avoid password prompt
    env = os.environ.copy()
    env["PGPASSWORD"] = db_info["password"]
    
    # Check if test database exists
    try:
        check_db = subprocess.run(
            ["psql", 
             "-h", db_info["host"], 
             "-p", db_info["port"], 
             "-U", db_info["user"], 
             "-d", "postgres", 
             "-c", f"SELECT 1 FROM pg_database WHERE datname = '{test_db_name}';"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env
        )
        
        if "1 row" not in check_db.stdout.decode():
            print(f"Creating test database {test_db_name}...")
            create_db = subprocess.run(
                ["psql", 
                 "-h", db_info["host"], 
                 "-p", db_info["port"], 
                 "-U", db_info["user"], 
                 "-d", "postgres", 
                 "-c", f"CREATE DATABASE {test_db_name};"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env
            )
            
            if create_db.returncode != 0:
                print(f"❌ Failed to create test database: {create_db.stderr.decode()}")
                return False
            
            print(f"✅ Test database {test_db_name} created")
    except Exception as e:
        print(f"⚠️ Could not check/create test database: {e}")
        print("Continuing with tests...")
    
    # Default pytest arguments if none provided
    if not pytest_args:
        # Run all tests with coverage
        pytest_args = [
            "--cov=app",
            "--cov-report=term-missing",
            "--cov-report=html",
            "-v",
            "tests/"
        ]
    
    # Run pytest
    print(f"Running tests with args: {' '.join(pytest_args)}")
    result = subprocess.run(["pytest"] + pytest_args)
    
    # Print results
    if result.returncode == 0:
        print("✅ All tests passed!")
    else:
        print(f"❌ Tests failed with exit code: {result.returncode}")
    
    return result.returncode == 0


if __name__ == "__main__":
    sys.exit(0 if run_tests() else 1)
</file>

<file path="scripts/seed_frontend_data.py">
#!/usr/bin/env python
"""
Seed database with sample data for frontend development.

This script creates sample users, templates, messages and campaigns
to make frontend development easier.

Usage:
    python scripts/seed_frontend_data.py
"""
import asyncio
import sys
import os
from pathlib import Path
from datetime import datetime, timedelta, timezone
import random
import uuid

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

# Import app modules
from app.db.session import initialize_database, async_session_factory
from app.db.repositories.users import UserRepository
from app.db.repositories.templates import TemplateRepository
from app.db.repositories.messages import MessageRepository
from app.db.repositories.campaigns import CampaignRepository
from app.core.security import get_password_hash
from app.schemas.message import MessageStatus

# Sample phone numbers
PHONE_NUMBERS = [
    "+12025550108", "+12025550112", "+12025550118", "+12025550121",
    "+12025550125", "+12025550132", "+12025550139", "+12025550144",
    "+12025550152", "+12025550158", "+12025550165", "+12025550171"
]

# Sample message templates
TEMPLATES = [
    {
        "name": "Welcome Message",
        "content": "Hi {{name}}, welcome to our service! We're glad you've joined us.",
        "description": "Template for welcoming new users"
    },
    {
        "name": "OTP Verification",
        "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
        "description": "Template for sending OTP codes"
    },
    {
        "name": "Appointment Reminder",
        "content": "Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule.",
        "description": "Template for appointment reminders"
    },
    {
        "name": "Order Confirmation",
        "content": "Your order #{{order_id}} has been confirmed! Estimated delivery: {{delivery_date}}. Track your package at {{tracking_url}}",
        "description": "Order confirmation message"
    },
    {
        "name": "Payment Reminder",
        "content": "Reminder: Your payment of ${{amount}} is due on {{due_date}}. Please ensure your account has sufficient funds.",
        "description": "Payment reminder notification"
    }
]

# Sample messages
MESSAGES = [
    "Your verification code is 123456",
    "Your appointment is confirmed for tomorrow at 10:00 AM",
    "Your order #12345 has been shipped and will arrive on Friday",
    "Thank you for your payment of $99.99",
    "Your subscription will renew on May 15, 2025",
    "Your account password has been reset successfully",
    "Your flight PO491 has been delayed by 30 minutes",
    "Your table reservation at Milano Restaurant is confirmed",
    "Your prescription is ready for pickup at Central Pharmacy",
    "Reminder: You have a meeting scheduled in 1 hour"
]

# Sample campaign names
CAMPAIGN_NAMES = [
    "Spring Sale Promotion",
    "Customer Feedback Survey",
    "Product Launch Announcement",
    "Abandoned Cart Reminder",
    "Loyalty Program Update"
]

async def create_test_user():
    """Create a test user if it doesn't exist."""
    async with async_session_factory() as session:
        # Create user repository
        user_repo = UserRepository(session)
        
        # Check if user exists
        existing_user = await user_repo.get_by_email("test@example.com")
        if existing_user:
            print(f"✅ Test user test@example.com already exists")
            return existing_user
        
        # Create user
        hashed_password = get_password_hash("Test1234!")
        user = await user_repo.create(
            email="test@example.com",
            hashed_password=hashed_password,
            full_name="Test User",
            role="user"
        )
        
        print(f"✅ Created test user: {user.email}")
        return user

async def create_test_templates(user_id):
    """Create test message templates."""
    async with async_session_factory() as session:
        # Create template repository
        template_repo = TemplateRepository(session)
        
        created_templates = []
        # Create templates
        for template_data in TEMPLATES:
            template = await template_repo.create_template(
                name=template_data["name"],
                content=template_data["content"],
                description=template_data["description"],
                user_id=user_id
            )
            created_templates.append(template)
            print(f"✅ Created template: {template_data['name']}")
        
        return created_templates

async def create_test_messages(user_id, template_id=None):
    """Create test messages with different statuses."""
    async with async_session_factory() as session:
        # Create message repository
        message_repo = MessageRepository(session)
        
        # Generate different message statuses
        statuses = [
            MessageStatus.PENDING,
            MessageStatus.SENT,
            MessageStatus.DELIVERED,
            MessageStatus.FAILED,
            MessageStatus.SCHEDULED
        ]
        
        created_messages = []
        # Create messages with different statuses
        for i, message_text in enumerate(MESSAGES):
            phone = random.choice(PHONE_NUMBERS)
            status = statuses[i % len(statuses)]
            
            # For scheduled messages, set a future time
            scheduled_at = None
            if status == MessageStatus.SCHEDULED:
                scheduled_at = datetime.now(timezone.utc) + timedelta(days=1)
            
            # Create message
            message = await message_repo.create_message(
                phone_number=phone,
                message_text=message_text,
                user_id=user_id,
                custom_id=f"sample-{uuid.uuid4().hex[:8]}",
                scheduled_at=scheduled_at,
                metadata={"sample": True, "template_id": template_id}
            )
            
            # If not scheduled, update to the appropriate status
            if status != MessageStatus.SCHEDULED and status != MessageStatus.PENDING:
                # Update message status
                await message_repo.update_message_status(
                    message_id=message.id,
                    status=status,
                    event_type="seeded_data",
                    reason="Sample data" if status == MessageStatus.FAILED else None,
                    gateway_message_id=f"gw-{uuid.uuid4()}" if status != MessageStatus.PENDING else None
                )
            
            created_messages.append(message)
            print(f"✅ Created message with status {status}: {message_text[:30]}...")
        
        return created_messages

async def create_test_campaigns(user_id):
    """Create test campaigns."""
    async with async_session_factory() as session:
        # Create campaign repository
        campaign_repo = CampaignRepository(session)
        message_repo = MessageRepository(session)
        
        created_campaigns = []
        # Create campaigns with different statuses
        statuses = ["draft", "active", "paused", "completed", "cancelled"]
        
        for i, name in enumerate(CAMPAIGN_NAMES):
            status = statuses[i % len(statuses)]
            
            # Create campaign
            campaign = await campaign_repo.create_campaign(
                name=name,
                description=f"Sample campaign: {name}",
                user_id=user_id,
                scheduled_start_at=datetime.now(timezone.utc) + timedelta(days=1),
                scheduled_end_at=datetime.now(timezone.utc) + timedelta(days=2),
                settings={"sample": True}
            )
            
            # Add 3-5 messages to each campaign
            msg_count = random.randint(3, 5)
            for j in range(msg_count):
                phone = random.choice(PHONE_NUMBERS)
                message_text = f"Campaign {name}: {random.choice(MESSAGES)}"
                
                await message_repo.create_message(
                    phone_number=phone,
                    message_text=message_text,
                    user_id=user_id,
                    campaign_id=campaign.id,
                    metadata={"campaign": name}
                )
            
            # Update campaign stats
            campaign.total_messages = msg_count
            
            # Update campaign status
            if status != "draft":
                await campaign_repo.update_campaign_status(
                    campaign_id=campaign.id,
                    status=status,
                    started_at=datetime.now(timezone.utc) - timedelta(days=1) if status != "draft" else None,
                    completed_at=datetime.now(timezone.utc) if status in ["completed", "cancelled"] else None
                )
                
                # Update stats for non-draft campaigns
                if status in ["active", "paused", "completed"]:
                    sent = msg_count if status in ["completed"] else random.randint(1, msg_count)
                    delivered = random.randint(0, sent) if status in ["completed"] else 0
                    failed = random.randint(0, msg_count - sent) if status in ["completed"] else 0
                    
                    await campaign_repo.update_campaign_stats(
                        campaign_id=campaign.id,
                        increment_sent=sent,
                        increment_delivered=delivered,
                        increment_failed=failed
                    )
            
            created_campaigns.append(campaign)
            print(f"✅ Created campaign with status {status}: {name}")
        
        return created_campaigns

async def seed_database():
    """Seed database with sample data."""
    print("🌱 Seeding database with frontend development data...")
    
    # Initialize database
    await initialize_database()
    
    # Create test user
    user = await create_test_user()
    
    # Create templates
    templates = await create_test_templates(user.id)
    
    # Create messages
    if templates:
        await create_test_messages(user.id, templates[0].id)
    else:
        await create_test_messages(user.id)
    
    # Create campaigns
    await create_test_campaigns(user.id)
    
    print("\n✅ Database seeded successfully with frontend development data!")
    print(f"📱 Test user: test@example.com")
    print(f"🔑 Password: Test1234!")

if __name__ == "__main__":
    asyncio.run(seed_database())
</file>

<file path="scripts/setup_test_db.py">
#!/usr/bin/env python
"""
Setup a test database for development.

This script:
1. Checks if the test database exists
2. Creates it if it doesn't
3. Runs all migrations
4. Seeds it with test data

Usage:
    python scripts/setup_test_db.py
"""
import sys
import os
import asyncio
import subprocess
from pathlib import Path
import re

# Add parent directory to path to allow importing app
sys.path.append(str(Path(__file__).parent.parent))

# Import app modules
from app.core.config import settings
from app.db.session import initialize_database
from app.db.repositories.users import UserRepository
from app.db.repositories.templates import TemplateRepository
from app.core.security import get_password_hash

# Test data to seed
TEST_USER = {
    "email": "test@example.com",
    "password": "Test1234!",
    "full_name": "Test User",
    "role": "user"
}

TEST_TEMPLATES = [
    {
        "name": "Welcome Message",
        "content": "Hi {{name}}, welcome to our service! We're glad you've joined us.",
        "description": "Template for welcoming new users"
    },
    {
        "name": "OTP Verification",
        "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
        "description": "Template for sending OTP codes"
    },
    {
        "name": "Appointment Reminder",
        "content": "Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule.",
        "description": "Template for appointment reminders"
    }
]

# Extract database connection info from the URL
def parse_db_url(url):
    """Parse database URL to extract connection information."""
    # PostgreSQL URL format: postgresql+asyncpg://user:password@host:port/dbname
    pattern = r"postgresql(?:\+asyncpg)?://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)"
    match = re.match(pattern, url)
    
    if match:
        return {
            "user": match.group(1),
            "password": match.group(2),
            "host": match.group(3),
            "port": match.group(4),
            "dbname": match.group(5)
        }
    return None

async def create_test_user():
    """Create a test user if it doesn't exist."""
    from app.db.session import async_session_factory
    
    async with async_session_factory() as session:
        # Create user repository
        user_repo = UserRepository(session)
        
        # Check if user exists
        existing_user = await user_repo.get_by_email(TEST_USER["email"])
        if existing_user:
            print(f"✅ Test user {TEST_USER['email']} already exists")
            return existing_user
        
        # Create user
        hashed_password = get_password_hash(TEST_USER["password"])
        user = await user_repo.create(
            email=TEST_USER["email"],
            hashed_password=hashed_password,
            full_name=TEST_USER["full_name"],
            role=TEST_USER["role"]
        )
        
        print(f"✅ Created test user: {user.email}")
        return user

async def create_test_templates(user_id):
    """Create test message templates."""
    from app.db.session import async_session_factory
    
    async with async_session_factory() as session:
        # Create template repository
        template_repo = TemplateRepository(session)
        
        # Create templates
        for template_data in TEST_TEMPLATES:
            await template_repo.create_template(
                name=template_data["name"],
                content=template_data["content"],
                description=template_data["description"],
                user_id=user_id
            )
            print(f"✅ Created template: {template_data['name']}")

async def setup_database():
    """Setup the test database."""
    try:
        # Get database configuration from settings
        db_info = parse_db_url(settings.DATABASE_URL)
        if not db_info:
            print("❌ Could not parse database URL. Please check the format.")
            return False
        
        # Extract database name and create connection to postgres database
        db_name = db_info["dbname"]
        postgres_url = f"postgresql://{db_info['user']}:{db_info['password']}@{db_info['host']}:{db_info['port']}/postgres"
        
        # Get environment variables for PGPASSWORD to avoid password prompt
        env = os.environ.copy()
        env["PGPASSWORD"] = db_info["password"]
        
        # Test connection using psql
        connection_test = subprocess.run(
            ["psql", 
             "-h", db_info["host"], 
             "-p", db_info["port"], 
             "-U", db_info["user"], 
             "-d", "postgres", 
             "-c", "SELECT 1;"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env
        )
        
        if connection_test.returncode != 0:
            print("❌ Could not connect to PostgreSQL server.")
            print(connection_test.stderr.decode())
            print("Please make sure PostgreSQL is running and the credentials are correct.")
            return False
        
        # Check if database exists
        check_db = subprocess.run(
            ["psql", 
             "-h", db_info["host"], 
             "-p", db_info["port"], 
             "-U", db_info["user"], 
             "-d", "postgres", 
             "-c", f"SELECT 1 FROM pg_database WHERE datname = '{db_name}';"],
            stdout=subprocess.PIPE,
            env=env
        )
        
        if "1 row" not in check_db.stdout.decode():
            print(f"Creating database {db_name}...")
            create_db = subprocess.run(
                ["psql", 
                 "-h", db_info["host"], 
                 "-p", db_info["port"], 
                 "-U", db_info["user"], 
                 "-d", "postgres", 
                 "-c", f"CREATE DATABASE {db_name};"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env
            )
            
            if create_db.returncode != 0:
                print(f"❌ Failed to create database {db_name}")
                print(create_db.stderr.decode())
                return False
            
            print(f"✅ Database {db_name} created successfully")
        else:
            print(f"✅ Database {db_name} already exists")
        
        # Run migrations
        print("Running Alembic migrations...")
        alembic = subprocess.run(
            ["alembic", "upgrade", "head"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        if alembic.returncode != 0:
            print("❌ Failed to run migrations")
            print(alembic.stderr.decode())
            return False
        
        print("✅ Migrations applied successfully")
        
        # Initialize database
        await initialize_database()
        
        # Create test user
        user = await create_test_user()
        
        # Create test templates
        await create_test_templates(user.id)
        
        print("\n🎉 Test database setup complete!")
        print(f"📝 Test user: {TEST_USER['email']}")
        print(f"🔑 Password: {TEST_USER['password']}")
        
        return True
        
    except Exception as e:
        print(f"❌ Error setting up test database: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(setup_database())
</file>

<file path="tests/core_functionality_test.py">
import pytest
import asyncio
from datetime import datetime, timezone, timedelta
from uuid import uuid4
from unittest.mock import AsyncMock, patch, MagicMock

from app.schemas.message import MessageStatus
from app.db.repositories.messages import MessageRepository
from app.db.repositories.templates import TemplateRepository
from app.db.repositories.campaigns import CampaignRepository
from app.services.event_bus.bus import get_event_bus
from app.services.sms.sender import SMSSender
from app.utils.phone import validate_phone


@pytest.fixture
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


import pytest_asyncio

@pytest_asyncio.fixture
async def event_bus():
    bus = get_event_bus()
    await bus.initialize()
    yield bus
    await bus.shutdown()


@pytest.fixture
def mock_session():
    session = AsyncMock()
    session.commit = AsyncMock()
    session.rollback = AsyncMock()
    session.close = AsyncMock()
    session.execute = AsyncMock()
    session.refresh = AsyncMock()
    session.add = MagicMock()
    session.delete = AsyncMock()
    # Mock session.begin for async context
    session.begin = MagicMock(return_value=AsyncMock(__aenter__=AsyncMock(return_value=None), __aexit__=AsyncMock(return_value=None)))
    return session


@pytest.fixture
def message_repository(mock_session):
    return MessageRepository(mock_session)


@pytest.fixture
def template_repository(mock_session):
    return TemplateRepository(mock_session)


@pytest.fixture
def campaign_repository(mock_session):
    return CampaignRepository(mock_session)


@pytest.fixture
def sms_sender(message_repository, event_bus):
    return SMSSender(message_repository, event_bus)


def test_phone_validation_valid_numbers():
    valid_numbers = ["+12025550108", "+447911123456", "+61412345678", "+33612345678"]
    for number in valid_numbers:
        is_valid, formatted, error, _ = validate_phone(number)
        assert is_valid
        assert formatted.startswith("+")


def test_phone_validation_invalid_numbers():
    invalid_numbers = ["not a number", "123", "+1234567890123456789", "+123abcd5678"]
    for number in invalid_numbers:
        is_valid, *_ = validate_phone(number)
        assert not is_valid


@pytest.mark.asyncio
async def test_create_message(message_repository):
    message_repository.session.refresh.side_effect = lambda x: x
    mock_campaign = MagicMock()
    mock_campaign.total_messages = 0
    with patch("app.db.repositories.campaigns.CampaignRepository.get_by_id", new_callable=AsyncMock, return_value=mock_campaign):
        message = await message_repository.create_message(
            phone_number="+12025550108",
            message_text="Test message",
            user_id="user-123",
            custom_id="custom-123",
            campaign_id="campaign-123"
        )
        assert message.phone_number == "+12025550108"
        assert message.status == MessageStatus.PENDING
        assert message_repository.session.add.call_count >= 1


@pytest.mark.asyncio
async def test_update_message_status(message_repository):
    message = MagicMock()
    message.id = "msg-123"
    message_repository.get_by_id = AsyncMock(return_value=message)
    message_repository.session.refresh.side_effect = lambda x: x
    updated_message = await message_repository.update_message_status(
        message_id="msg-123",
        status=MessageStatus.SENT,
        event_type="test",
        gateway_message_id="gw-123"
    )
    assert updated_message is not None
    message_repository.get_by_id.assert_awaited_with("msg-123")
    assert message_repository.session.add.call_count >= 1


@pytest.mark.asyncio
async def test_apply_template(template_repository):
    template = MagicMock()
    template.content = "Hello {{name}}, your code is {{code}}"
    template_repository.get_by_id = AsyncMock(return_value=template)
    result = await template_repository.apply_template(
        template_id="template-123",
        variables={"name": "John", "code": "123456"}
    )
    assert result == "Hello John, your code is 123456"
    template_repository.get_by_id.assert_awaited_with("template-123")


@pytest.mark.asyncio
async def test_create_template(template_repository):
    template_repository.session.refresh.side_effect = lambda x: x
    template = await template_repository.create_template(
        name="Test Template",
        content="Hello {{name}}",
        description="Test description",
        user_id="user-123"
    )
    assert template.name == "Test Template"
    template_repository.session.add.assert_called_with(template)
    template_repository.session.commit.assert_called()


@pytest.mark.asyncio
async def test_create_campaign(campaign_repository):
    campaign_repository.session.refresh.side_effect = lambda x: x
    campaign = await campaign_repository.create_campaign(
        name="Test Campaign",
        description="Test description",
        user_id="user-123",
        scheduled_start_at=datetime.now(timezone.utc) + timedelta(days=1)
    )
    assert campaign.name == "Test Campaign"
    campaign_repository.session.add.assert_called_with(campaign)
    campaign_repository.session.commit.assert_called()


@pytest.mark.asyncio
async def test_update_campaign_status(campaign_repository):
    campaign = MagicMock()
    campaign.id = "campaign-123"
    campaign.status = "draft"
    campaign_repository.get_by_id = AsyncMock(return_value=campaign)
    with patch("app.services.event_bus.bus.get_event_bus") as mock_get_bus:
        mock_bus = AsyncMock()
        mock_get_bus.return_value = mock_bus
        updated_campaign = await campaign_repository.update_campaign_status(
            campaign_id="campaign-123",
            status="active"
        )
        assert updated_campaign is not None
        assert campaign.status == "active"
        campaign_repository.session.add.assert_called_with(campaign)
        assert mock_bus.publish.called


@pytest.mark.asyncio
async def test_event_bus_subscribe_publish(event_bus):
    callback = AsyncMock()
    subscriber_id = await event_bus.subscribe("test_event", callback)
    assert event_bus.get_subscriber_count("test_event") == 1
    test_data = {"test": "data"}
    success = await event_bus.publish("test_event", test_data)
    assert success
    callback.assert_called_once()
    call_args = callback.call_args[0][0]
    assert call_args["test"] == "data"
    assert call_args["event_type"] == "test_event"
    assert "timestamp" in call_args
    assert "event_id" in call_args
    await event_bus.unsubscribe("test_event", subscriber_id)
    assert event_bus.get_subscriber_count("test_event") == 0


@pytest.mark.asyncio
async def test_sms_sender_send_message(sms_sender):
    sms_sender._send_to_gateway = AsyncMock(return_value={
        "status": MessageStatus.SENT,
        "gateway_message_id": "gw-123"
    })

    mock_message = MagicMock()
    mock_message.id = "msg-123"
    mock_message.dict = MagicMock(return_value={"id": "msg-123"})

    sms_sender.message_repository.create_message = AsyncMock(return_value=mock_message)
    sms_sender.message_repository.update_message_status = AsyncMock(return_value=mock_message)
    sms_sender.message_repository.get_by_id = AsyncMock(return_value=mock_message)  # ✅ add this line

    result = await sms_sender.send_message(
        phone_number="+12025550108",
        message_text="Test message",
        user_id="user-123"
    )

    assert result == {"id": "msg-123"}
    assert sms_sender.message_repository.create_message.called
    assert sms_sender._send_to_gateway.called
    assert sms_sender.message_repository.update_message_status.called
</file>

<file path="tests/unit/api/messages/test_messages_endpoints.py">
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_send_valid_batch(async_client: AsyncClient, override_auth):
    payload = {
        "messages": [
            {"phone_number": "+1234567890", "message": "Batch Msg 1"},
            {"phone_number": "+1987654321", "message": "Batch Msg 2"}
        ],
        "options": {}
    }
    response = await async_client.post("/api/v1/messages/batch", json=payload)
    assert response.status_code == 202
    assert "results" in response.json()

@pytest.mark.asyncio
async def test_send_empty_batch(async_client: AsyncClient, override_auth):
    payload = {
        "messages": [],
        "options": {}
    }
    response = await async_client.post("/api/v1/messages/batch", json=payload)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_batch_with_invalid_phone(async_client: AsyncClient, override_auth):
    payload = {
        "messages": [
            {"phone_number": "invalid", "message": "Failing message"}
        ],
        "options": {}
    }
    response = await async_client.post("/api/v1/messages/batch", json=payload)
    assert response.status_code in [422, 502]


@pytest.mark.asyncio
async def test_delete_existing_message(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    response = await async_client.delete(f"/api/v1/messages/{message_id}")
    assert response.status_code == 204

@pytest.mark.asyncio
async def test_delete_nonexistent_message(async_client: AsyncClient, override_auth):
    message_id = "nonexistent-msg-id"
    response = await async_client.delete(f"/api/v1/messages/{message_id}")
    assert response.status_code == 404

@pytest.mark.asyncio
async def test_get_existing_message(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    response = await async_client.get(f"/api/v1/messages/{message_id}")
    assert response.status_code == 200
    assert response.json()["id"] == message_id

@pytest.mark.asyncio
async def test_get_nonexistent_message(async_client: AsyncClient, override_auth):
    message_id = "nonexistent-msg-id"
    response = await async_client.get(f"/api/v1/messages/{message_id}")
    assert response.status_code == 404


@pytest.mark.asyncio
async def test_import_valid_csv(async_client: AsyncClient, override_auth):
    csv_content = "phone\n+1234567890\n+1987654321"
    file = {"file": ("contacts.csv", csv_content, "text/csv")}
    params = {
        "message_template": "Test message",
        "delimiter": ",",
        "has_header": "true",
        "phone_column": "phone"
    }
    response = await async_client.post("/api/v1/messages/import", params=params, files=file)
    assert response.status_code == 202
    assert "task_id" in response.json()

@pytest.mark.asyncio
async def test_import_missing_column(async_client: AsyncClient, override_auth):
    csv_content = "name\nAlice\nBob"
    file = {"file": ("contacts.csv", csv_content, "text/csv")}
    params = {
        "message_template": "Hi",
        "delimiter": ",",
        "has_header": "true",
        "phone_column": "phone"
    }
    response = await async_client.post("/api/v1/messages/import", params=params, files=file)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_import_invalid_format(async_client: AsyncClient, override_auth):
    csv_content = "random|data|columns"
    file = {"file": ("contacts.csv", csv_content, "text/csv")}
    params = {
        "message_template": "Hi again",
        "delimiter": ",",
        "has_header": "true",
        "phone_column": "phone"
    }
    response = await async_client.post("/api/v1/messages/import", params=params, files=file)
    assert response.status_code == 422


@pytest.mark.asyncio
async def test_list_all_messages(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert isinstance(data["items"], list)
    assert "page_info" in data
    assert isinstance(data["page_info"], dict)

@pytest.mark.asyncio
async def test_filter_messages_by_status(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/?status=sent")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert isinstance(data["items"], list)

@pytest.mark.asyncio
async def test_filter_messages_by_phone(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/?phone_number=+1234567890")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert isinstance(data["items"], list)

@pytest.mark.asyncio
async def test_filter_messages_invalid_date(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/?from_date=invalid-date")
    assert response.status_code in [422, 500]

@pytest.mark.asyncio
async def test_send_valid_message(async_client: AsyncClient, override_auth):
    payload = {
        "phone_number": "+1234567890",
        "message": "Hello there!",
        "scheduled_at": None,
        "custom_id": "test-msg-123"
    }
    response = await async_client.post("/api/v1/messages/send", json=payload)
    assert response.status_code == 202
    assert "id" in response.json()

@pytest.mark.asyncio
async def test_send_invalid_phone(async_client: AsyncClient, override_auth):
    payload = {
        "phone_number": "invalid-phone",
        "message": "Hello there!",
        "scheduled_at": None,
        "custom_id": "test-msg-456"
    }
    response = await async_client.post("/api/v1/messages/send", json=payload)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_send_empty_message(async_client: AsyncClient, override_auth):
    payload = {
        "phone_number": "+1234567890",
        "message": "",
        "scheduled_at": None,
        "custom_id": "test-msg-789"
    }
    response = await async_client.post("/api/v1/messages/send", json=payload)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_get_existing_task_status(async_client: AsyncClient, override_auth):
    task_id = "existing-task-id"
    response = await async_client.get(f"/api/v1/messages/tasks/{task_id}")
    assert response.status_code == 200
    assert "status" in response.json()

@pytest.mark.asyncio
async def test_get_nonexistent_task_status(async_client: AsyncClient, override_auth):
    task_id = "nonexistent-task-id"
    response = await async_client.get(f"/api/v1/messages/tasks/{task_id}")
    assert response.status_code == 404

@pytest.mark.asyncio
async def test_update_valid_message_status(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    payload = {
        "status": "delivered",
        "reason": "confirmed by carrier"
    }
    response = await async_client.put(f"/api/v1/messages/{message_id}/status", json=payload)
    assert response.status_code == 200
    assert response.json()["status"] == "delivered"

@pytest.mark.asyncio
async def test_update_status_message_not_found(async_client: AsyncClient, override_auth):
    message_id = "nonexistent-msg-id"
    payload = {
        "status": "failed",
        "reason": "user unreachable"
    }
    response = await async_client.put(f"/api/v1/messages/{message_id}/status", json=payload)
    assert response.status_code == 404

@pytest.mark.asyncio
async def test_update_status_invalid_value(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    payload = {
        "status": "not_a_valid_status",
        "reason": "some reason"
    }
    response = await async_client.put(f"/api/v1/messages/{message_id}/status", json=payload)
    assert response.status_code == 422
</file>

<file path=".gitignore">
# Python specific
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.env
*.venv
env/
venv/
ENV/
.venv/
env.bak/
venv.bak/
*.egg
*.egg-info/
dist/
build/
*.log

# FastAPI specific
instance/
*.db
*.sqlite3

# IDE specific
.vscode/
.idea/
*.swp
*.swo

# OS generated files
.DS_Store
Thumbs.db

# Test and coverage reports
htmlcov/
.tox/
.nox/
.coverage
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
</file>

<file path="alembic/README">
Generic single-database configuration.
</file>

<file path="alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="app/api/v1/endpoints/auth.py">
"""
API endpoints for authentication.
"""
from datetime import datetime, timedelta, timezone
from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException, status, Body
from fastapi.security import OAuth2PasswordRequestForm

from app.core.config import settings
from app.core.exceptions import AuthenticationError, AuthorizationError, NotFoundError
from app.api.v1.dependencies import (
    get_current_user,
    get_user_repository,
    validate_permissions
)
from app.schemas.user import (
    User,
    UserCreate,
    Token,
    APIKey,
    APIKeyCreate
)
from app.core.security import (
    create_access_token,
    verify_password,
    get_password_hash
)

router = APIRouter()


@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    user_repository = Depends(get_user_repository)
):
    """
    OAuth2 compatible token login, get an access token for future requests.
    """
    try:
        # Authenticate user
        user = await user_repository.get_by_email(form_data.username)
        if not user:
            raise AuthenticationError("Incorrect email or password")
        
        # Verify password
        if not verify_password(form_data.password, user.hashed_password):
            raise AuthenticationError("Incorrect email or password")
        
        # Check if user is active
        if not user.is_active:
            raise AuthenticationError("Inactive user")
        
        # Create access token
        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
        expires_at = datetime.now(timezone.utc) + access_token_expires
        
        access_token = create_access_token(
            data={
                "sub": str(user.id),
                "role": user.role,
                "exp": expires_at
            },
            expires_delta=access_token_expires
        )
        
        return {
            "access_token": access_token,
            "token_type": "bearer",
            "expires_at": expires_at
        }
        
    except AuthenticationError as e:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=str(e),
            headers={"WWW-Authenticate": "Bearer"},
        )


@router.post("/register", response_model=User, status_code=status.HTTP_201_CREATED)
async def register_user(
    user_data: UserCreate,
    user_repository = Depends(get_user_repository)
):
    """
    Register a new user.
    """
    # Check if user already exists
    existing_user = await user_repository.get_by_email(user_data.email)
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="Email already registered"
        )
    
    # Create new user
    hashed_password = get_password_hash(user_data.password)
    
    user = await user_repository.create(
        email=user_data.email,
        hashed_password=hashed_password,
        full_name=user_data.full_name,
        is_active=user_data.is_active,
        role=user_data.role
    )
    
    return user


@router.get("/me", response_model=User)
async def read_users_me(
    current_user: User = Depends(get_current_user)
):
    """
    Get current user information.
    """
    return current_user


@router.post("/keys", response_model=APIKey)
async def create_api_key(
    api_key_data: APIKeyCreate,
    current_user: User = Depends(get_current_user),
    user_repository = Depends(get_user_repository)
):
    """
    Create a new API key.
    
    This is the only time the full API key will be returned.
    """
    # Create API key
    api_key = await user_repository.create_api_key(
        user_id=current_user.id,
        name=api_key_data.name,
        expires_at=api_key_data.expires_at,
        permissions=api_key_data.permissions
    )
    
    return api_key


@router.get("/keys", response_model=List[APIKey])
async def list_api_keys(
    current_user: User = Depends(get_current_user),
    user_repository = Depends(get_user_repository)
):
    """
    List all API keys for the current user.
    
    Note: The full API key value is not returned, only the ID and metadata.
    """
    # List API keys
    api_keys = await user_repository.list_api_keys(user_id=current_user.id)
    
    # Remove sensitive information
    for key in api_keys:
        key.key = f"{key.key[:8]}..." if key.key else None
    
    return api_keys


@router.delete("/keys/{key_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_api_key(
    key_id: str,
    current_user: User = Depends(get_current_user),
    user_repository = Depends(get_user_repository)
):
    """
    Delete an API key.
    """
    # Get API key
    api_key = await user_repository.get_api_key_by_id(key_id)
    if not api_key:
        raise NotFoundError(message="API key not found")
    
    # Check ownership
    if api_key.user_id != str(current_user.id) and current_user.role != "admin":
        raise AuthorizationError(message="Not authorized to delete this API key")
    
    # Delete API key
    success = await user_repository.delete_api_key(key_id)
    if not success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete API key"
        )
    
    return None
</file>

<file path="app/api/v1/endpoints/templates.py">
# app/api/v1/endpoints/templates.py
"""
API endpoints for message templates.
"""
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Path, Query, status, Body
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from app.api.v1.dependencies import get_current_user
from app.core.exceptions import ValidationError, NotFoundError
from app.schemas.template import (
    MessageTemplateCreate,
    MessageTemplateUpdate,
    MessageTemplateResponse,
    MessageWithTemplate
)
from app.schemas.user import User
from app.utils.pagination import PaginationParams, paginate_response
from app.services.sms.sender import get_sms_sender
from app.db.session import get_repository_context

router = APIRouter()


@router.post("/", response_model=MessageTemplateResponse, status_code=status.HTTP_201_CREATED)
async def create_template(
    template: MessageTemplateCreate,
    current_user: User = Depends(get_current_user)
):
    """
    Create a new message template.
    
    Templates can include variables in the format {{variable_name}} which will
    be replaced when sending messages.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Create template
            result = await template_repo.create_template(
                name=template.name,
                content=template.content,
                description=template.description,
                variables=template.variables,
                is_active=template.is_active,
                user_id=current_user.id
            )
            
            return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating template: {str(e)}")


@router.get("/", response_model=Dict[str, Any])
async def list_templates(
    pagination: PaginationParams = Depends(),
    active_only: bool = Query(False, description="Return only active templates"),
    current_user: User = Depends(get_current_user)
):
    """
    List message templates for the current user.
    
    Returns a paginated list of templates.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get templates
            templates, total = await template_repo.get_templates_for_user(
                user_id=current_user.id,
                active_only=active_only,
                skip=pagination.skip,
                limit=pagination.limit
            )
            
            # Return paginated response
            return paginate_response(
                items=[template.dict() for template in templates],
                total=total,
                pagination=pagination
            )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing templates: {str(e)}")


@router.get("/{template_id}", response_model=MessageTemplateResponse)
async def get_template(
    template_id: str = Path(..., description="Template ID"),
    current_user: User = Depends(get_current_user)
):
    """
    Get a specific message template.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to access this template")
            
            return template
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting template: {str(e)}")


@router.put("/{template_id}", response_model=MessageTemplateResponse)
async def update_template(
    template_update: MessageTemplateUpdate,
    template_id: str = Path(..., description="Template ID"),
    current_user: User = Depends(get_current_user)
):
    """
    Update a message template.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to update this template")
            
            # Extract variables from content if content was updated
            update_data = template_update.dict(exclude_unset=True)
            if "content" in update_data:
                import re
                pattern = r"{{([a-zA-Z0-9_]+)}}"
                update_data["variables"] = list(set(re.findall(pattern, update_data["content"])))
            
            # Update template
            updated_template = await template_repo.update(id=template_id, obj_in=update_data)
            
            return updated_template
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating template: {str(e)}")


@router.delete("/{template_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_template(
    template_id: str = Path(..., description="Template ID"),
    current_user: User = Depends(get_current_user)
):
    """
    Delete a message template.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to delete this template")
            
            # Delete template
            success = await template_repo.delete(id=template_id)
            
            if not success:
                raise HTTPException(status_code=500, detail="Failed to delete template")
            
            # Return no content
            return None
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting template: {str(e)}")


# Create a Pydantic model for the apply request
class TemplateApplyRequest(BaseModel):
    """Request model for applying a template."""
    template_id: str
    variables: Dict[str, str]


@router.post("/apply")
async def apply_template(
    request: TemplateApplyRequest = Body(...),
    current_user: User = Depends(get_current_user)
):
    """
    Apply variables to a template and return the result.
    
    This endpoint is useful for previewing how a template will look with specific variables.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(request.template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {request.template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to access this template")
            
            # Apply template
            result = await template_repo.apply_template(
                template_id=request.template_id,
                variables=request.variables
            )
            
            # Check for missing variables
            import re
            missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", result)
            
            return {
                "result": result,
                "missing_variables": missing_vars
            }
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error applying template: {str(e)}")


@router.post("/send", status_code=status.HTTP_202_ACCEPTED)
async def send_with_template(
    message: MessageWithTemplate,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender)
):
    """
    Send a message using a template.
    
    Applies the provided variables to the template and sends the resulting message.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(message.template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {message.template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to use this template")
            
            # Apply template
            message_text = await template_repo.apply_template(
                template_id=message.template_id,
                variables=message.variables
            )
            
            # Check for missing variables
            import re
            missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", message_text)
            if missing_vars:
                raise ValidationError(
                    message="Missing template variables", 
                    details={"missing_variables": missing_vars}
                )
        
        # Send message using sms_sender which already uses context managers internally
        result = await sms_sender.send_message(
            phone_number=message.phone_number,
            message_text=message_text,
            user_id=current_user.id,
            scheduled_at=message.scheduled_at,
            custom_id=message.custom_id,
            metadata={"template_id": message.template_id, "template_variables": message.variables}
        )
        
        return result
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error sending message: {str(e)}")
</file>

<file path="app/core/exceptions.py">
"""
Custom exception classes for Inboxerr Backend.
"""
from typing import Any, Dict, Optional


class InboxerrException(Exception):
    """Base exception class for Inboxerr application."""
    
    def __init__(
        self,
        message: str,
        code: str = "INTERNAL_ERROR",
        status_code: int = 500,
        details: Optional[Dict[str, Any]] = None,
    ):
        self.message = message
        self.code = code
        self.status_code = status_code
        self.details = details or {}
        super().__init__(message)


class AuthenticationError(InboxerrException):
    """Raised when authentication fails."""
    
    def __init__(
        self,
        message: str = "Authentication failed",
        code: str = "AUTHENTICATION_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=401, details=details)


class AuthorizationError(InboxerrException):
    """Raised when a user doesn't have permission."""
    
    def __init__(
        self,
        message: str = "Not authorized",
        code: str = "AUTHORIZATION_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=403, details=details)


class ValidationError(InboxerrException):
    """Raised for validation errors."""
    
    def __init__(
        self,
        message: str = "Validation error",
        code: str = "VALIDATION_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=422, details=details)


class NotFoundError(InboxerrException):
    """Raised when a resource is not found."""
    
    def __init__(
        self,
        message: str = "Resource not found",
        code: str = "NOT_FOUND",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=404, details=details)


class SMSGatewayError(InboxerrException):
    """Raised when there's an error with the SMS gateway."""
    
    def __init__(
        self,
        message: str = "SMS Gateway error",
        code: str = "SMS_GATEWAY_ERROR",
        details: Optional[Dict[str, Any]] = None,
        status_code: int = 502,
    ):
        super().__init__(message=message, code=code, status_code=status_code, details=details)


class RetryableError(InboxerrException):
    """Error that can be retried."""
    
    def __init__(
        self,
        message: str = "Retryable error",
        code: str = "RETRYABLE_ERROR",
        details: Optional[Dict[str, Any]] = None,
        retry_after: int = 60,
    ):
        details = details or {}
        details["retry_after"] = retry_after
        super().__init__(message=message, code=code, status_code=503, details=details)


class WebhookError(InboxerrException):
    """Raised when there's an issue with webhook processing."""
    
    def __init__(
        self,
        message: str = "Webhook error",
        code: str = "WEBHOOK_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=400, details=details)


class SMSAuthError(SMSGatewayError):
    """Raised when SMS gateway credentials are invalid."""
    def __init__(
        self,
        message: str = "Invalid SMS gateway credentials",
        details: Optional[Dict[str, Any]] = None
    ):
        super().__init__(
            message=message,
            code="SMS_AUTH_ERROR",
            status_code=401,
            details=details
        )
</file>

<file path="app/core/security.py">
"""
Security utilities for authentication and authorization.
"""
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional, Union
import jwt
from passlib.context import CryptContext
import secrets
import string

from app.core.config import settings

# Password hashing context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """
    Verify a password against a hash.
    
    Args:
        plain_password: Plain-text password
        hashed_password: Hashed password
        
    Returns:
        bool: True if password matches hash
    """
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    """
    Hash a password.
    
    Args:
        password: Plain-text password
        
    Returns:
        str: Hashed password
    """
    return pwd_context.hash(password)


def create_access_token(
    data: Dict[str, Any],
    expires_delta: Optional[timedelta] = None
) -> str:
    """
    Create a JWT access token.
    
    Args:
        data: Data to encode in the token
        expires_delta: Token expiration time
        
    Returns:
        str: JWT token
    """
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(minutes=15)
    
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(
        to_encode,
        settings.SECRET_KEY,
        algorithm="HS256"
    )
    
    return encoded_jwt


def generate_api_key() -> str:
    """
    Generate a secure API key.
    
    Returns:
        str: API key
    """
    # Characters to use in API key
    alphabet = string.ascii_letters + string.digits
    
    # Generate a random string for the API key
    api_key = ''.join(secrets.choice(alphabet) for _ in range(32))
    
    # Add prefix for identification
    return f"ibx_{''.join(secrets.choice(alphabet) for _ in range(8))}_{api_key}"


def validate_api_key(api_key: str) -> bool:
    """
    Validate API key format.
    
    Args:
        api_key: API key to validate
        
    Returns:
        bool: True if format is valid
    """
    # Check format (prefix_random_key)
    parts = api_key.split('_')
    if len(parts) != 3:
        return False
    
    prefix, random_part, key = parts
    
    # Validate prefix
    if prefix != "ibx":
        return False
    
    # Validate random part length
    if len(random_part) != 8:
        return False
    
    # Validate key length
    if len(key) != 32:
        return False
    
    # Validate characters
    valid_chars = set(string.ascii_letters + string.digits)
    return all(c in valid_chars for c in random_part + key)


def generate_webhook_signing_key() -> str:
    """
    Generate a secure webhook signing key.
    
    Returns:
        str: Webhook signing key
    """
    # Generate a random string for the signing key
    return secrets.token_hex(32)  # 64 character hex string


def create_hmac_signature(payload: str, secret_key: str, timestamp: str) -> str:
    """
    Create HMAC signature for webhook payload validation.
    
    Args:
        payload: JSON payload as string
        secret_key: Secret key for signing
        timestamp: Timestamp string
        
    Returns:
        str: HMAC signature
    """
    import hmac
    import hashlib
    
    message = (payload + timestamp).encode()
    signature = hmac.new(
        secret_key.encode(),
        message,
        hashlib.sha256
    ).hexdigest()
    
    return signature


def verify_webhook_signature(
    payload: str,
    signature: str,
    secret_key: str,
    timestamp: str,
    tolerance: int = 300
) -> bool:
    """
    Verify webhook signature.
    
    Args:
        payload: JSON payload as string
        signature: Signature to verify
        secret_key: Secret key for signing
        timestamp: Timestamp used in signature
        tolerance: Timestamp tolerance in seconds
        
    Returns:
        bool: True if signature is valid
    """
    import hmac
    import time
    
    # Verify timestamp is within tolerance
    try:
        ts = int(timestamp)
        current_time = int(time.time())
        if abs(current_time - ts) > tolerance:
            return False
    except (ValueError, TypeError):
        return False
    
    # Calculate expected signature
    expected = create_hmac_signature(payload, secret_key, timestamp)
    
    # Compare signatures (constant-time comparison)
    return hmac.compare_digest(expected, signature)
</file>

<file path="app/db/repositories/base.py">
"""
Base repository with common database operations.
"""
from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union
from uuid import uuid4

from fastapi.encoders import jsonable_encoder
from pydantic import BaseModel
from sqlalchemy import select, update, delete
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.base import Base

# Define generic types for models
ModelType = TypeVar("ModelType", bound=Base)
CreateSchemaType = TypeVar("CreateSchemaType", bound=BaseModel)
UpdateSchemaType = TypeVar("UpdateSchemaType", bound=BaseModel)


class BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):
    """
    Base repository with common CRUD operations.
    
    Generic repository pattern implementation for database access.
    """
    
    def __init__(self, session: AsyncSession, model: Type[ModelType]):
        """
        Initialize repository with session and model.
        
        Args:
            session: Database session
            model: SQLAlchemy model class
        """
        self.session = session
        self.model = model
        self.session_is_owned = False
    
    async def __aenter__(self):
        """Support async context manager protocol."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Clean up resources when exiting context."""
        if self._session_is_owned:
            await self.close()
    
    async def close(self):
        """Close the session if we own it."""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def get_by_id(self, id: str) -> Optional[ModelType]:
        """
        Get a record by ID.
        
        Args:
            id: Record ID
            
        Returns:
            ModelType: Found record or None
        """
        query = select(self.model).where(self.model.id == id)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def get_by_attribute(self, attr_name: str, attr_value: Any) -> Optional[ModelType]:
        """
        Get a record by a specific attribute.
        
        Args:
            attr_name: Attribute name
            attr_value: Attribute value
            
        Returns:
            ModelType: Found record or None
        """
        query = select(self.model).where(getattr(self.model, attr_name) == attr_value)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def list(
        self, 
        *,
        filters: Optional[Dict[str, Any]] = None,
        skip: int = 0, 
        limit: int = 100
    ) -> List[ModelType]:
        """
        Get a list of records with optional filtering.
        
        Args:
            filters: Optional filters as dict
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            List[ModelType]: List of records
        """
        query = select(self.model)
        
        # Apply filters if provided
        if filters:
            for attr_name, attr_value in filters.items():
                if hasattr(self.model, attr_name) and attr_value is not None:
                    query = query.where(getattr(self.model, attr_name) == attr_value)
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def create(self, *, obj_in: Union[CreateSchemaType, Dict[str, Any]]) -> ModelType:
        """
        Create a new record.
        
        Args:
            obj_in: Data to create record with
            
        Returns:
            ModelType: Created record
        """
        # Convert to dict if it's a Pydantic model
        obj_in_data = obj_in if isinstance(obj_in, dict) else obj_in.dict(exclude_unset=True)
        
        # Create model instance
        db_obj = self.model(**obj_in_data)
        
        # Generate ID if not provided
        if not db_obj.id:
            db_obj.id = str(uuid4())
        
        # Add to session
        self.session.add(db_obj)
        await self.session.commit()
        await self.session.refresh(db_obj)
        
        return db_obj
    
    async def update(
        self, 
        *,
        id: str,
        obj_in: Union[UpdateSchemaType, Dict[str, Any]]
    ) -> Optional[ModelType]:
        """
        Update a record.
        
        Args:
            id: Record ID
            obj_in: Data to update record with
            
        Returns:
            ModelType: Updated record or None
        """
        # Get current record
        db_obj = await self.get_by_id(id)
        if not db_obj:
            return None
        
        # Convert to dict if it's a Pydantic model
        update_data = obj_in if isinstance(obj_in, dict) else obj_in.dict(exclude_unset=True)
        
        # Remove None values
        update_data = {k: v for k, v in update_data.items() if v is not None}
        
        # Update record
        for field, value in update_data.items():
            if hasattr(db_obj, field):
                setattr(db_obj, field, value)
        
        # Save changes
        self.session.add(db_obj)
        await self.session.commit()
        await self.session.refresh(db_obj)
        
        return db_obj
    
    async def delete(self, *, id: str) -> bool:
        """
        Delete a record.
        
        Args:
            id: Record ID
            
        Returns:
            bool: True if deleted, False if not found
        """
        # Check if record exists
        db_obj = await self.get_by_id(id)
        if not db_obj:
            return False
        
        # Delete record
        await self.session.delete(db_obj)
        await self.session.commit()
        
        return True
    
    async def count(self, *, filters: Optional[Dict[str, Any]] = None) -> int:
        """
        Count records with optional filtering.
        
        Args:
            filters: Optional filters as dict
            
        Returns:
            int: Number of records
        """
        from sqlalchemy import func
        
        query = select(func.count()).select_from(self.model)
        
        # Apply filters if provided
        if filters:
            for attr_name, attr_value in filters.items():
                if hasattr(self.model, attr_name) and attr_value is not None:
                    query = query.where(getattr(self.model, attr_name) == attr_value)
        
        result = await self.session.execute(query)
        return result.scalar_one()
    

    async def execute_in_transaction(self, func, *args, **kwargs):
        """
        Execute a function within a transaction.
        
        Args:
            func: Async function to execute
            args: Function positional arguments
            kwargs: Function keyword arguments
            
        Returns:
            The result of the function
        """
        async with self.session.begin():
            return await func(*args, **kwargs)
</file>

<file path="app/db/repositories/webhooks.py">
"""
Webhook repository for database operations related to webhooks.
"""
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Dict, Any, Tuple
from uuid import uuid4

from sqlalchemy import select, update, delete, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.repositories.base import BaseRepository
from app.models.webhook import Webhook, WebhookDelivery, WebhookEvent
from app.core.security import generate_webhook_signing_key


class WebhookRepository(BaseRepository[Webhook, Dict[str, Any], Dict[str, Any]]):
    """Webhook repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and Webhook model."""
        super().__init__(session=session, model=Webhook)
    
    async def create_webhook(
        self,
        *,
        name: str,
        url: str,
        event_types: List[str],
        user_id: str,
        secret_key: Optional[str] = None,
        gateway_webhook_id: Optional[str] = None
    ) -> Webhook:
        """
        Create a new webhook.
        
        Args:
            name: Webhook name
            url: Webhook URL
            event_types: List of event types to receive
            user_id: User ID
            secret_key: Optional secret key for signature validation
            gateway_webhook_id: Optional gateway webhook ID
            
        Returns:
            Webhook: Created webhook
        """
        # Generate secret key if not provided
        if not secret_key:
            secret_key = generate_webhook_signing_key()
        
        webhook = Webhook(
            id=str(uuid4()),
            name=name,
            url=url,
            event_types=event_types,
            user_id=user_id,
            secret_key=secret_key,
            gateway_webhook_id=gateway_webhook_id,
            is_active=True
        )
        
        self.session.add(webhook)
        await self.session.commit()
        await self.session.refresh(webhook)
        
        return webhook
    
    async def get_webhooks_for_event(
        self,
        *,
        event_type: str,
        user_id: Optional[str] = None
    ) -> List[Webhook]:
        """
        Get webhooks for a specific event type.
        
        Args:
            event_type: Event type
            user_id: Optional user ID to filter webhooks
            
        Returns:
            List[Webhook]: List of matching webhooks
        """
        query = select(Webhook).where(
            and_(
                Webhook.is_active == True,
                Webhook.event_types.contains([event_type])
            )
        )
        
        if user_id:
            query = query.where(Webhook.user_id == user_id)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def create_webhook_delivery(
        self,
        *,
        webhook_id: str,
        event_type: str,
        message_id: Optional[str],
        payload: Dict[str, Any],
        status_code: Optional[int] = None,
        is_success: bool = False,
        error_message: Optional[str] = None,
        retry_count: int = 0
    ) -> WebhookDelivery:
        """
        Record a webhook delivery attempt.
        
        Args:
            webhook_id: Webhook ID
            event_type: Event type
            message_id: Message ID (if applicable)
            payload: Webhook payload
            status_code: HTTP status code
            is_success: Whether delivery was successful
            error_message: Error message if failed
            retry_count: Number of retry attempts
            
        Returns:
            WebhookDelivery: Created webhook delivery record
        """
        delivery = WebhookDelivery(
            id=str(uuid4()),
            webhook_id=webhook_id,
            event_type=event_type,
            message_id=message_id,
            payload=payload,
            status_code=status_code,
            is_success=is_success,
            error_message=error_message,
            retry_count=retry_count
        )
        
        if not is_success and retry_count < 3:  # Configure max retries
            # Schedule next retry with exponential backoff
            backoff = 5 * (2 ** retry_count)  # 5, 10, 20 minutes
            delivery.next_retry_at = datetime.now(timezone.utc) + timedelta(minutes=backoff)
        
        self.session.add(delivery)
        await self.session.commit()
        await self.session.refresh(delivery)
        
        # Update webhook stats
        await self._update_webhook_stats(
            webhook_id=webhook_id,
            is_success=is_success,
            last_triggered=datetime.now(timezone.utc)
        )
        
        return delivery
    
    async def _update_webhook_stats(
        self,
        *,
        webhook_id: str,
        is_success: bool,
        last_triggered: datetime
    ) -> None:
        """
        Update webhook statistics.
        
        Args:
            webhook_id: Webhook ID
            is_success: Whether delivery was successful
            last_triggered: Timestamp of delivery attempt
        """
        webhook = await self.get_by_id(webhook_id)
        if not webhook:
            return
        
        webhook.last_triggered_at = last_triggered
        
        if is_success:
            webhook.success_count += 1
        else:
            webhook.failure_count += 1
        
        self.session.add(webhook)
        await self.session.commit()
    
    async def get_pending_retries(
        self,
        *,
        limit: int = 10
    ) -> List[WebhookDelivery]:
        """
        Get webhook deliveries pending retry.
        
        Args:
            limit: Maximum number of deliveries to return
            
        Returns:
            List[WebhookDelivery]: List of deliveries pending retry
        """
        now = datetime.now(timezone.utc)
        
        query = select(WebhookDelivery).where(
            and_(
                WebhookDelivery.is_success == False,
                WebhookDelivery.next_retry_at <= now,
                WebhookDelivery.next_retry_at.is_not(None),
                WebhookDelivery.retry_count < 3  # Configure max retries
            )
        ).order_by(WebhookDelivery.next_retry_at).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update_delivery_status(
        self,
        *,
        delivery_id: str,
        status_code: int,
        is_success: bool,
        error_message: Optional[str] = None,
        increment_retry: bool = False
    ) -> Optional[WebhookDelivery]:
        """
        Update webhook delivery status.
        
        Args:
            delivery_id: Delivery ID
            status_code: HTTP status code
            is_success: Whether delivery was successful
            error_message: Error message if failed
            increment_retry: Whether to increment retry count
            
        Returns:
            WebhookDelivery: Updated delivery record or None
        """
        delivery = await self.session.get(WebhookDelivery, delivery_id)
        if not delivery:
            return None
        
        delivery.status_code = status_code
        delivery.is_success = is_success
        delivery.error_message = error_message
        
        if increment_retry:
            delivery.retry_count += 1
        
        if not is_success and delivery.retry_count < 3:  # Configure max retries
            # Schedule next retry with exponential backoff
            backoff = 5 * (2 ** delivery.retry_count)  # 5, 10, 20 minutes
            delivery.next_retry_at = datetime.now(timezone.utc) + timedelta(minutes=backoff)
        else:
            delivery.next_retry_at = None
        
        self.session.add(delivery)
        await self.session.commit()
        await self.session.refresh(delivery)
        
        # Update webhook stats
        await self._update_webhook_stats(
            webhook_id=delivery.webhook_id,
            is_success=is_success,
            last_triggered=datetime.now(timezone.utc)
        )
        
        return delivery
    
    async def create_webhook_event(
        self,
        *,
        event_type: str,
        payload: Dict[str, Any],
        phone_number: Optional[str] = None,
        message_id: Optional[str] = None,
        gateway_message_id: Optional[str] = None
    ) -> WebhookEvent:
        """
        Record a webhook event received from SMS gateway.
        
        Args:
            event_type: Event type
            payload: Event payload
            phone_number: Phone number
            message_id: Message ID
            gateway_message_id: Gateway message ID
            
        Returns:
            WebhookEvent: Created webhook event
        """
        event = WebhookEvent(
            id=str(uuid4()),
            event_type=event_type,
            phone_number=phone_number,
            message_id=message_id,
            gateway_message_id=gateway_message_id,
            payload=payload,
            processed=False
        )
        
        self.session.add(event)
        await self.session.commit()
        await self.session.refresh(event)
        
        return event
    
    async def mark_event_processed(
        self,
        *,
        event_id: str,
        error_message: Optional[str] = None
    ) -> Optional[WebhookEvent]:
        """
        Mark a webhook event as processed.
        
        Args:
            event_id: Event ID
            error_message: Optional error message
            
        Returns:
            WebhookEvent: Updated event or None
        """
        event = await self.session.get(WebhookEvent, event_id)
        if not event:
            return None
        
        event.processed = True
        event.error_message = error_message
        
        self.session.add(event)
        await self.session.commit()
        await self.session.refresh(event)
        
        return event
    
    async def get_unprocessed_events(
        self,
        *,
        limit: int = 10
    ) -> List[WebhookEvent]:
        """
        Get unprocessed webhook events.
        
        Args:
            limit: Maximum number of events to return
            
        Returns:
            List[WebhookEvent]: List of unprocessed events
        """
        query = select(WebhookEvent).where(
            WebhookEvent.processed == False
        ).order_by(WebhookEvent.created_at).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
</file>

<file path="app/models/metrics.py">
"""
Database models for metrics.
"""
from datetime import datetime, timezone, date
from typing import Optional, Dict, Any

from sqlalchemy import Column, String, Integer, Float, Date, DateTime, JSON, ForeignKey, UniqueConstraint
from sqlalchemy.orm import relationship

from app.models.base import Base


class UserMetrics(Base):
    """Model for storing user-level metrics."""
    
    # Identification and relationships
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    date = Column(Date, nullable=False, index=True)
    
    # Message metrics
    messages_sent = Column(Integer, default=0, nullable=False)
    messages_delivered = Column(Integer, default=0, nullable=False)
    messages_failed = Column(Integer, default=0, nullable=False)
    messages_scheduled = Column(Integer, default=0, nullable=False)
    
    # Campaign metrics
    campaigns_created = Column(Integer, default=0, nullable=False)
    campaigns_completed = Column(Integer, default=0, nullable=False)
    campaigns_active = Column(Integer, default=0, nullable=False)
    
    # Template metrics
    templates_created = Column(Integer, default=0, nullable=False)
    templates_used = Column(Integer, default=0, nullable=False)
    
    # Usage metrics
    quota_total = Column(Integer, default=1000, nullable=False)
    quota_used = Column(Integer, default=0, nullable=False)
    
    # Additional stats
    meta_data = Column(JSON, nullable=True)
    
    # Relationships
    user = relationship("User", back_populates="metrics")
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('user_id', 'date', name='uix_user_date'),
    )
</file>

<file path="app/schemas/metrics.py">
"""
Pydantic schemas for metrics-related API operations.
"""
from typing import List, Dict, Any
from datetime import date
from pydantic import BaseModel, Field


class PeriodInfo(BaseModel):
    """Schema for period information."""
    start_date: str = Field(..., description="Start date in ISO format")
    end_date: str = Field(..., description="End date in ISO format")


class MessageMetrics(BaseModel):
    """Schema for message metrics."""
    sent: int = Field(..., description="Number of sent messages")
    delivered: int = Field(..., description="Number of delivered messages")
    failed: int = Field(..., description="Number of failed messages")
    delivery_rate: float = Field(..., description="Delivery rate percentage")


class CampaignMetrics(BaseModel):
    """Schema for campaign metrics."""
    created: int = Field(..., description="Number of created campaigns")
    completed: int = Field(..., description="Number of completed campaigns")
    active: int = Field(..., description="Number of active campaigns")


class TemplateMetrics(BaseModel):
    """Schema for template metrics."""
    created: int = Field(..., description="Number of created templates")
    used: int = Field(..., description="Number of times templates were used")


class QuotaMetrics(BaseModel):
    """Schema for quota metrics."""
    used: int = Field(..., description="Number of messages used from quota")
    total: int = Field(..., description="Total quota limit")
    percent: float = Field(..., description="Percentage of quota used")


class DailyData(BaseModel):
    """Schema for daily metrics data."""
    date: str = Field(..., description="Date in ISO format")
    sent: int = Field(..., description="Messages sent on this date")
    delivered: int = Field(..., description="Messages delivered on this date")
    failed: int = Field(..., description="Messages failed on this date")


class MetricsSummary(BaseModel):
    """Schema for metrics summary."""
    period: PeriodInfo = Field(..., description="Period information")
    messages: MessageMetrics = Field(..., description="Message metrics")
    campaigns: CampaignMetrics = Field(..., description="Campaign metrics")
    templates: TemplateMetrics = Field(..., description="Template metrics")
    quota: QuotaMetrics = Field(..., description="Quota metrics")


class DashboardMetricsResponse(BaseModel):
    """Schema for dashboard metrics response."""
    summary: MetricsSummary = Field(..., description="Summary metrics")
    daily_data: List[DailyData] = Field(..., description="Daily metrics data for charts")
    period: str = Field(..., description="Requested period")

    class Config:
        """Pydantic config."""
        from_attributes = True


class UsageMetricsResponse(BaseModel):
    """Schema for usage metrics response."""
    message_count: int = Field(..., description="Total message count")
    delivery_rate: float = Field(..., description="Overall delivery rate")
    quota: QuotaMetrics = Field(..., description="Quota information")

    class Config:
        """Pydantic config."""
        from_attributes = True


class SystemMessageMetrics(BaseModel):
    """Schema for system-wide message metrics."""
    total: int = Field(..., description="Total messages in system")
    sent: int = Field(..., description="Total sent messages")
    delivered: int = Field(..., description="Total delivered messages")
    failed: int = Field(..., description="Total failed messages")
    last_24h: int = Field(..., description="Messages in last 24 hours")


class SystemUserMetrics(BaseModel):
    """Schema for system-wide user metrics."""
    total: int = Field(..., description="Total users in system")
    active: int = Field(..., description="Active users")
    new_today: int = Field(..., description="New users today")


class SystemCampaignMetrics(BaseModel):
    """Schema for system-wide campaign metrics."""
    total: int = Field(..., description="Total campaigns in system")
    active: int = Field(..., description="Active campaigns")
    completed_today: int = Field(..., description="Campaigns completed today")


class SystemMetricsResponse(BaseModel):
    """Schema for system metrics response (admin only)."""
    messages: SystemMessageMetrics = Field(..., description="System message metrics")
    users: SystemUserMetrics = Field(..., description="System user metrics")
    campaigns: SystemCampaignMetrics = Field(..., description="System campaign metrics")

    class Config:
        """Pydantic config."""
        from_attributes = True
</file>

<file path="app/services/event_bus/bus.py">
"""
Enhanced event bus implementation for asynchronous messaging between components.
Improvements:
- Better lock handling
- Enhanced error handling and reporting
- Subscriber management
- Event batching support
- Proper subscriber cleanup
"""
import asyncio
import logging
import time
import uuid
from typing import Dict, List, Callable, Any, Set, Optional, Tuple
from datetime import datetime, timezone
from contextlib import asynccontextmanager

from app.services.event_bus.events import EventType, Event

logger = logging.getLogger("inboxerr.eventbus")


class EventBus:
    """
    Enhanced event bus for asynchronous messaging between components.
    
    Supports subscription to events, publishing events, and now includes:
    - Better lock handling for thread safety
    - Error propagation for subscribers
    - Event batching
    - Subscriber cleanup
    """
    
    def __init__(self):
        """Initialize the event bus."""
        self._subscribers: Dict[str, List[Tuple[str, Callable]]] = {}
        self._subscriber_ids: Dict[str, Set[str]] = {}
        self._lock = asyncio.Lock()
        self._initialized = False
        self._event_history: List[Dict[str, Any]] = []  # For debugging
        self._max_history = 100  # Maximum events to keep in history
        self._failed_deliveries: Dict[str, List[Dict[str, Any]]] = {}  # Failed event deliveries
    
    async def initialize(self) -> None:
        """Initialize the event bus."""
        if self._initialized:
            return
        
        logger.info("Initializing event bus")
        self._initialized = True
    
    async def shutdown(self) -> None:
        """Shutdown the event bus and clean up resources."""
        logger.info("Shutting down event bus")
        self._initialized = False
        
        # Clear subscribers
        async with self._lock:
            self._subscribers.clear()
            self._subscriber_ids.clear()
    
    @asynccontextmanager
    async def batch(self):
        """
        Context manager for batching multiple events.
        
        This allows multiple events to be published atomically.
        """
        # Create a batch container
        batch = []
        
        # Define the add_event function that will be used within the context
        async def add_event(event_type: str, data: Dict[str, Any]) -> None:
            batch.append((event_type, data))
        
        try:
            # Yield the add_event function for use within the context
            yield add_event
            
            # Process the batch after the context exits
            for event_type, data in batch:
                await self.publish(event_type, data)
                
        except Exception as e:
            logger.error(f"Error in event batch: {e}", exc_info=True)
            # Re-raise the exception after logging
            raise
    
    async def publish(self, event_type: str, data: Dict[str, Any]) -> bool:
        """
        Publish an event to subscribers.
        
        Args:
            event_type: Type of event
            data: Event data
            
        Returns:
            bool: True if event was successfully published
        """
        if not self._initialized:
            await self.initialize()
        
        subscribers = []
        subscriber_ids = []
        
        # Get subscribers with lock
        async with self._lock:
            if event_type in self._subscribers:
                subscribers = self._subscribers[event_type].copy()
                subscriber_ids = list(self._subscriber_ids[event_type])
        
        if not subscribers:
            logger.debug(f"No subscribers for event: {event_type}")
            return True
        
        # Add timestamp if not present
        if "timestamp" not in data:
            data["timestamp"] = datetime.now(timezone.utc).isoformat()
        
        # Add event type for reference
        data["event_type"] = event_type
        # Add unique event ID
        data["event_id"] = str(uuid.uuid4())
        
        # Keep history for debugging
        if len(self._event_history) >= self._max_history:
            self._event_history.pop(0)
        self._event_history.append({
            "event_type": event_type,
            "data": data,
            "subscribers": subscriber_ids,
            "timestamp": data["timestamp"]
        })
        
        # Execute callbacks outside of the lock
        logger.debug(f"Publishing event {event_type} to {len(subscribers)} subscribers")
        
        all_successful = True
        
        for subscriber_id, callback in subscribers:
            try:
                await callback(data)
            except asyncio.CancelledError:
                # Re-raise cancellation to allow proper task cleanup
                logger.warning(f"Subscriber {subscriber_id} was cancelled during event {event_type}")
                raise
            except Exception as e:
                logger.error(f"Error in subscriber {subscriber_id} for {event_type}: {e}", exc_info=True)
                
                # Record failed delivery
                if subscriber_id not in self._failed_deliveries:
                    self._failed_deliveries[subscriber_id] = []
                    
                self._failed_deliveries[subscriber_id].append({
                    "event_type": event_type,
                    "event_id": data["event_id"],
                    "error": str(e),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                })
                
                # Limit failed deliveries history
                if len(self._failed_deliveries[subscriber_id]) > self._max_history:
                    self._failed_deliveries[subscriber_id].pop(0)
                
                all_successful = False
        
        return all_successful
    
    async def subscribe(
        self,
        event_type: str,
        callback: Callable,
        subscriber_id: Optional[str] = None
    ) -> str:
        """
        Subscribe to an event type.
        
        Args:
            event_type: Event type to subscribe to
            callback: Function to call when event occurs
            subscriber_id: Optional subscriber ID
            
        Returns:
            str: Subscriber ID
        """
        if not self._initialized:
            await self.initialize()
        
        # Generate subscriber ID if not provided
        if subscriber_id is None:
            subscriber_id = f"{callback.__module__}.{callback.__name__}_{str(uuid.uuid4())[:8]}"
        
        async with self._lock:
            # Initialize event type if not exists
            if event_type not in self._subscribers:
                self._subscribers[event_type] = []
                self._subscriber_ids[event_type] = set()
            
            # Add subscriber if not already subscribed
            if subscriber_id not in self._subscriber_ids[event_type]:
                self._subscribers[event_type].append((subscriber_id, callback))
                self._subscriber_ids[event_type].add(subscriber_id)
                logger.info(f"Subscribed to {event_type}: {subscriber_id}")
            else:
                # Update callback for existing subscriber ID
                for i, (sid, _) in enumerate(self._subscribers[event_type]):
                    if sid == subscriber_id:
                        self._subscribers[event_type][i] = (subscriber_id, callback)
                        logger.debug(f"Updated subscriber callback for {event_type}: {subscriber_id}")
                        break
        
        return subscriber_id
    
    async def unsubscribe(self, event_type: str, subscriber_id: str) -> bool:
        """
        Unsubscribe from an event type.
        
        Args:
            event_type: Event type to unsubscribe from
            subscriber_id: Subscriber ID
            
        Returns:
            bool: True if unsubscribed, False if not found
        """
        if not self._initialized:
            await self.initialize()
        
        async with self._lock:
            if event_type not in self._subscribers:
                return False
            
            if subscriber_id not in self._subscriber_ids[event_type]:
                return False
            
            # Find and remove the subscriber
            self._subscribers[event_type] = [
                (sid, callback) for sid, callback in self._subscribers[event_type]
                if sid != subscriber_id
            ]
            self._subscriber_ids[event_type].remove(subscriber_id)
            
            logger.info(f"Unsubscribed from {event_type}: {subscriber_id}")
            
            # Clean up failed deliveries for this subscriber
            if subscriber_id in self._failed_deliveries:
                del self._failed_deliveries[subscriber_id]
            
            return True
    
    async def unsubscribe_all(self, subscriber_id: str) -> int:
        """
        Unsubscribe from all event types.
        
        Args:
            subscriber_id: Subscriber ID
            
        Returns:
            int: Number of subscriptions removed
        """
        if not self._initialized:
            await self.initialize()
        
        count = 0
        
        async with self._lock:
            for event_type in list(self._subscribers.keys()):
                # Check if subscriber exists for this event type
                if subscriber_id in self._subscriber_ids[event_type]:
                    # Remove from subscribers list
                    self._subscribers[event_type] = [
                        (sid, callback) for sid, callback in self._subscribers[event_type]
                        if sid != subscriber_id
                    ]
                    # Remove from subscriber IDs set
                    self._subscriber_ids[event_type].remove(subscriber_id)
                    count += 1
            
            # Clean up failed deliveries for this subscriber
            if subscriber_id in self._failed_deliveries:
                del self._failed_deliveries[subscriber_id]
        
        if count > 0:
            logger.info(f"Unsubscribed {subscriber_id} from {count} event types")
        
        return count
    
    def get_subscriber_count(self, event_type: Optional[str] = None) -> int:
        """
        Get the number of subscribers.
        
        Args:
            event_type: Optional event type to count subscribers for
            
        Returns:
            int: Number of subscribers
        """
        if event_type:
            return len(self._subscribers.get(event_type, []))
        else:
            return sum(len(subscribers) for subscribers in self._subscribers.values())
    
    def get_event_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent event history for debugging.
        
        Args:
            limit: Maximum number of events to return
            
        Returns:
            List[Dict]: Recent events
        """
        return self._event_history[-limit:] if self._event_history else []
    
    def get_failed_deliveries(self, subscriber_id: Optional[str] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        Get failed event deliveries.
        
        Args:
            subscriber_id: Optional subscriber ID to filter by
            
        Returns:
            Dict: Failed deliveries by subscriber ID
        """
        if subscriber_id:
            return {subscriber_id: self._failed_deliveries.get(subscriber_id, [])}
        else:
            return self._failed_deliveries


# Singleton instance
_event_bus = EventBus()

def get_event_bus() -> EventBus:
    """Get the singleton event bus instance."""
    return _event_bus
</file>

<file path="app/services/rate_limiter.py">
"""
Rate limiting service for API request throttling.
"""
import asyncio
import time
from typing import Dict, Any, Optional, Tuple
import logging
from datetime import datetime, timezone

from app.core.config import settings

logger = logging.getLogger("inboxerr.rate_limiter")

class RateLimiter:
    """
    Service for enforcing rate limits on API requests.
    
    Uses a simple in-memory storage for tracking request counts.
    For production, consider using Redis or another distributed storage.
    """
    
    def __init__(self):
        """Initialize the rate limiter with default limits."""
        self._requests = {}
        self._lock = asyncio.Lock()
        
        # Default rate limits by operation type
        self._rate_limits = {
            "send_message": {"requests": 60, "period": 60},  # 60 requests per minute
            "send_batch": {"requests": 10, "period": 60},    # 10 batch requests per minute
            "import_messages": {"requests": 5, "period": 300},  # 5 imports per 5 minutes
            "default": {"requests": 100, "period": 60},      # Default: 100 requests per minute
        }
    
    async def check_rate_limit(
        self, 
        user_id: str, 
        operation: str = "default"
    ) -> bool:
        """
        Check if a request is within rate limits.
        
        Args:
            user_id: ID of the user making the request
            operation: Type of operation being performed
            
        Returns:
            bool: True if request is allowed, raises exception otherwise
            
        Raises:
            HTTPException: If rate limit is exceeded
        """
        from fastapi import HTTPException, status
        
        # Get rate limit for operation
        limit = self._rate_limits.get(operation, self._rate_limits["default"])
        
        # Create key for this user and operation
        key = f"{user_id}:{operation}"
        
        current_time = time.time()
        
        async with self._lock:
            # Initialize if not exists
            if key not in self._requests:
                self._requests[key] = {"count": 0, "reset_at": current_time + limit["period"]}
            
            # Check if we need to reset the counter
            if current_time > self._requests[key]["reset_at"]:
                self._requests[key] = {"count": 0, "reset_at": current_time + limit["period"]}
            
            # Check if we're over the limit
            if self._requests[key]["count"] >= limit["requests"]:
                reset_in = int(self._requests[key]["reset_at"] - current_time)
                logger.warning(f"Rate limit exceeded for {key}. Reset in {reset_in} seconds.")
                
                # Calculate when the rate limit will reset
                reset_at = datetime.fromtimestamp(self._requests[key]["reset_at"])
                
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=f"Rate limit exceeded. Try again in {reset_in} seconds.",
                    headers={"Retry-After": str(reset_in)}
                )
            
            # Increment counter
            self._requests[key]["count"] += 1
            
            logger.debug(f"Rate limit for {key}: {self._requests[key]['count']}/{limit['requests']}")
            
            return True
    
    def set_limit(self, operation: str, requests: int, period: int) -> None:
        """
        Set a custom rate limit for an operation.
        
        Args:
            operation: Operation type to set limit for
            requests: Maximum number of requests allowed
            period: Time period in seconds
        """
        self._rate_limits[operation] = {"requests": requests, "period": period}
    
    async def get_limit_status(self, user_id: str, operation: str = "default") -> Dict[str, Any]:
        """
        Get current rate limit status for a user and operation.
        
        Args:
            user_id: User ID
            operation: Operation type
            
        Returns:
            Dict: Rate limit status information
        """
        # Get rate limit for operation
        limit = self._rate_limits.get(operation, self._rate_limits["default"])
        
        # Create key for this user and operation
        key = f"{user_id}:{operation}"
        
        current_time = time.time()
        
        async with self._lock:
            # Handle case where user hasn't made any requests yet
            if key not in self._requests:
                return {
                    "limit": limit["requests"],
                    "remaining": limit["requests"],
                    "reset": int(current_time + limit["period"]),
                    "used": 0
                }
            
            # Reset counter if needed
            if current_time > self._requests[key]["reset_at"]:
                self._requests[key] = {"count": 0, "reset_at": current_time + limit["period"]}
            
            # Return current status
            return {
                "limit": limit["requests"],
                "remaining": max(0, limit["requests"] - self._requests[key]["count"]),
                "reset": int(self._requests[key]["reset_at"]),
                "used": self._requests[key]["count"]
            }


# Singleton instance for dependency injection
_rate_limiter = RateLimiter()

def get_rate_limiter() -> RateLimiter:
    """Get the singleton rate limiter instance."""
    return _rate_limiter
</file>

<file path="app/services/webhooks/models.py">
# app/services/webhooks/models.py
"""
Pydantic models for webhook payloads from SMS Gateway.
"""
from datetime import datetime, timezone
from typing import Dict, Any, Optional, Literal
from pydantic import BaseModel, Field

class SmsReceivedPayload(BaseModel):
    """Payload for sms:received event."""
    message_id: str = Field(..., alias="messageId")
    message: str
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    received_at: datetime = Field(..., alias="receivedAt")
    
class SmsSentPayload(BaseModel):
    """Payload for sms:sent event."""
    message_id: str = Field(..., alias="messageId")
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    sent_at: datetime = Field(..., alias="sentAt")
    
class SmsDeliveredPayload(BaseModel):
    """Payload for sms:delivered event."""
    message_id: str = Field(..., alias="messageId")
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    delivered_at: datetime = Field(..., alias="deliveredAt")
    
class SmsFailedPayload(BaseModel):
    """Payload for sms:failed event."""
    message_id: str = Field(..., alias="messageId")
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    failed_at: datetime = Field(..., alias="failedAt")
    reason: str
    
class SystemPingPayload(BaseModel):
    """Payload for system:ping event."""
    health: Dict[str, Any]
    
EventType = Literal["sms:received", "sms:sent", "sms:delivered", "sms:failed", "system:ping"]

class WebhookPayload(BaseModel):
    """Base webhook payload from SMS Gateway."""
    device_id: str = Field(..., alias="deviceId")
    event: EventType
    id: str
    webhook_id: str = Field(..., alias="webhookId")
    payload: Dict[str, Any]  # Will be converted to specific payload types based on event
</file>

<file path="app/utils/phone.py">
"""
Phone number validation and formatting utilities.
"""
import re
from typing import Any, Tuple, Dict, Optional, List
import logging

logger = logging.getLogger("inboxerr.phone")

try:
    import phonenumbers
    from phonenumbers import NumberParseException, PhoneNumberFormat
    PHONENUMBERS_AVAILABLE = True
except ImportError:
    PHONENUMBERS_AVAILABLE = False
    logger.warning("phonenumbers library not available, using basic validation")


class PhoneValidationError(Exception):
    """Exception raised for phone validation errors."""
    
    def __init__(self, message: str, details: Optional[Dict] = None):
        self.message = message
        self.details = details or {}
        super().__init__(message)


def validate_phone_basic(number: str) -> Tuple[bool, str, Optional[str]]:
    """
    Basic phone number validation without external libraries.
    
    Args:
        number: Phone number to validate
        
    Returns:
        Tuple[bool, str, str]: (is_valid, formatted_number, error_message)
    """
    # Remove common formatting characters
    cleaned = re.sub(r'[\s\-\(\)\.]+', '', number)
    
    # Check if it's just digits and maybe a leading +
    if not re.match(r'^\+?\d+$', cleaned):
        return False, number, "Phone number contains invalid characters"
    
    # Ensure it starts with + for E.164 format
    if not cleaned.startswith('+'):
        cleaned = '+' + cleaned
    
    # Basic length check
    if len(cleaned) < 8:
        return False, cleaned, "Phone number too short"
    if len(cleaned) > 16:
        return False, cleaned, "Phone number too long"
    
    return True, cleaned, None


def validate_phone_advanced(number: str) -> Tuple[bool, str, Optional[str], Optional[Dict[str, Any]]]:
    """
    Advanced phone number validation using the phonenumbers library.
    
    Args:
        number: Phone number to validate
        
    Returns:
        Tuple[bool, str, str, dict]: (is_valid, formatted_number, error_message, metadata)
    """
    metadata = {}
    
    try:
        # Parse the phone number
        try:
            parsed = phonenumbers.parse(number, None)
        except NumberParseException as e:
            return False, number, f"Parse error: {str(e)}", None
        
        # Check if it's a valid number
        if not phonenumbers.is_valid_number(parsed):
            return False, number, "Invalid phone number", None
        
        # Format in E.164 format
        formatted = phonenumbers.format_number(
            parsed, PhoneNumberFormat.E164
        )
        
        # Get the country and carrier
        country = phonenumbers.region_code_for_number(parsed)
        metadata["country"] = country
        
        # Check if it's a mobile number
        number_type = phonenumbers.number_type(parsed)
        is_mobile = (number_type == phonenumbers.PhoneNumberType.MOBILE)
        metadata["is_mobile"] = is_mobile
        
        # Check for other properties
        metadata["number_type"] = str(number_type)
        metadata["country_code"] = parsed.country_code
        metadata["national_number"] = parsed.national_number
        
        # Additional validations
        is_possible = phonenumbers.is_possible_number(parsed)
        if not is_possible:
            return False, formatted, "Number is not possible", metadata
        
        return True, formatted, None, metadata
    except Exception as e:
        return False, number, f"Validation error: {str(e)}", None


def validate_phone(number: str, strict: bool = False) -> Tuple[bool, str, Optional[str], Optional[Dict]]:
    """
    Validate and format a phone number.
    
    Uses the phonenumbers library if available, otherwise falls back to basic validation.
    
    Args:
        number: Phone number to validate
        strict: Whether to apply strict validation (country code check, etc.)
        
    Returns:
        Tuple[bool, str, str, dict]: (is_valid, formatted_number, error_message, metadata)
    """
    if PHONENUMBERS_AVAILABLE:
        return validate_phone_advanced(number)
    else:
        is_valid, formatted, error = validate_phone_basic(number)
        return is_valid, formatted, error, None


def is_valid_phone(number: str, strict: bool = False) -> bool:
    """
    Check if a phone number is valid.
    
    Args:
        number: Phone number to validate
        strict: Whether to apply strict validation
        
    Returns:
        bool: True if valid, False otherwise
    """
    is_valid, _, _, _ = validate_phone(number, strict)
    return is_valid


def format_phone(number: str) -> str:
    """
    Format a phone number in E.164 format.
    
    Args:
        number: Phone number to format
        
    Returns:
        str: Formatted phone number or original if invalid
        
    Raises:
        PhoneValidationError: If the phone number is invalid
    """
    is_valid, formatted, error, _ = validate_phone(number)
    if not is_valid:
        raise PhoneValidationError(error or "Invalid phone number", {"number": number})
    return formatted


def validate_batch_phone_numbers(phone_numbers: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Validate a batch of phone numbers.
    
    Args:
        phone_numbers: List of phone numbers to validate
        
    Returns:
        Dict: Dictionary with 'valid' and 'invalid' lists
    """
    valid = []
    invalid = []
    
    for number in phone_numbers:
        is_valid, formatted, error, metadata = validate_phone(number)
        if is_valid:
            valid.append({
                "original": number,
                "formatted": formatted,
                "metadata": metadata or {}
            })
        else:
            invalid.append({
                "original": number,
                "error": error,
                "metadata": metadata or {}
            })
    
    return {
        "valid": valid,
        "invalid": invalid,
        "summary": {
            "total": len(phone_numbers),
            "valid_count": len(valid),
            "invalid_count": len(invalid)
        }
    }


def extract_phone_numbers(text: str) -> List[str]:
    """
    Extract potential phone numbers from text.
    
    Args:
        text: Text to extract phone numbers from
        
    Returns:
        List[str]: List of potential phone numbers
    """
    # Define regex patterns for phone number detection
    patterns = [
        r'\+\d{1,3}\s?\d{1,14}',  # +1 123456789
        r'\(\d{1,4}\)\s?\d{1,14}', # (123) 456789
        r'\d{1,4}[- .]\d{1,4}[- .]\d{1,10}'  # 123-456-7890
    ]
    
    results = []
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        results.extend(matches)
    
    # Deduplicate and return
    return list(set(results))
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  # API service
  api:
    build: .
    container_name: inboxerr-api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/inboxerr
      - SMS_GATEWAY_URL=${SMS_GATEWAY_URL:-https://endpointnumber1.work.gd/api/3rdparty/v1}
      - SMS_GATEWAY_LOGIN=${SMS_GATEWAY_LOGIN:-}
      - SMS_GATEWAY_PASSWORD=${SMS_GATEWAY_PASSWORD:-}
      - SECRET_KEY=${SECRET_KEY:-CHANGEME_IN_PRODUCTION}
      - WEBHOOK_HOST=0.0.0.0
      - WEBHOOK_PORT=5000
      - LOG_LEVEL=DEBUG
    depends_on:
      db:
        condition: service_healthy
      db-init:
        condition: service_completed_successfully
    networks:
      - inboxerr-network
    restart: unless-stopped

  # Database service
  db:
    image: postgres:14-alpine
    container_name: inboxerr-db
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=inboxerr
    ports:
      - "5432:5432"
    networks:
      - inboxerr-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Database initialization service
  db-init:
    build: .
    container_name: inboxerr-db-init
    command: >
      sh -c "
        echo 'Waiting for database to be ready...' &&
        sleep 5 &&
        echo 'Running database migrations...' &&
        alembic upgrade head &&
        echo 'Creating admin user...' &&
        python app/scripts/create_admin.py &&
        echo 'Database initialization completed.'
      "
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/inboxerr
      - SECRET_KEY=${SECRET_KEY:-CHANGEME_IN_PRODUCTION}
    volumes:
      - .:/app
    depends_on:
      db:
        condition: service_healthy
    networks:
      - inboxerr-network
    restart: "no"  # Run once and exit

  # Optional: Redis for caching and task queue
  redis:
    image: redis:alpine
    container_name: inboxerr-redis
    ports:
      - "6379:6379"
    networks:
      - inboxerr-network
    restart: unless-stopped

  # Optional: PgAdmin for database management
  pgadmin:
    image: dpage/pgadmin4
    container_name: inboxerr-pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@inboxerr.com
      - PGADMIN_DEFAULT_PASSWORD=admin
    ports:
      - "5050:80"
    depends_on:
      - db
    networks:
      - inboxerr-network
    restart: unless-stopped

networks:
  inboxerr-network:
    driver: bridge

volumes:
  postgres_data:
</file>

<file path="project_structure.md">
# Inboxerr Backend Project Structure

```
/inboxerr-backend/
│
├── /app/                      # Application package
│   ├── __init__.py            # Package initializer
│   ├── main.py                # FastAPI application entry point
│   │
│   ├── /api/                  # API endpoints and routing
│   │   ├── __init__.py
│   │   ├── router.py          # Main API router
│   │   ├── /v1/               # API version 1
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/     # API endpoints by resource
│   │   │   │   ├── __init__.py
│   │   │   │   ├── auth.py    # Authentication endpoints
│   │   │   │   ├── messages.py # SMS message endpoints
│   │   │   │   ├── webhooks.py # Webhook endpoints
│   │   │   │   └── metrics.py  # Metrics and reporting endpoints
│   │   │   └── dependencies.py # API-specific dependencies
│   │
│   ├── /core/                 # Core application components
│   │   ├── __init__.py
│   │   ├── config.py          # Application configuration
│   │   ├── security.py        # Security utilities (auth, encryption)
│   │   ├── events.py          # Event handlers for application lifecycle
│   │   └── exceptions.py      # Custom exception classes
│   │
│   ├── /db/                   # Database related code
│   │   ├── __init__.py
│   │   ├── base.py            # Base DB session setup
│   │   ├── session.py         # DB session management
│   │   └── repositories/      # Repository pattern implementations
│   │       ├── __init__.py
│   │       ├── base.py        # Base repository class
│   │       ├── messages.py    # Message repository
│   │       └── users.py       # User repository
│   │
│   ├── /models/               # Database models
│   │   ├── __init__.py
│   │   ├── base.py            # Base model class
│   │   ├── message.py         # SMS message model
│   │   ├── user.py            # User model
│   │   └── webhook.py         # Webhook model
│   │
│   ├── /schemas/              # Pydantic schemas for API
│   │   ├── __init__.py
│   │   ├── base.py            # Base schema
│   │   ├── message.py         # Message schemas
│   │   ├── user.py            # User schemas
│   │   ├── webhook.py         # Webhook schemas
│   │   └── metrics.py         # Metrics schemas
│   │
│   ├── /services/             # Business logic services
│   │   ├── __init__.py
│   │   ├── sms/               # SMS related services
│   │   │   ├── __init__.py
│   │   │   ├── sender.py      # SMS sender implementation
│   │   │   ├── validator.py   # Phone/message validation
│   │   │   └── gateway.py     # SMS gateway client
│   │   │
│   │   ├── event_bus/         # Event management
│   │   │   ├── __init__.py
│   │   │   ├── bus.py         # Event bus implementation
│   │   │   ├── events.py      # Event definitions
│   │   │   └── handlers/      # Event handlers
│   │   │       ├── __init__.py
│   │   │       ├── message_handlers.py
│   │   │       └── system_handlers.py
│   │   │
│   │   ├── webhooks/          # Webhook handling
│   │   │   ├── __init__.py
│   │   │   ├── handler.py     # Webhook processor
│   │   │   └── manager.py     # Webhook registration/management
│   │   │
│   │   └── metrics/           # Metrics collection
│   │       ├── __init__.py
│   │       └── collector.py   # Metrics collector
│   │
│   └── /utils/                # Utility functions and helpers
│       ├── __init__.py
│       ├── phone.py           # Phone number utilities
│       ├── logging.py         # Logging configuration
│       └── pagination.py      # Pagination utilities
│
├── /alembic/                  # Database migrations
│   ├── env.py                 # Alembic environment
│   ├── README                 # Alembic readme
│   ├── script.py.mako         # Migration script template
│   └── /versions/             # Migration scripts
│
├── /tests/                    # Test suite
│   ├── __init__.py
│   ├── conftest.py            # Test configuration and fixtures
│   ├── /unit/                 # Unit tests
│   │   ├── __init__.py
│   │   ├── /services/         # Tests for services
│   │   └── /api/              # Tests for API endpoints
│   └── /integration/          # Integration tests
│       ├── __init__.py
│       └── /api/              # API integration tests
│
├── /scripts/                  # Utility scripts
│   ├── seed_db.py             # Database seeding script
│   └── generate_keys.py       # Generate security keys
│
├── .env.example               # Example environment variables
├── .gitignore                 # Git ignore file
├── docker-compose.yml         # Docker Compose configuration
├── Dockerfile                 # Docker build configuration
├── pyproject.toml             # Python project metadata
├── requirements.txt           # Python dependencies
├── requirements-dev.txt       # Development dependencies
└── README.md                  # Project documentation
```
</file>

<file path="scripts/seed_db.py">
"""
Placeholder script for seeding the database in development.
Currently not in use — extend as needed.
Seed the database with essential data (e.g., admin user).
Currently skips message seeding — placeholder for future use.

"""

import subprocess
import os
import sys

def run_admin_script():
    """Ensure an admin user exists by running create_admin.py."""
    try:
        subprocess.run(
            [sys.executable, "app/scripts/create_admin.py"],
            check=True,
            cwd=os.getcwd(),  # Ensures correct working directory
            env={**os.environ, "PYTHONPATH": os.getcwd()}
        )
        print("👮 Admin user created.")
    except subprocess.CalledProcessError:
        print("⚠️ Failed to create admin user. Check create_admin.py.")


def seed():
    print("🌱 [SKIPPED] No seed data logic implemented yet.")

if __name__ == "__main__":
    seed()
    run_admin_script()
</file>

<file path="alembic/env.py">
from logging.config import fileConfig
import os
import sys
from pathlib import Path

# Add the parent directory to sys.path
sys.path.append(str(Path(__file__).parent.parent))

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context
from app.core.config import settings
from app.db.base import Base


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# Get database URL from environment or settings
db_url = os.environ.get("ALEMBIC_DB_URL", settings.DATABASE_URL)

# Convert asyncpg URL to standard psycopg2 URL for Alembic
if "+asyncpg" in db_url:
    db_url = db_url.replace("+asyncpg", "")


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    # Set SQL Alchemy URL from our db_url
    config.set_main_option("sqlalchemy.url", db_url)
    
    url = config.get_main_option("sqlalchemy.url")
    
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Set SQL Alchemy URL from our db_url
    config.set_main_option("sqlalchemy.url", db_url)

    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="app/api/router.py">
"""
Main API router that includes all endpoint routers.
"""
from fastapi import APIRouter

from app.api.v1.endpoints import auth, messages, webhooks, metrics, campaigns, templates


# Create main API router
api_router = APIRouter()

# Include all endpoint routers with appropriate tags
api_router.include_router(
    auth.router, 
    prefix="/auth", 
    tags=["Authentication"]
)
api_router.include_router(
    messages.router, 
    prefix="/messages", 
    tags=["Messages"]
)
api_router.include_router(
    campaigns.router, 
    prefix="/campaigns", 
    tags=["Campaigns"]
)
api_router.include_router(
    templates.router, 
    prefix="/templates", 
    tags=["Templates"]
)
api_router.include_router(
    webhooks.router, 
    prefix="/webhooks", 
    tags=["Webhooks"]
)
api_router.include_router(
    metrics.router, 
    prefix="/metrics", 
    tags=["Metrics"]
)
</file>

<file path="app/api/v1/dependencies.py">
"""
Dependencies for API endpoints.
"""
from fastapi import Depends, HTTPException, status, Header
from fastapi.security import OAuth2PasswordBearer
from typing import Optional, List
import jwt
from datetime import datetime, timedelta, timezone

from app.core.config import settings
from app.core.exceptions import AuthenticationError, AuthorizationError
from app.schemas.user import User, TokenData, UserRole
from app.db.session import get_repository_factory
from app.db.repositories.users import UserRepository
from app.services.rate_limiter import RateLimiter

# OAuth2 scheme for token authentication
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_PREFIX}/auth/token")

#: Dependency injection provider for UserRepository.
#: This uses FastAPI's built-in dependency system to automatically inject
get_user_repository = get_repository_factory(UserRepository)




async def get_rate_limiter():
    """Get rate limiter service."""
    return RateLimiter()


async def get_current_user(
    token: str = Depends(oauth2_scheme),
    user_repository = Depends(get_user_repository)
) -> User:
    """
    Get the current authenticated user from the JWT token.
    
    Args:
        token: JWT token from Authorization header
        user_repository: User repository for database access
        
    Returns:
        User: The authenticated user
        
    Raises:
        AuthenticationError: If the token is invalid or expired
    """
    try:
        # Decode JWT token
        payload = jwt.decode(
            token, 
            settings.SECRET_KEY, 
            algorithms=["HS256"]
        )
        token_data = TokenData(**payload)
        
        # Check if token is expired
        if token_data.exp and datetime.now(timezone.utc) > token_data.exp:
            raise AuthenticationError("Token has expired")
        
        # Get user from database
        user = await user_repository.get_by_id(token_data.sub)
        if not user:
            raise AuthenticationError("User not found")
        
        # Check if user is active
        if not user.is_active:
            raise AuthenticationError("User is inactive")
        
        return user
        
    except jwt.PyJWTError:
        raise AuthenticationError("Invalid token")


async def verify_api_key(
    api_key: str = Header(..., alias=settings.API_KEY_HEADER),
    user_repository = Depends(get_user_repository)
) -> User:
    """
    Verify API key and return the associated user.
    
    Args:
        api_key: API key from header
        user_repository: User repository for database access
        
    Returns:
        User: The authenticated user
        
    Raises:
        AuthenticationError: If the API key is invalid
    """
    try:
        # Get API key from database
        api_key_record = await user_repository.get_api_key(api_key)
        if not api_key_record:
            raise AuthenticationError("Invalid API key")
        
        # Check if API key is active
        if not api_key_record.is_active:
            raise AuthenticationError("API key is inactive")
        
        # Check if API key is expired
        if api_key_record.expires_at and datetime.now(timezone.utc) > api_key_record.expires_at:
            raise AuthenticationError("API key has expired")
        
        # Get user associated with API key
        user = await user_repository.get_by_id(api_key_record.user_id)
        if not user:
            raise AuthenticationError("User not found")
        
        # Check if user is active
        if not user.is_active:
            raise AuthenticationError("User is inactive")
        
        # Update last used timestamp
        await user_repository.update_api_key_usage(api_key)
        
        return user
        
    except Exception as e:
        if isinstance(e, AuthenticationError):
            raise
        raise AuthenticationError("API key verification failed")


async def get_current_active_user(
    current_user: User = Depends(get_current_user)
) -> User:
    """
    Get the current active user.
    
    Args:
        current_user: Current authenticated user
        
    Returns:
        User: The authenticated active user
        
    Raises:
        AuthenticationError: If the user is inactive
    """
    if not current_user.is_active:
        raise AuthenticationError("Inactive user")
    return current_user


async def validate_permissions(
    required_permissions: List[str],
    current_user: User = Depends(get_current_user)
) -> None:
    """
    Validate that the current user has the required permissions.
    
    Args:
        required_permissions: List of required permissions
        current_user: Current authenticated user
        
    Raises:
        AuthorizationError: If the user doesn't have the required permissions
    """
    # Admin role has all permissions
    if current_user.role == UserRole.ADMIN:
        return
    
    # TODO: Implement proper permission checking
    # For now, just verify role-based access
    if current_user.role != UserRole.API and "api" in required_permissions:
        raise AuthorizationError("Insufficient permissions")
</file>

<file path="app/api/v1/endpoints/webhooks.py">
# app/api/v1/endpoints/webhooks.py
"""
API endpoints for webhook management.
"""
import logging
from typing import Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Request, Body, Header, status

from app.api.v1.dependencies import get_current_user
from app.schemas.user import User
from app.services.webhooks.manager import process_gateway_webhook
from app.services.webhooks.manager import fetch_registered_webhooks_from_gateway


router = APIRouter()
logger = logging.getLogger("inboxerr.webhooks")

@router.get("/")
async def list_webhooks(
    current_user: User = Depends(get_current_user)
):
    """
    List all webhooks for the current user.
    """
    # This is a stub - implementation will be added later
    return {"message": "Webhook listing not implemented yet"}

@router.post("/gateway", status_code=status.HTTP_200_OK)
async def webhook_receiver(
    request: Request,
    x_signature: str = Header(None),
    x_timestamp: str = Header(None)
):
    """
    Receive webhooks from the SMS Gateway.
    
    This endpoint is called by the SMS Gateway when events occur.
    No authentication is required as we validate using signatures.
    """
    # Get raw body for signature validation
    body = await request.body()
    
    # Prepare headers for signature verification
    headers = {
        "X-Signature": x_signature,
        "X-Timestamp": x_timestamp
    }
    
    # Process the webhook
    success, result = await process_gateway_webhook(body, headers)
    
    if not success:
        logger.error(f"Error processing webhook: {result.get('error')}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=result.get('error', 'Error processing webhook')
        )
    
    return result

@router.get("/registered", tags=["Webhooks"])
async def get_registered_gateway_webhooks(current_user: User = Depends(get_current_user)):
    """
    Fetch registered webhooks from the external SMS Gateway.
    This helps confirm webhook registration status.
    """
    webhooks = await fetch_registered_webhooks_from_gateway()
    return {"registered_webhooks": webhooks}
</file>

<file path="app/core/events.py">
"""
Event handlers for application lifecycle events.
"""
import asyncio
import logging
from typing import Dict, List, Any

from app.core.config import settings
from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType

logger = logging.getLogger("inboxerr")

# Collection of background tasks to manage
background_tasks: List[asyncio.Task] = []


async def startup_event_handler() -> None:
    """
    Handle application startup.
    
    Initialize services, database connections, and start background processes.
    """
    logger.info("Starting Inboxerr Backend application")
    
    # Initialize database (async)
    try:
        # We'll implement this in the database module
        from app.db.session import initialize_database
        await initialize_database()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing database: {e}")
        # Don't raise error to allow startup to continue
    
    # Initialize event bus
    try:
        event_bus = get_event_bus()
        await event_bus.initialize()
        logger.info("Event bus initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing event bus: {e}")
    
    # Start retry engine if enabled
    if settings.RETRY_ENABLED:
        try:
            from app.services.sms.retry_engine import get_retry_engine
            retry_engine = await get_retry_engine()
            retry_task = asyncio.create_task(retry_engine.start())
            background_tasks.append(retry_task)
            logger.info("Retry engine started successfully")
        except Exception as e:
            logger.error(f"Error starting retry engine: {e}")
    
    # Start webhook listener if enabled
    try:
        from app.services.webhooks.manager import initialize_webhook_manager
        await initialize_webhook_manager()
        logger.info("Webhook manager initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing webhooks: {e}")
    
    # Initialize metrics collector
    try:
        from app.services.metrics.collector import initialize_metrics
        await initialize_metrics()
        logger.info("Metrics collector initialized successfully")
        # Start metrics background task
        if settings.METRICS_ENABLED:
            try:
                from app.services.metrics.collector import schedule_metrics_update
                metrics_task = asyncio.create_task(schedule_metrics_update())
                metrics_task.set_name("metrics_updater")
                background_tasks.append(metrics_task)
                logger.info("Metrics background task started")
            except Exception as e:
                logger.error(f"Error starting metrics task: {e}")
    except Exception as e:
        logger.error(f"Error initializing metrics: {e}")
    
    # Log successful startup
    logger.info(f"✅ {settings.PROJECT_NAME} v{settings.VERSION} startup complete")


async def shutdown_event_handler() -> None:
    """
    Handle application shutdown.
    
    Clean up resources and close connections properly.
    """
    logger.info("Shutting down Inboxerr Backend application")
    
    # Publish shutdown event
    try:
        event_bus = get_event_bus()
        await event_bus.publish(EventType.SYSTEM_SHUTDOWN, {
            "reason": "Application shutdown",
            "graceful": True
        })
        logger.info("Published shutdown event")
    except Exception as e:
        logger.error(f"Error publishing shutdown event: {e}")
    
    # Cancel all background tasks
    for task in background_tasks:
        if not task.done():
            task.cancel()
            try:
                # Wait briefly for task to cancel
                await asyncio.wait_for(task, timeout=5.0)
            except (asyncio.TimeoutError, asyncio.CancelledError):
                logger.warning(f"Task {task.get_name()} was cancelled")
    
    # Close database connections
    try:
        from app.db.session import close_database_connections
        await close_database_connections()
        logger.info("Database connections closed")
    except Exception as e:
        logger.error(f"Error closing database connections: {e}")
    
    # Shutdown webhook manager
    try:
        from app.services.webhooks.manager import shutdown_webhook_manager
        await shutdown_webhook_manager()
        logger.info("Webhook manager shutdown complete")
    except Exception as e:
        logger.error(f"Error shutting down webhook manager: {e}")
    
    logger.info("✅ Application shutdown complete")
</file>

<file path="app/db/base.py">
"""
Import all models here to ensure they are registered with SQLAlchemy.
"""
# Import Base
from app.models.base import Base

# Import all models
from app.models.user import User, APIKey
from app.models.campaign import Campaign
from app.models.message import Message, MessageEvent, MessageBatch, MessageTemplate
from app.models.webhook import Webhook, WebhookDelivery, WebhookEvent

# Metrics Models
from app.models.metrics import UserMetrics

# This allows alembic to auto-discover all models when creating migrations
</file>

<file path="app/db/repositories/campaigns.py">
# app/db/repositories/campaigns.py
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any, Tuple
from app.utils.ids import generate_prefixed_id, IDPrefix


from sqlalchemy import select, update, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.repositories.base import BaseRepository
from app.models.campaign import Campaign
from app.models.message import Message


class CampaignRepository(BaseRepository[Campaign, Dict[str, Any], Dict[str, Any]]):
    """Campaign repository for campaign operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize repository with session."""
        super().__init__(session=session, model=Campaign)
    
    async def create_campaign(
        self,
        *,
        name: str,
        user_id: str,
        description: Optional[str] = None,
        scheduled_start_at: Optional[datetime] = None,
        scheduled_end_at: Optional[datetime] = None,
        settings: Optional[Dict[str, Any]] = None
    ) -> Campaign:
        """
        Create a new campaign.
        
        Args:
            name: Campaign name
            user_id: User ID
            description: Optional campaign description
            scheduled_start_at: Optional scheduled start time
            scheduled_end_at: Optional scheduled end time
            settings: Optional campaign settings
            
        Returns:
            Campaign: Created campaign
        """
        campaign_id = generate_prefixed_id(IDPrefix.CAMPAIGN)
        campaign = Campaign(
            id=campaign_id,
            name=name,
            description=description,
            status="draft",
            user_id=user_id,
            scheduled_start_at=scheduled_start_at,
            scheduled_end_at=scheduled_end_at,
            settings=settings or {}
        )
        
        self.session.add(campaign)
        await self.session.commit()
        await self.session.refresh(campaign)
        
        return campaign
    
    async def update_campaign_status(
        self,
        *,
        campaign_id: str,
        status: str,
        started_at: Optional[datetime] = None,
        completed_at: Optional[datetime] = None
    ) -> Optional[Campaign]:
        """
        Update campaign status with proper transaction handling.
        
        Args:
            campaign_id: Campaign ID
            status: New status
            started_at: Optional start timestamp
            completed_at: Optional completion timestamp
            
        Returns:
            Campaign: Updated campaign or None
        """
        # Start a transaction
        async with self.session.begin():
            # Get campaign
            campaign = await self.get_by_id(campaign_id)
            if not campaign:
                return None
            
            old_status = campaign.status
            campaign.status = status
            
            if started_at:
                campaign.started_at = started_at
            
            if completed_at:
                campaign.completed_at = completed_at
            
            # If status is active and no start time, set it now
            if status == "active" and not campaign.started_at:
                campaign.started_at = datetime.now(timezone.utc)
            
            # If status is completed and no completion time, set it now
            if status in ["completed", "cancelled", "failed"] and not campaign.completed_at:
                campaign.completed_at = datetime.now(timezone.utc)
            
            # Add campaign to session
            self.session.add(campaign)
            
            # If transitioning from draft to active, also update any pending messages
            # that are associated with this campaign
            if old_status == "draft" and status == "active":
                from app.models.message import Message
                from app.schemas.message import MessageStatus
                
                # Update messages
                query = update(Message).where(
                    and_(
                        Message.campaign_id == campaign_id,
                        Message.status == MessageStatus.PENDING,
                        or_(
                            Message.scheduled_at.is_(None),
                            Message.scheduled_at <= datetime.now(timezone.utc)
                        )
                    )
                ).values(
                    status=MessageStatus.PROCESSED
                )
                
                await self.session.execute(query)
            
            # Publish event about status change
            from app.services.event_bus.bus import get_event_bus
            from app.services.event_bus.events import EventType
            
            event_bus = get_event_bus()
            event_type = None
            
            if status == "active":
                event_type = EventType.CAMPAIGN_STARTED
            elif status == "paused":
                event_type = EventType.CAMPAIGN_PAUSED
            elif status == "completed":
                event_type = EventType.CAMPAIGN_COMPLETED
            elif status == "cancelled":
                event_type = EventType.CAMPAIGN_CANCELLED
            elif status == "failed":
                event_type = EventType.CAMPAIGN_FAILED
                
            if event_type:
                await event_bus.publish(
                    event_type,
                    {
                        "campaign_id": campaign_id,
                        "previous_status": old_status,
                        "new_status": status,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                )
            
            # No explicit commit needed - will be committed at the end of the context manager
            await self.session.refresh(campaign)
            
            return campaign

    async def update_campaign_stats(
        self,
        *,
        campaign_id: str,
        increment_sent: int = 0,
        increment_delivered: int = 0,
        increment_failed: int = 0
    ) -> Optional[Campaign]:
        """
        Update campaign statistics.
        
        Args:
            campaign_id: Campaign ID
            increment_sent: Increment sent count
            increment_delivered: Increment delivered count
            increment_failed: Increment failed count
            
        Returns:
            Campaign: Updated campaign or None
        """
        campaign = await self.get_by_id(campaign_id)
        if not campaign:
            return None
        
        # Update counts
        campaign.sent_count += increment_sent
        campaign.delivered_count += increment_delivered
        campaign.failed_count += increment_failed
        
        # Check if campaign is complete
        total_processed = campaign.sent_count + campaign.failed_count
        if total_processed >= campaign.total_messages and campaign.total_messages > 0:
            campaign.status = "completed"
            campaign.completed_at = datetime.now(timezone.utc)
        
        self.session.add(campaign)
        await self.session.commit()
        await self.session.refresh(campaign)
        
        return campaign
    
    async def get_campaigns_for_user(
        self,
        *,
        user_id: str,
        status: Optional[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Campaign], int]:
        """
        Get campaigns for a user with optional filtering.
        
        Args:
            user_id: User ID
            status: Optional status filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Campaign], int]: List of campaigns and total count
        """
        # Base query
        query = select(Campaign).where(Campaign.user_id == user_id)
        count_query = select(func.count()).select_from(Campaign).where(Campaign.user_id == user_id)
        
        # Apply status filter
        if status:
            query = query.where(Campaign.status == status)
            count_query = count_query.where(Campaign.status == status)
        
        # Order by created_at desc
        query = query.order_by(desc(Campaign.created_at))
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        campaigns = result.scalars().all()
        total = count_result.scalar_one()
        
        return campaigns, total
    
    async def add_messages_to_campaign(
        self,
        *,
        campaign_id: str,
        phone_numbers: List[str],
        message_text: str,
        user_id: str
    ) -> int:
        """
        Add messages to a campaign.
        
        Args:
            campaign_id: Campaign ID
            phone_numbers: List of recipient phone numbers
            message_text: Message content
            user_id: User ID
            
        Returns:
            int: Number of messages added
        """
        from app.db.repositories.messages import MessageRepository
        from app.utils.phone import validate_phone
        
        # Get campaign
        campaign = await self.get_by_id(campaign_id)
        if not campaign:
            return 0
        
        # Validate campaign belongs to user
        if campaign.user_id != user_id:
            return 0
        
        # TODO: Implement bulk insertion for better performance
        message_repo = MessageRepository(self.session)
        added_count = 0
        
        for phone in phone_numbers:
            # Basic validation
            is_valid, formatted_number, error, _ = validate_phone(phone)
            if is_valid:
                # Add message to campaign
                await message_repo.create_message(
                    phone_number=formatted_number,
                    message_text=message_text,
                    user_id=user_id,
                    scheduled_at=campaign.scheduled_start_at,
                    metadata={"campaign_id": campaign_id},
                    campaign_id=campaign_id  # Direct link to campaign
                )
                added_count += 1
        
        # Update campaign message count
        if added_count > 0:
            campaign.total_messages += added_count
            self.session.add(campaign)
            await self.session.commit()
        
        return added_count
</file>

<file path="app/db/repositories/templates.py">
# app/db/repositories/templates.py
"""
Repository for message template operations.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any, Tuple
from app.utils.ids import generate_prefixed_id, IDPrefix
import re
import logging

from sqlalchemy import select, update, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.db.repositories.base import BaseRepository
from app.models.message import MessageTemplate
from app.schemas.template import MessageTemplateCreate, MessageTemplateUpdate

logger = logging.getLogger("inboxerr.templates")


class TemplateRepository(BaseRepository[MessageTemplate, MessageTemplateCreate, MessageTemplateUpdate]):
    """Repository for message template operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize repository with session."""
        super().__init__(session=session, model=MessageTemplate)
    
    async def create_template(
        self,
        *,
        name: str,
        content: str,
        user_id: str,
        description: Optional[str] = None,
        variables: Optional[List[str]] = None,
        is_active: bool = True
    ) -> MessageTemplate:
        """
        Create a new message template.
        
        Args:
            name: Template name
            content: Template content with placeholders
            user_id: User ID
            description: Optional template description
            variables: Optional list of variables
            is_active: Whether the template is active
            
        Returns:
            MessageTemplate: Created template
        """
        # Extract variables from content if not provided
        if variables is None:
            pattern = r"{{([a-zA-Z0-9_]+)}}"
            variables = list(set(re.findall(pattern, content)))
        
        # Create template
        template_id = generate_prefixed_id(IDPrefix.TEMPLATE)
        template = MessageTemplate(
            id=template_id,
            name=name,
            content=content,
            description=description,
            is_active=is_active,
            user_id=user_id,
            variables=variables
        )
        
        self.session.add(template)
        await self.session.commit()
        await self.session.refresh(template)

        # Publish template created event
        try:
            event_bus = get_event_bus()
            await event_bus.publish(
                EventType.TEMPLATE_CREATED, 
                {
                    "template_id": template.id,
                    "user_id": user_id,
                    "name": name
                }
            )
        except Exception as e:
            logger.warning(f"Failed to publish template created event: {e}")
        
        return template
    
    async def get_templates_for_user(
        self,
        *,
        user_id: str,
        active_only: bool = False,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[MessageTemplate], int]:
        """
        Get message templates for a user.
        
        Args:
            user_id: User ID
            active_only: Whether to return only active templates
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[MessageTemplate], int]: List of templates and total count
        """
        # Base query
        query = select(MessageTemplate).where(MessageTemplate.user_id == user_id)
        count_query = select(func.count()).select_from(MessageTemplate).where(MessageTemplate.user_id == user_id)
        
        # Filter active templates if requested
        if active_only:
            query = query.where(MessageTemplate.is_active == True)
            count_query = count_query.where(MessageTemplate.is_active == True)
        
        # Order by name
        query = query.order_by(MessageTemplate.name)
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        templates = result.scalars().all()
        total = count_result.scalar_one()
        
        return templates, total
    
    async def apply_template(
        self,
        *,
        template_id: str,
        variables: Dict[str, str]
    ) -> Optional[str]:
        """
        Apply variables to a template.
        
        Args:
            template_id: Template ID
            variables: Dictionary of variable values
            
        Returns:
            str: Processed template content or None if template not found
        """
        # Get template
        template = await self.get_by_id(template_id)
        if not template:
            return None
        
        # Apply variables to template
        content = template.content
        
        for key, value in variables.items():
            # Replace {{key}} with value
            content = content.replace(f"{{{{{key}}}}}", value)

        # Publish template used event
        try:
            event_bus = get_event_bus()
            await event_bus.publish(
                EventType.TEMPLATE_USED, 
                {
                    "template_id": template_id,
                    "user_id": template.user_id,
                    "name": template.name
                }
            )
        except Exception as e:
            logger.warning(f"Failed to publish template used event: {e}")
        
        return content
</file>

<file path="app/db/repositories/users.py">
"""
User repository for database operations related to users.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any
from app.utils.ids import generate_prefixed_id, IDPrefix
from uuid import uuid4

from sqlalchemy import select, update
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.security import get_password_hash, generate_api_key
from app.db.repositories.base import BaseRepository
from app.models.user import User, APIKey
from app.schemas.user import UserCreate, UserUpdate


class UserRepository(BaseRepository[User, UserCreate, UserUpdate]):
    """User repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and User model."""
        super().__init__(session=session, model=User)
    
    async def get_by_email(self, email: str) -> Optional[User]:
        """
        Get a user by email.
        
        Args:
            email: User email
            
        Returns:
            User: Found user or None
        """
        return await self.get_by_attribute("email", email)
    
    async def create(
        self, 
        *,
        email: str,
        hashed_password: str,
        full_name: Optional[str] = None,
        is_active: bool = True,
        role: str = "user"
    ) -> User:
        """
        Create a new user.
        
        Args:
            email: User email
            hashed_password: Hashed password
            full_name: User's full name
            is_active: Whether the user is active
            role: User role
            
        Returns:
            User: Created user
        """
        db_obj = User(
            email=email,
            hashed_password=hashed_password,
            full_name=full_name,
            is_active=is_active,
            role=role
        )
        
        self.session.add(db_obj)
        await self.session.commit()
        await self.session.refresh(db_obj)
        
        return db_obj
    
    async def update_password(self, *, user_id: str, new_password: str) -> Optional[User]:
        """
        Update user password.
        
        Args:
            user_id: User ID
            new_password: New password (plain text)
            
        Returns:
            User: Updated user or None
        """
        # Hash the new password
        hashed_password = get_password_hash(new_password)
        
        # Update the user
        return await self.update(
            id=user_id,
            obj_in={"hashed_password": hashed_password}
        )
    
    async def create_api_key(
        self, 
        *,
        user_id: str,
        name: str,
        expires_at: Optional[datetime] = None,
        permissions: List[str] = None
    ) -> APIKey:
        """
        Create a new API key for a user.
        
        Args:
            user_id: User ID
            name: API key name
            expires_at: Expiration timestamp
            permissions: List of permissions
            
        Returns:
            APIKey: Created API key
        """
        # Generate API key
        key_value = generate_api_key()
        
        # Create API key
        api_key = APIKey(
            id=str(uuid4()),
            key=key_value,
            name=name,
            user_id=user_id,
            expires_at=expires_at,
            is_active=True,
            permissions=permissions or []
        )
        
        self.session.add(api_key)
        await self.session.commit()
        await self.session.refresh(api_key)
        
        return api_key
    
    async def get_api_key(self, key: str) -> Optional[APIKey]:
        """
        Get an API key by its value.
        
        Args:
            key: API key value
            
        Returns:
            APIKey: Found API key or None
        """
        query = select(APIKey).where(APIKey.key == key, APIKey.is_active == True)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def get_api_key_by_id(self, key_id: str) -> Optional[APIKey]:
        """
        Get an API key by its ID.
        
        Args:
            key_id: API key ID
            
        Returns:
            APIKey: Found API key or None
        """
        query = select(APIKey).where(APIKey.id == key_id)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def list_api_keys(self, user_id: str) -> List[APIKey]:
        """
        List all API keys for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            List[APIKey]: List of API keys
        """
        query = select(APIKey).where(APIKey.user_id == user_id)
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update_api_key_usage(self, key: str) -> bool:
        """
        Update the last_used_at timestamp for an API key.
        
        Args:
            key: API key value
            
        Returns:
            bool: True if updated, False if not found
        """
        query = update(APIKey).where(
            APIKey.key == key, 
            APIKey.is_active == True
        ).values(
            last_used_at=datetime.now(timezone.utc)
        )
        
        result = await self.session.execute(query)
        await self.session.commit()
        
        return result.rowcount > 0
    
    async def delete_api_key(self, key_id: str) -> bool:
        """
        Delete an API key.
        
        Args:
            key_id: API key ID
            
        Returns:
            bool: True if deleted, False if not found
        """
        api_key = await self.get_api_key_by_id(key_id)
        if not api_key:
            return False
            
        await self.session.delete(api_key)
        await self.session.commit()
        
        return True
    
    async def deactivate_api_key(self, key_id: str) -> bool:
        """
        Deactivate an API key without deleting it.
        
        Args:
            key_id: API key ID
            
        Returns:
            bool: True if deactivated, False if not found
        """
        api_key = await self.get_api_key_by_id(key_id)
        if not api_key:
            return False
            
        api_key.is_active = False
        self.session.add(api_key)
        await self.session.commit()
        
        return True
</file>

<file path="app/db/session.py">
"""
Database session management.
"""
# app/db/session.py
import logging
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Any, Type, TypeVar

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from fastapi import Depends

from app.core.config import settings
from app.db.base import Base

logger = logging.getLogger("inboxerr.db")

# Create async engine with optimized pool settings
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,
    # Enhanced pool settings for better concurrency
    pool_size=20,
    max_overflow=30,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True
)

# Create async session factory
async_session_factory = sessionmaker(
    engine, class_=AsyncSession, expire_on_commit=False, autoflush=False
)

# Context manager for database sessions
@asynccontextmanager
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    """
    Context manager for database sessions.
    
    Automatically handles commit/rollback and ensures session is closed.
    
    Usage:
        async with get_session() as session:
            # Use session here
    """
    session = async_session_factory()
    try:
        yield session
        await session.commit()
        logger.debug("Database session committed")
    except Exception as e:
        await session.rollback()
        logger.error(f"Database session rolled back due to: {str(e)}")
        raise
    finally:
        await session.close()
        logger.debug("Database session closed")

# Dependency function for FastAPI endpoints
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Get a database session for FastAPI endpoints via dependency injection.
    """
    async with get_session() as session:
        yield session

# Create a type variable for repository types
T = TypeVar('T')

# Factory function for repositories
def get_repository_factory(repo_type: Type[T]):
    """
    Create a repository factory for use with FastAPI dependency injection.
    
    Usage:
        @router.get("/")
        async def endpoint(repo = Depends(get_repository_factory(UserRepository))):
            # Use repo here
    """
    async def _get_repo(session: AsyncSession = Depends(get_db)) -> T:
        return repo_type(session)
    return _get_repo

# Context manager for repositories
@asynccontextmanager
async def get_repository_context(repo_type: Type[T]) -> AsyncGenerator[T, None]:
    """
    Get a repository with managed session lifecycle.
    
    Usage:
        async with get_repository_context(UserRepository) as repo:
            # Use repo here
    """
    async with get_session() as session:
        yield repo_type(session)

# Legacy function for backward compatibility
async def get_repository(repo_type: Type[T]) -> T:
    """
    Legacy repository factory (for backward compatibility).
    Warning: Session must be manually closed when using this function.
    
    This will be deprecated - use get_repository_context instead.
    """
    logger.warning(
        "Using deprecated get_repository function - session won't be automatically closed. "
        "Consider using get_repository_context instead."
    )
    session = async_session_factory()
    return repo_type(session)

async def initialize_database() -> None:
    """
    Initialize the database connection pool and run any startup tasks.
    
    This should be called during application startup.
    """
    logger.info("Initializing database connection pool")
    
    # Test database connection
    async with get_session() as session:
        try:
            # Use text() for raw SQL queries
            from sqlalchemy import text
            query = text("SELECT 1")
            await session.execute(query)
            logger.info("Database connection successful")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
            
    logger.info("Database initialization complete")


async def close_database_connections() -> None:
    """
    Close all database connections in the pool.
    
    This should be called during application shutdown.
    """
    logger.info("Closing database connections")
    
    # Dispose the engine to close all connections in the pool
    await engine.dispose()
    
    logger.info("Database connections closed")
</file>

<file path="app/models/base.py">
"""
Base database model with common fields and methods.
"""
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, Optional

from sqlalchemy import Column, DateTime, String
from sqlalchemy.ext.declarative import as_declarative, declared_attr


@as_declarative()
class Base:
    """Base class for all database models."""
    
    # Generate __tablename__ automatically from class name
    @declared_attr
    def __tablename__(cls) -> str:
        return cls.__name__.lower()
    
    # Common columns for all models
    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc), nullable=False)
    updated_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc), onupdate=lambda: datetime.now(timezone.utc), nullable=False)
    
    def dict(self) -> Dict[str, Any]:
        """Convert model to dictionary."""
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Base":
        """Create model instance from dictionary."""
        return cls(**{
            k: v for k, v in data.items() 
            if k in [c.name for c in cls.__table__.columns]
        })
</file>

<file path="app/models/webhook.py">
"""
Database models for webhook management.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text
from sqlalchemy.orm import relationship

from app.models.base import Base


class Webhook(Base):
    """Webhook configuration model."""
    
    # Webhook configuration
    name = Column(String, nullable=False)
    url = Column(String, nullable=False)
    event_types = Column(JSON, nullable=False)  # List of event types to send
    is_active = Column(Boolean, default=True, nullable=False)
    secret_key = Column(String, nullable=True)  # For signature validation
    
    # Ownership and association
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    gateway_webhook_id = Column(String, nullable=True)  # ID from SMS gateway
    
    # Stats
    last_triggered_at = Column(DateTime(timezone=True), nullable=True)
    success_count = Column(Integer, default=0, nullable=False)
    failure_count = Column(Integer, default=0, nullable=False)
    
    # Relationships
    user = relationship("User")
    deliveries = relationship("WebhookDelivery", back_populates="webhook", cascade="all, delete-orphan")


class WebhookDelivery(Base):
    """Model for tracking webhook delivery attempts."""
    
    webhook_id = Column(String, ForeignKey("webhook.id"), nullable=False, index=True)
    event_type = Column(String, nullable=False, index=True)
    message_id = Column(String, ForeignKey("message.id"), nullable=True, index=True)
    payload = Column(JSON, nullable=False)
    status_code = Column(Integer, nullable=True)
    is_success = Column(Boolean, nullable=False)
    error_message = Column(String, nullable=True)
    retry_count = Column(Integer, default=0, nullable=False)
    next_retry_at = Column(DateTime(timezone=True), nullable=True)
    
    # Relationships
    webhook = relationship("Webhook", back_populates="deliveries")
    message = relationship("Message")


class WebhookEvent(Base):
    """Model for tracking webhook events received from SMS gateway."""
    
    event_type = Column(String, nullable=False, index=True)
    phone_number = Column(String, nullable=True, index=True)
    message_id = Column(String, nullable=True, index=True)
    gateway_message_id = Column(String, nullable=True, index=True)
    payload = Column(JSON, nullable=False)
    processed = Column(Boolean, default=False, nullable=False)
    error_message = Column(String, nullable=True)
</file>

<file path="app/schemas/user.py">
"""
Pydantic schemas for user-related API operations.
"""
from typing import List, Optional
from datetime import datetime, timezone
from enum import Enum
from pydantic import BaseModel, Field, EmailStr, validator


class UserRole(str, Enum):
    """User role enum."""
    ADMIN = "admin"
    USER = "user"
    API = "api"


class UserBase(BaseModel):
    """Base user schema."""
    email: Optional[EmailStr] = Field(None, description="User email address")
    full_name: Optional[str] = Field(None, description="User's full name")
    is_active: Optional[bool] = Field(True, description="Whether the user is active")
    role: Optional[UserRole] = Field(UserRole.USER, description="User role")


class UserCreate(UserBase):
    """Schema for creating a new user."""
    email: EmailStr = Field(..., description="User email address")
    password: str = Field(..., description="User password")
    
    @validator("password")
    def validate_password(cls, v):
        """Validate password strength."""
        if len(v) < 8:
            raise ValueError("Password must be at least 8 characters long")
        if not any(char.isdigit() for char in v):
            raise ValueError("Password must contain at least one digit")
        if not any(char.isupper() for char in v):
            raise ValueError("Password must contain at least one uppercase letter")
        return v


class UserUpdate(UserBase):
    """Schema for updating a user."""
    password: Optional[str] = Field(None, description="User password")
    
    @validator("password")
    def validate_password(cls, v):
        """Validate password if provided."""
        if v is not None:
            if len(v) < 8:
                raise ValueError("Password must be at least 8 characters long")
            if not any(char.isdigit() for char in v):
                raise ValueError("Password must contain at least one digit")
            if not any(char.isupper() for char in v):
                raise ValueError("Password must contain at least one uppercase letter")
        return v


class User(UserBase):
    """Schema for user response."""
    id: str = Field(..., description="User ID")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class UserInDB(User):
    """Schema for user in database (with hashed password)."""
    hashed_password: str = Field(..., description="Hashed password")


class Token(BaseModel):
    """Schema for authentication token."""
    access_token: str
    token_type: str = "bearer"
    expires_at: datetime


class TokenData(BaseModel):
    """Schema for token payload."""
    sub: str  # User ID
    exp: Optional[datetime] = None
    role: Optional[str] = None


class APIKey(BaseModel):
    """Schema for API key."""
    id: str = Field(..., description="API key ID")
    key: str = Field(..., description="API key")
    name: str = Field(..., description="API key name")
    user_id: str = Field(..., description="User who owns the API key")
    created_at: datetime = Field(..., description="Creation timestamp")
    expires_at: Optional[datetime] = Field(None, description="Expiration timestamp")
    is_active: bool = Field(True, description="Whether the API key is active")
    last_used_at: Optional[datetime] = Field(None, description="Last usage timestamp")
    permissions: List[str] = Field(default=[], description="List of permissions")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class APIKeyCreate(BaseModel):
    """Schema for creating a new API key."""
    name: str = Field(..., description="API key name")
    expires_at: Optional[datetime] = Field(None, description="Expiration timestamp")
    permissions: Optional[List[str]] = Field(default=[], description="List of permissions")
</file>

<file path="app/services/campaigns/processor.py">
# app/services/campaigns/processor.py
import asyncio
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import uuid

from app.core.config import settings
from app.db.repositories.campaigns import CampaignRepository
from app.db.repositories.messages import MessageRepository
from app.schemas.message import MessageStatus
from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.services.sms.sender import SMSSender, get_sms_sender
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.campaigns")

class CampaignProcessor:
    """
    Service for processing SMS campaigns.
    
    Manages campaign execution, chunked processing, and status tracking.
    Uses context managers for database operations to prevent connection leaks.
    """
    
    def __init__(
        self,
        sms_sender: SMSSender,
        event_bus: Any
    ):
        """
        Initialize campaign processor with required dependencies.
        
        Removed repository dependencies to prevent long-lived connections.
        The repositories will be created as needed using context managers.
        
        Args:
            sms_sender: SMS sender service
            event_bus: Event bus for publishing events
        """
        self.sms_sender = sms_sender
        self.event_bus = event_bus
        self._processing_campaigns = set()
        self._chunk_size = settings.BATCH_SIZE  # Default from settings
        self._semaphore = asyncio.Semaphore(5)  # Limit concurrent campaigns
    
    async def start_campaign(self, campaign_id: str, user_id: str) -> bool:
        """
        Start a campaign.
        
        Args:
            campaign_id: Campaign ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if campaign was started, False otherwise
        """
        # Use context manager for repository access
        async with get_repository_context(CampaignRepository) as campaign_repository:
            # Get campaign
            campaign = await campaign_repository.get_by_id(campaign_id)
            if not campaign:
                return False
            
            # Validate ownership
            if campaign.user_id != user_id:
                return False
            
            # Check if campaign can be started
            if campaign.status != "draft" and campaign.status != "paused":
                return False
            
            # Update status to active
            updated = await campaign_repository.update_campaign_status(
                campaign_id=campaign_id,
                status="active",
                started_at=datetime.now(timezone.utc)
            )
            
            if not updated:
                return False
            
            # Get total_messages for the event
            total_messages = campaign.total_messages
        
        # Start processing in background
        asyncio.create_task(self._process_campaign(campaign_id))
        
        # Publish event
        await self.event_bus.publish(
            EventType.BATCH_CREATED,
            {
                "campaign_id": campaign_id,
                "user_id": user_id,
                "total_messages": total_messages
            }
        )
        
        return True
    
    async def pause_campaign(self, campaign_id: str, user_id: str) -> bool:
        """
        Pause a campaign.
        
        Args:
            campaign_id: Campaign ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if campaign was paused, False otherwise
        """
        # Use context manager for repository access
        async with get_repository_context(CampaignRepository) as campaign_repository:
            # Get campaign
            campaign = await campaign_repository.get_by_id(campaign_id)
            if not campaign:
                return False
            
            # Validate ownership
            if campaign.user_id != user_id:
                return False
            
            # Check if campaign can be paused
            if campaign.status != "active":
                return False
            
            # Update status to paused
            updated = await campaign_repository.update_campaign_status(
                campaign_id=campaign_id,
                status="paused"
            )
            
            return updated is not None
    
    async def cancel_campaign(self, campaign_id: str, user_id: str) -> bool:
        """
        Cancel a campaign.
        
        Args:
            campaign_id: Campaign ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if campaign was cancelled, False otherwise
        """
        # Use context manager for repository access
        async with get_repository_context(CampaignRepository) as campaign_repository:
            # Get campaign
            campaign = await campaign_repository.get_by_id(campaign_id)
            if not campaign:
                return False
            
            # Validate ownership
            if campaign.user_id != user_id:
                return False
            
            # Check if campaign can be cancelled
            if campaign.status in ["completed", "cancelled", "failed"]:
                return False
            
            # Update status to cancelled
            updated = await campaign_repository.update_campaign_status(
                campaign_id=campaign_id,
                status="cancelled",
                completed_at=datetime.now(timezone.utc)
            )
            
            return updated is not None
    
    async def _process_campaign(self, campaign_id: str) -> None:
        """
        Process a campaign in the background.
        
        Args:
            campaign_id: Campaign ID
        """
        if campaign_id in self._processing_campaigns:
            logger.warning(f"Campaign {campaign_id} is already being processed")
            return
        
        # Mark as processing
        self._processing_campaigns.add(campaign_id)
        
        try:
            # Check campaign status with context manager
            async with get_repository_context(CampaignRepository) as campaign_repository:
                campaign = await campaign_repository.get_by_id(campaign_id)
                if not campaign or campaign.status != "active":
                    return
            
            # Process in chunks until complete
            async with self._semaphore:
                await self._process_campaign_chunks(campaign_id)
                
        except Exception as e:
            logger.error(f"Error processing campaign {campaign_id}: {e}", exc_info=True)
            # Update campaign status to failed with a new context manager
            try:
                async with get_repository_context(CampaignRepository) as campaign_repository:
                    await campaign_repository.update_campaign_status(
                        campaign_id=campaign_id,
                        status="failed",
                        completed_at=datetime.now(timezone.utc)
                    )
            except Exception as update_error:
                logger.error(f"Failed to update campaign status: {update_error}")
        finally:
            # Remove from processing set
            self._processing_campaigns.remove(campaign_id)
    
    async def _process_campaign_chunks(self, campaign_id: str) -> None:
        """
        Process campaign messages in chunks.
        
        Args:
            campaign_id: Campaign ID
        """
        # Query pending messages in chunks
        offset = 0
        
        while True:
            # Check if campaign is still active with context manager
            campaign = None
            async with get_repository_context(CampaignRepository) as campaign_repository:
                campaign = await campaign_repository.get_by_id(campaign_id)
                if not campaign or campaign.status != "active":
                    logger.info(f"Campaign {campaign_id} is no longer active, stopping processing")
                    return
            
            # Get next chunk of messages with context manager
            messages = []
            total = 0
            async with get_repository_context(MessageRepository) as message_repository:
                messages, total = await message_repository.get_messages_for_campaign(
                    campaign_id=campaign_id,
                    status=MessageStatus.PENDING,
                    skip=offset,
                    limit=self._chunk_size
                )
            
            # If no more messages, campaign is complete
            if not messages:
                logger.info(f"No more pending messages for campaign {campaign_id}")
                async with get_repository_context(CampaignRepository) as campaign_repository:
                    await campaign_repository.update_campaign_status(
                        campaign_id=campaign_id,
                        status="completed",
                        completed_at=datetime.now(timezone.utc)
                    )
                return
            
            # Process this chunk
            await self._process_message_chunk(campaign_id, messages)
            
            # Update offset for next chunk
            offset += len(messages)
            
            # Small delay between chunks to avoid overloading
            await asyncio.sleep(0.5)
    
    async def _process_message_chunk(self, campaign_id: str, messages: List[Any]) -> None:
        """
        Process a chunk of messages with proper context management.
        
        Args:
            campaign_id: Campaign ID
            messages: List of message objects
        """
        # Process each message in the chunk
        success_count = 0
        fail_count = 0
        
        for message in messages:
            try:
                # Use SMS sender to send the message
                result = await self.sms_sender._send_to_gateway(
                    phone_number=message.phone_number,
                    message_text=message.message,
                    custom_id=message.custom_id or str(uuid.uuid4())
                )
                
                # Update message status with context manager
                async with get_repository_context(MessageRepository) as message_repository:
                    await message_repository.update_message_status(
                        message_id=message.id,
                        status=result.get("status", MessageStatus.PENDING),
                        event_type="campaign_process",
                        gateway_message_id=result.get("gateway_message_id"),
                        data=result
                    )
                
                success_count += 1
                
                # Add delay between messages to avoid overloading gateway
                await asyncio.sleep(settings.DELAY_BETWEEN_SMS)
                
            except Exception as e:
                logger.error(f"Error processing message {message.id}: {e}")
                
                # Update message status to failed with context manager
                try:
                    async with get_repository_context(MessageRepository) as message_repository:
                        await message_repository.update_message_status(
                            message_id=message.id,
                            status=MessageStatus.FAILED,
                            event_type="campaign_process_error",
                            reason=str(e),
                            data={"error": str(e)}
                        )
                except Exception as update_error:
                    logger.error(f"Failed to update message status: {update_error}")
                
                fail_count += 1
        
        # Update campaign stats with context manager
        if success_count > 0 or fail_count > 0:
            try:
                async with get_repository_context(CampaignRepository) as campaign_repository:
                    await campaign_repository.update_campaign_stats(
                        campaign_id=campaign_id,
                        increment_sent=success_count,
                        increment_failed=fail_count
                    )
            except Exception as update_error:
                logger.error(f"Failed to update campaign stats: {update_error}")
        
        logger.info(f"Processed chunk for campaign {campaign_id}: {success_count} sent, {fail_count} failed")


# Dependency injection function
async def get_campaign_processor():
    """
    Get campaign processor service instance.
    
    Uses the SMS sender with its dedicated context management but doesn't create
    long-lived repository instances. Each operation in the processor will
    create repositories within context managers as needed.
    """
    from app.services.event_bus.bus import get_event_bus
    from app.services.sms.sender import get_sms_sender
    
    sms_sender = await get_sms_sender()
    event_bus = get_event_bus()
    
    return CampaignProcessor(
        sms_sender=sms_sender,
        event_bus=event_bus
    )
</file>

<file path="README.md">
# Inboxerr Backend

API backend for SMS management and delivery.

## Features

- ✅ Send individual and batch SMS messages
- ✅ Track message delivery status
- ✅ Import contacts from CSV
- ✅ Scheduled message delivery
- ✅ Webhook integration for real-time updates
- ✅ User authentication and API key management
- ✅ Message templates
- ✅ Comprehensive retry handling
- ✅ Event-driven architecture

## Technology Stack

- **Framework**: FastAPI
- **Database**: PostgreSQL with SQLAlchemy (async)
- **Authentication**: JWT and API keys
- **Containerization**: Docker & Docker Compose
- **API Documentation**: OpenAPI/Swagger
- **Testing**: pytest
- **SMS Gateway Integration**: Android SMS Gateway

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Python 3.10+
- Android SMS Gateway credentials

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/inboxerr-backend.git
   cd inboxerr-backend
   ```

2. Copy the example environment file:
   ```bash
   cp .env.example .env
   ```
   
3. Update the `.env` file with your configuration:
   ```
   SMS_GATEWAY_URL=https://endpointnumber1.work.gd/api/3rdparty/v1
   SMS_GATEWAY_LOGIN=your_login
   SMS_GATEWAY_PASSWORD=your_password
   SECRET_KEY=your_secret_key
   ```

4. Start the application with Docker Compose:
   ```bash
   docker-compose up -d
   ```

5. Run database migrations:
   ```bash
   docker-compose exec api alembic upgrade head
   ```

6. Access the API at `http://localhost:8000/api/docs`

### Development Setup

For local development without Docker:

1. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   pip install -r requirements-dev.txt
   ```

3. Set up environment variables:
   ```bash
   export DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/inboxerr
   export SMS_GATEWAY_URL=https://endpointnumber1.work.gd/api/3rdparty/v1
   export SMS_GATEWAY_LOGIN=your_login
   export SMS_GATEWAY_PASSWORD=your_password
   ```

4. Run the application:
   ```bash
   uvicorn app.main:app --reload
   ```

## API Endpoints

### Authentication

- `POST /api/v1/auth/token` - Get access token
- `POST /api/v1/auth/register` - Register new user
- `GET /api/v1/auth/me` - Get current user info
- `POST /api/v1/auth/keys` - Create API key
- `GET /api/v1/auth/keys` - List API keys

### Messages

- `POST /api/v1/messages/send` - Send a single message
- `POST /api/v1/messages/batch` - Send batch of messages
- `POST /api/v1/messages/import` - Import contacts and send messages
- `GET /api/v1/messages/{message_id}` - Get message details
- `GET /api/v1/messages` - List messages
- `PUT /api/v1/messages/{message_id}/status` - Update message status
- `DELETE /api/v1/messages/{message_id}` - Delete message

### Webhooks

- `GET /api/v1/webhooks` - List webhooks
- `POST /api/v1/webhooks` - Register webhook
- `DELETE /api/v1/webhooks/{webhook_id}` - Delete webhook
- `GET /api/v1/webhooks/logs` - Get webhook delivery logs

## Project Structure

```
/inboxerr-backend/
│
├── /app/                      # Application package
│   ├── __init__.py            # Package initializer
│   ├── main.py                # FastAPI application entry point
│   │
│   ├── /api/                  # API endpoints and routing
│   │   ├── __init__.py
│   │   ├── router.py          # Main API router
│   │   ├── /v1/               # API version 1
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/     # API endpoints by resource
│   │   │   │   ├── __init__.py
│   │   │   │   ├── auth.py    # Authentication endpoints
│   │   │   │   ├── messages.py # SMS message endpoints
│   │   │   │   ├── webhooks.py # Webhook endpoints
│   │   │   │   └── metrics.py  # Metrics and reporting endpoints
│   │   │   └── dependencies.py # API-specific dependencies
│   │
│   ├── /core/                 # Core application components
│   │   ├── __init__.py
│   │   ├── config.py          # Application configuration
│   │   ├── security.py        # Security utilities (auth, encryption)
│   │   ├── events.py          # Event handlers for application lifecycle
│   │   └── exceptions.py      # Custom exception classes
│   │
│   ├── /db/                   # Database related code
│   │   ├── __init__.py
│   │   ├── base.py            # Base DB session setup
│   │   ├── session.py         # DB session management
│   │   └── repositories/      # Repository pattern implementations
│   │       ├── __init__.py
│   │       ├── base.py        # Base repository class
│   │       ├── messages.py    # Message repository
│   │       └── users.py       # User repository
│   │
│   ├── /models/               # Database models
│   │   ├── __init__.py
│   │   ├── base.py            # Base model class
│   │   ├── message.py         # SMS message model
│   │   ├── user.py            # User model
│   │   └── webhook.py         # Webhook model
│   │
│   ├── /schemas/              # Pydantic schemas for API
│   │   ├── __init__.py
│   │   ├── base.py            # Base schema
│   │   ├── message.py         # Message schemas
│   │   ├── user.py            # User schemas
│   │   ├── webhook.py         # Webhook schemas
│   │   └── metrics.py         # Metrics schemas
│   │
│   ├── /services/             # Business logic services
│   │   ├── __init__.py
│   │   ├── sms/               # SMS related services
│   │   │   ├── __init__.py
│   │   │   ├── sender.py      # SMS sender implementation
│   │   │   ├── validator.py   # Phone/message validation
│   │   │   └── gateway.py     # SMS gateway client
│   │   │
│   │   ├── event_bus/         # Event management
│   │   │   ├── __init__.py
│   │   │   ├── bus.py         # Event bus implementation
│   │   │   ├── events.py      # Event definitions
│   │   │   └── handlers/      # Event handlers
│   │   │       ├── __init__.py
│   │   │       ├── message_handlers.py
│   │   │       └── system_handlers.py
│   │   │
│   │   ├── webhooks/          # Webhook handling
│   │   │   ├── __init__.py
│   │   │   ├── handler.py     # Webhook processor
│   │   │   └── manager.py     # Webhook registration/management
│   │   │
│   │   └── metrics/           # Metrics collection
│   │       ├── __init__.py
│   │       └── collector.py   # Metrics collector
│   │
│   └── /utils/                # Utility functions and helpers
│       ├── __init__.py
│       ├── phone.py           # Phone number utilities
│       ├── logging.py         # Logging configuration
│       └── pagination.py      # Pagination utilities
│
├── /alembic/                  # Database migrations
│   ├── env.py                 # Alembic environment
│   ├── README                 # Alembic readme
│   ├── script.py.mako         # Migration script template
│   └── /versions/             # Migration scripts
│
├── /tests/                    # Test suite
│   ├── __init__.py
│   ├── conftest.py            # Test configuration and fixtures
│   ├── /unit/                 # Unit tests
│   │   ├── __init__.py
│   │   ├── /services/         # Tests for services
│   │   └── /api/              # Tests for API endpoints
│   └── /integration/          # Integration tests
│       ├── __init__.py
│       └── /api/              # API integration tests
│
├── /scripts/                  # Utility scripts
│   ├── seed_db.py             # Database seeding script
│   └── generate_keys.py       # Generate security keys
│
├── .env.example               # Example environment variables
├── .gitignore                 # Git ignore file
├── docker-compose.yml         # Docker Compose configuration
├── Dockerfile                 # Docker build configuration
├── pyproject.toml             # Python project metadata
├── requirements.txt           # Python dependencies
├── requirements-dev.txt       # Development dependencies
└── README.md                  # Project documentation
```

## Usage Examples

### Sending a Single SMS

```python
import requests
import json

url = "http://localhost:8000/api/v1/messages/send"
headers = {
    "Authorization": "Bearer YOUR_ACCESS_TOKEN",
    "Content-Type": "application/json"
}
data = {
    "phone_number": "+1234567890",
    "message": "Hello from Inboxerr!"
}

response = requests.post(url, headers=headers, data=json.dumps(data))
print(response.json())
```

### Importing Contacts from CSV

```python
import requests

url = "http://localhost:8000/api/v1/messages/import"
headers = {
    "Authorization": "Bearer YOUR_ACCESS_TOKEN"
}
files = {
    "file": open("contacts.csv", "rb")
}
data = {
    "message_template": "Hello {{name}}, this is a test message!"
}

response = requests.post(url, headers=headers, files=files, data=data)
print(response.json())
```

# Inboxerr API Updates

## Message Template System

The Inboxerr API now includes a robust Message Template System, allowing you to:

- Create reusable templates with variable placeholders
- Apply variables to templates and preview the results
- Send messages using templates with personalized data
- Manage templates (create, update, delete, list)

### Getting Started with Templates

1. **Create a new template**:
   ```bash
   curl -X POST "http://localhost:8000/api/v1/templates" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "name": "Welcome Template",
       "content": "Hello {{name}}, welcome to our service!",
       "description": "Welcome message for new users"
     }'
   ```

2. **Send a message using a template**:
   ```bash
   curl -X POST "http://localhost:8000/api/v1/templates/send" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "template_id": "YOUR_TEMPLATE_ID",
       "phone_number": "+1234567890",
       "variables": {
         "name": "John"
       }
     }'
   ```

See the full [Message Template System User Guide](path/to/message-template-system-user-guide.md) for more details.

## Database Management

We've added tools to simplify database migration and setup:

### Generating Migrations

To generate a new migration after changing your models:

```bash
python scripts/generate_migration.py "Description of your changes"
```

### Setting Up a Test Database

To set up a test database with sample data:

```bash
python scripts/setup_test_db.py
```

This will:
1. Create a test database if it doesn't exist
2. Run all migrations
3. Create a test user and sample templates

Test user credentials:
- Email: test@example.com
- Password: Test1234!

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="app/api/v1/endpoints/messages.py">
"""
API endpoints for SMS message management.
"""
import csv
import io
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, UploadFile, File, Query, Path, status
import logging


from app.api.v1.dependencies import get_current_user, get_rate_limiter, verify_api_key
from app.core.exceptions import ValidationError, SMSGatewayError, NotFoundError
from app.schemas.message import (
    MessageCreate,
    MessageResponse, 
    BatchMessageRequest, 
    BatchMessageResponse,
    MessageStatus,
    MessageStatusUpdate,
    GlobalBulkDeleteRequest,
    BulkDeleteResponse
)
from app.services.sms.sender import get_sms_sender
from app.schemas.user import User
from app.utils.pagination import PaginationParams, paginate_response
from app.utils.pagination import PaginatedResponse
from app.db.session import get_repository_context

router = APIRouter()
logger = logging.getLogger("inboxerr.endpoint")

# ===========================
# COLLECTION OPERATIONS
# ===========================

@router.get("/", response_model=PaginatedResponse[MessageResponse])
async def list_messages(
    pagination: PaginationParams = Depends(),
    status: Optional[str] = Query(None, description="Filter by message status"),
    phone_number: Optional[str] = Query(None, description="Filter by phone number"),
    from_date: Optional[str] = Query(None, description="Filter from date (ISO format)"),
    to_date: Optional[str] = Query(None, description="Filter to date (ISO format)"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    List messages with optional filtering.
    
    Returns a paginated list of messages for the current user.
    """
    try:
        filters = {
            "status": status,
            "phone_number": phone_number,
            "from_date": from_date,
            "to_date": to_date,
            "user_id": current_user.id
        }
        
        # Get messages with pagination
        messages, total = await sms_sender.list_messages(
            filters=filters,
            skip=pagination.skip,
            limit=pagination.limit
        )
        
        # Return paginated response
        return paginate_response(
            items=messages,
            total=total,
            pagination=pagination
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing messages: {str(e)}")

@router.post("/send", response_model=MessageResponse, status_code=202)
async def send_message(
    message: MessageCreate,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Send a single SMS message.
    
    - **phone_number**: Recipient phone number in E.164 format (e.g., +1234567890)
    - **message**: Content of the SMS message
    - **scheduled_at**: Optional timestamp to schedule the message for future delivery
    """
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "send_message")
    
    try:
        # Send message asynchronously
        result = await sms_sender.send_message(
            phone_number=message.phone_number,
            message_text=message.message,
            user_id=current_user.id,
            scheduled_at=message.scheduled_at,
            custom_id=message.custom_id,
        )
        
        return result
        
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except SMSGatewayError as e:
        raise HTTPException(status_code=502, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error sending message: {str(e)}")


@router.post("/batch", response_model=BatchMessageResponse, status_code=202)
async def send_batch(
    batch: BatchMessageRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Send a batch of SMS messages.
    
    - **messages**: List of messages to send
    - **options**: Optional batch processing options
    """
    # Check rate limits - higher limit for batch operations
    await rate_limiter.check_rate_limit(current_user.id, "send_batch")
    
    if not batch.messages:
        raise ValidationError(message="Batch contains no messages")
    
    try:
        # Process batch asynchronously
        result = await sms_sender.send_batch(
            messages=batch.messages,
            user_id=current_user.id,
            options=batch.options,
        )
        
        return result
        
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except SMSGatewayError as e:
        raise HTTPException(status_code=502, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing batch: {str(e)}")


@router.post("/import", status_code=202)
async def import_messages(
    file: UploadFile = File(...),
    message_template: str = Query(..., description="Message template to send"),
    delimiter: str = Query(",", description="CSV delimiter"),
    has_header: bool = Query(True, description="Whether CSV has a header row"),
    phone_column: str = Query("phone", description="Column name containing phone numbers"),
    background_tasks: BackgroundTasks = None,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Import phone numbers from CSV and send messages.
    
    - **file**: CSV file with phone numbers
    - **message_template**: Template for the message to send
    - **delimiter**: CSV delimiter character
    - **has_header**: Whether the CSV has a header row
    - **phone_column**: Column name containing phone numbers (if has_header=True)
    """
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "import_messages")
    
    try:
        # Read CSV file
        contents = await file.read()
        csv_file = io.StringIO(contents.decode('utf-8'))
        
        # Parse CSV
        csv_reader = csv.reader(csv_file, delimiter=delimiter)
        
        # Skip header if present
        if has_header:
            header = next(csv_reader)
            try:
                phone_index = header.index(phone_column)
            except ValueError:
                raise ValidationError(
                    message=f"Column '{phone_column}' not found in CSV header",
                    details={"available_columns": header}
                )
        else:
            phone_index = 0  # Assume first column has phone numbers
        
        # Extract phone numbers
        phone_numbers = []
        for row in csv_reader:
            if row and len(row) > phone_index:
                phone = row[phone_index].strip()
                if phone:
                    phone_numbers.append(phone)
        
        if not phone_numbers:
            raise ValidationError(message="No valid phone numbers found in CSV")
        
        # Process in background
        task_id = await sms_sender.schedule_batch_from_numbers(
            phone_numbers=phone_numbers,
            message_text=message_template,
            user_id=current_user.id,
        )
        
        return {
            "status": "accepted",
            "message": f"Processing {len(phone_numbers)} messages",
            "task_id": task_id,
        }
        
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error importing messages: {str(e)}")


@router.delete("/bulk", response_model=BulkDeleteResponse)
async def bulk_delete_messages(
    request: GlobalBulkDeleteRequest,
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Global bulk delete messages by message IDs with event safety.
    
    This endpoint efficiently deletes multiple messages by their specific IDs across
    campaigns or for orphaned message cleanup. Enhanced with delivery event safety
    checking for edge cases, power user operations, and system maintenance tasks.
    
    **Event Safety Features:**
    - Pre-deletion check for delivery events
    - Requires explicit confirmation to delete tracking data
    - Clear warnings about analytics data loss
    - Two-phase deletion (events first, then messages)
    
    **Safety Features:**
    - User authorization (can only delete own messages)
    - Smaller batch limits (1K vs 10K for campaign-scoped)
    - Optional campaign context validation
    - Detailed failure reporting for partial operations
    - Comprehensive audit logging
    
    **Use Cases:**
    - Cross-campaign message cleanup by power users
    - Frontend multi-select bulk operations
    - Orphaned message removal during maintenance
    - Compliance-driven deletion by specific message IDs
    - System administration tasks
    
    **Performance:**
    - Handles up to 1K message deletions efficiently
    - Uses optimized IN clause with PostgreSQL
    - Smaller batches for safety vs campaign operations
    - Batched processing prevents server overload
    
    Args:
        request: Global bulk delete request with message IDs, confirmation, and force options
        current_user: Authenticated user (injected by dependency)
        rate_limiter: Rate limiting for bulk operations (injected by dependency)
    
    Returns:
        BulkDeleteResponse: Detailed results including event safety information
        
    Raises:
        HTTPException 400: Invalid request or missing confirmation
        HTTPException 403: User not authorized for specified messages
        HTTPException 429: Rate limit exceeded
        HTTPException 500: Database or internal server error
    """
    import time
    
    # Check rate limits for global bulk operations
    await rate_limiter.check_rate_limit(current_user.id, "bulk_delete_global")
    
    start_time = time.time()
    
    try:
        # Use repository context for proper connection management
        from app.db.repositories.messages import MessageRepository
        
        # Perform global bulk deletion with event safety
        async with get_repository_context(MessageRepository) as message_repo:
            # Execute bulk deletion with enhanced safety
            deleted_count, failed_message_ids, metadata = await message_repo.bulk_delete_messages(
                message_ids=request.message_ids,
                user_id=current_user.id,
                campaign_id=request.campaign_id,
                force_delete=request.force_delete
            )
            
            # Calculate execution time
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # Build applied filters for audit trail
            filters_applied = {
                "message_count": len(request.message_ids),
                "unique_messages": len(set(request.message_ids)),
                "force_delete": request.force_delete
            }
            if request.campaign_id:
                filters_applied["campaign_context"] = request.campaign_id
            
            # Build detailed error messages for failed deletions
            error_messages = []
            if failed_message_ids:
                if metadata.get("requires_confirmation"):
                    # Failed due to event safety check
                    error_messages = metadata.get("safety_warnings", [])
                else:
                    # Failed due to other reasons (not found, not owned, etc.)
                    error_messages = [
                        f"Failed to delete message: {msg_id} (not found or not owned by user)" 
                        for msg_id in failed_message_ids
                    ]
            
            # Build response with enhanced metadata
            response = BulkDeleteResponse(
                deleted_count=deleted_count,
                campaign_id=request.campaign_id,
                failed_count=len(failed_message_ids),
                errors=error_messages,
                operation_type="global",
                filters_applied=filters_applied,
                execution_time_ms=execution_time_ms,
                requires_confirmation=metadata.get("requires_confirmation", False),
                events_count=metadata.get("events_count"),
                events_deleted=metadata.get("events_deleted", 0),
                safety_warnings=metadata.get("safety_warnings", []),
                batch_info=None  # Global operations use smaller fixed batches
            )
            
            # Log successful operation for audit
            logger.info(
                f"User {current_user.id} performed global bulk delete of {deleted_count} messages "
                f"in {execution_time_ms}ms. Failed: {len(failed_message_ids)}. "
                f"Campaign context: {request.campaign_id}. Events deleted: {metadata.get('events_deleted', 0)}. "
                f"Force delete: {request.force_delete}"
            )
            
            return response
            
    except Exception as e:
        # Log error and return generic error message
        logger.error(f"Error in global bulk_delete_messages: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Error performing global bulk deletion: {str(e)}"
        )

# ===========================
# NESTED RESOURCES SECTION
# ===========================

@router.get("/tasks/{task_id}", response_model=dict)
async def get_task_status(
    task_id: str = Path(..., description="Task ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Check the status of a background task.
    
    Used for tracking progress of batch operations and imports.
    """
    try:
        task_status = await sms_sender.get_task_status(task_id, user_id=current_user.id)
        if not task_status:
            raise NotFoundError(message=f"Task {task_id} not found")
        
        return task_status
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving task status: {str(e)}")
    

# ===========================
# INDIVIDUAL RESOURCES (ALWAYS LAST)
# ===========================


@router.get("/{message_id}", response_model=MessageResponse)
async def get_message(
    message_id: str = Path(..., description="Message ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Get details of a specific message.
    """
    try:
        message = await sms_sender.get_message(message_id, user_id=current_user.id)
        if not message:
            raise NotFoundError(message=f"Message {message_id} not found")
        return message
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving message: {str(e)}")



@router.put("/{message_id}/status", response_model=MessageResponse)
async def update_message_status(
    status_update: MessageStatusUpdate,
    message_id: str = Path(..., description="Message ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Update the status of a message.
    
    This is primarily for administrative purposes or handling external status updates.
    """
    try:
        # Verify the user has permission to update this message
        message = await sms_sender.get_message(message_id, user_id=current_user.id)
        if not message:
            raise NotFoundError(message=f"Message {message_id} not found")
        
        # Update the status
        updated_message = await sms_sender.update_message_status(
            message_id=message_id,
            status=status_update.status,
            reason=status_update.reason,
            user_id=current_user.id
        )
        
        return updated_message
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating message status: {str(e)}")


@router.delete("/{message_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_message(
    message_id: str = Path(..., description="Message ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Delete a message.
    
    This will only remove it from the database, but cannot recall messages already sent.
    """
    try:
        # Verify the message exists and belongs to the user
        message = await sms_sender.get_message(message_id, user_id=current_user.id)
        if not message:
            raise NotFoundError(message=f"Message {message_id} not found")
        
        # Delete the message
        success = await sms_sender.delete_message(message_id, user_id=current_user.id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to delete message")
        
        return None  # No content response
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting message: {str(e)}")
</file>

<file path="app/api/v1/endpoints/metrics.py">
"""
API endpoints for metrics and reporting.
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Dict, Any, Optional

from app.api.v1.dependencies import get_current_user
from app.schemas.user import User
from app.schemas.metrics import (
    DashboardMetricsResponse, 
    UsageMetricsResponse,
    SystemMetricsResponse
)
from app.services.metrics.collector import get_user_metrics, get_system_metrics

router = APIRouter()


@router.get("/", response_model=SystemMetricsResponse)
async def get_metrics(
    current_user: User = Depends(get_current_user)
):
    """
    Get system metrics and statistics.
    Admin users get system-wide metrics, regular users get their own metrics.
    """
    if current_user.role == "admin":
        # Admins get system-wide metrics
        return await get_system_metrics()
    else:
        # Regular users get their dashboard metrics instead
        user_metrics = await get_user_metrics(current_user.id)
        
        # Transform user metrics to system format for consistency
        return {
            "messages": {
                "total": user_metrics["summary"]["messages"]["sent"],
                "sent": user_metrics["summary"]["messages"]["sent"],
                "delivered": user_metrics["summary"]["messages"]["delivered"],
                "failed": user_metrics["summary"]["messages"]["failed"],
                "last_24h": 0  # User-specific doesn't have this breakdown
            },
            "users": {
                "total": 1,  # Just the current user
                "active": 1,
                "new_today": 0
            },
            "campaigns": {
                "total": user_metrics["summary"]["campaigns"]["created"],
                "active": user_metrics["summary"]["campaigns"]["active"],
                "completed_today": 0
            }
        }


@router.get("/usage", response_model=UsageMetricsResponse)
async def get_usage_metrics(
    current_user: User = Depends(get_current_user)
):
    """
    Get usage metrics for the current user.
    """
    user_metrics = await get_user_metrics(current_user.id)
    
    # Format for the usage endpoint
    return {
        "message_count": user_metrics["summary"]["messages"]["sent"],
        "delivery_rate": user_metrics["summary"]["messages"]["delivery_rate"],
        "quota": {
            "used": user_metrics["summary"]["quota"]["used"],
            "total": user_metrics["summary"]["quota"]["total"],
            "percent": user_metrics["summary"]["quota"]["percent"]
        }
    }


@router.get("/dashboard", response_model=DashboardMetricsResponse)
async def get_dashboard_metrics(
    period: str = Query("week", description="Time period: day, week, month, year"),
    current_user: User = Depends(get_current_user)
):
    """
    Get metrics formatted for dashboard display.
    """
    metrics = await get_user_metrics(current_user.id, period=period)
    
    # Return data already formatted for dashboard
    return metrics
</file>

<file path="app/models/campaign.py">
# app/models/campaign.py
from datetime import datetime, timezone
from typing import List, Optional

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text
from sqlalchemy.orm import relationship

from app.models.base import Base


class Campaign(Base):
    """Campaign model for bulk SMS messaging."""
    
    # Basic campaign information
    name = Column(String, nullable=False, index=True)
    description = Column(Text, nullable=True)
    
    # Campaign status
    status = Column(String, nullable=False, default="draft", index=True)  # draft, active, paused, completed, cancelled, failed
    
    # Campaign statistics
    total_messages = Column(Integer, default=0, nullable=False)
    sent_count = Column(Integer, default=0, nullable=False)
    delivered_count = Column(Integer, default=0, nullable=False)
    failed_count = Column(Integer, default=0, nullable=False)
    
    # Campaign configuration
    scheduled_start_at = Column(DateTime(timezone=True), nullable=True, index=True)
    scheduled_end_at = Column(DateTime(timezone=True), nullable=True)
    started_at = Column(DateTime(timezone=True), nullable=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Campaign settings
    settings = Column(JSON, nullable=True, default=dict)  # Store campaign-specific settings
    
    # Ownership
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    
    # Relationships
    user = relationship("User")
    # In Campaign model:
    messages = relationship("Message", back_populates="campaign", cascade="all, delete-orphan")
    
    # Helper properties
    @property
    def progress_percentage(self) -> float:
        """Calculate the campaign progress percentage."""
        if self.total_messages == 0:
            return 0
        return round((self.sent_count / self.total_messages) * 100, 2)
    
    @property
    def delivery_success_rate(self) -> float:
        """Calculate the delivery success rate."""
        if self.sent_count == 0:
            return 0
        return round((self.delivered_count / self.sent_count) * 100, 2)
</file>

<file path="app/models/message.py">
"""
Database models for SMS messages.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text
from sqlalchemy.orm import relationship

from app.models.base import Base


class Message(Base):
    """SMS message model."""
    
    # Core message data
    custom_id = Column(String, unique=True, index=True, nullable=True)
    phone_number = Column(String, nullable=False, index=True)
    message = Column(Text, nullable=False)
    status = Column(String, nullable=False, default="pending", index=True)

    # Campaign relationship
    campaign_id = Column(String, ForeignKey("campaign.id"), nullable=True, index=True)
    campaign = relationship("Campaign", back_populates="messages")
    
    # Timestamps for status tracking
    scheduled_at = Column(DateTime(timezone=True), nullable=True, index=True)
    sent_at = Column(DateTime(timezone=True), nullable=True)
    delivered_at = Column(DateTime(timezone=True), nullable=True)
    failed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Additional data
    reason = Column(String, nullable=True)
    gateway_message_id = Column(String, nullable=True, index=True)
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    meta_data = Column(JSON, nullable=True)  # Changed from 'metadata' to 'meta_data'
    
    # SMS parts tracking
    parts_count = Column(Integer, default=1, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="messages")
    events = relationship("MessageEvent", back_populates="message", cascade="all, delete-orphan")
    batch_id = Column(String, ForeignKey("messagebatch.id"), nullable=True, index=True)
    batch = relationship("MessageBatch", back_populates="messages")


class MessageEvent(Base):
    """Model for tracking message events and status changes."""
    
    message_id = Column(String, ForeignKey("message.id"), nullable=False, index=True)
    event_type = Column(String, nullable=False, index=True)
    status = Column(String, nullable=False)
    data = Column(JSON, nullable=True)
    
    # Relationships
    message = relationship("Message", back_populates="events")


class MessageBatch(Base):
    """Model for tracking message batches."""
    
    name = Column(String, nullable=True)
    total = Column(Integer, default=0, nullable=False)
    processed = Column(Integer, default=0, nullable=False)
    successful = Column(Integer, default=0, nullable=False)
    failed = Column(Integer, default=0, nullable=False)
    status = Column(String, nullable=False, default="pending", index=True)
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Relationships
    messages = relationship("Message", back_populates="batch")
    user = relationship("User")


class MessageTemplate(Base):
    """Model for storing reusable message templates."""
    
    name = Column(String, nullable=False, index=True)
    content = Column(Text, nullable=False)
    description = Column(String, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    variables = Column(JSON, nullable=True)  # Define expected variables in the template
    
    # Relationships
    user = relationship("User")
</file>

<file path="app/models/user.py">
"""
Database models for user management.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Boolean, Column, String, DateTime, JSON, ForeignKey
from sqlalchemy.orm import relationship

from app.models.base import Base


class User(Base):
    """User model for authentication and authorization."""
    
    email = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    full_name = Column(String, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    role = Column(String, default="user", nullable=False)
    
    # Relationships
    api_keys = relationship("APIKey", back_populates="user", cascade="all, delete-orphan")
    messages = relationship("Message", back_populates="user", cascade="all, delete-orphan")
    metrics = relationship("UserMetrics", back_populates="user", cascade="all, delete-orphan")



class APIKey(Base):
    """API key model for API authentication."""
    
    key = Column(String, unique=True, index=True, nullable=False)
    name = Column(String, nullable=False)
    user_id = Column(String, ForeignKey("user.id"), nullable=False)
    expires_at = Column(DateTime(timezone=True), nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    last_used_at = Column(DateTime(timezone=True), nullable=True)
    permissions = Column(JSON, default=list, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="api_keys")
</file>

<file path="app/schemas/campaign.py">
# app/schemas/campaign.py
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from uuid import UUID
from enum import Enum

from pydantic import BaseModel, Field, validator



class CampaignStatus(str, Enum):
    """
    Campaign status enum.
    
    Defines all possible states a campaign can be in during its lifecycle.
    Uses string enum to ensure JSON serialization compatibility and type safety.
    """
    DRAFT = "draft"
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"


class CampaignBase(BaseModel):
    """Base schema for campaign data."""
    name: str = Field(..., description="Campaign name")
    description: Optional[str] = Field(None, description="Campaign description")
    scheduled_start_at: Optional[datetime] = Field(None, description="Scheduled start time")
    scheduled_end_at: Optional[datetime] = Field(None, description="Scheduled end time")
    settings: Optional[Dict[str, Any]] = Field(default={}, description="Campaign settings")


class CampaignCreate(CampaignBase):
    """Schema for creating a new campaign."""
    pass


class CampaignCreateFromCSV(BaseModel):
    """Schema for creating a campaign from CSV file."""
    name: str = Field(..., description="Campaign name")
    description: Optional[str] = Field(None, description="Campaign description")
    message_template: str = Field(..., description="Message template to send")
    scheduled_start_at: Optional[datetime] = Field(None, description="Scheduled start time")
    scheduled_end_at: Optional[datetime] = Field(None, description="Scheduled end time")
    settings: Optional[Dict[str, Any]] = Field(default={}, description="Campaign settings")


class CampaignUpdate(BaseModel):
    """Schema for updating a campaign."""
    name: Optional[str] = Field(None, description="Campaign name")
    description: Optional[str] = Field(None, description="Campaign description")
    scheduled_start_at: Optional[datetime] = Field(None, description="Scheduled start time")
    scheduled_end_at: Optional[datetime] = Field(None, description="Scheduled end time")
    settings: Optional[Dict[str, Any]] = Field(None, description="Campaign settings")


class CampaignResponse(CampaignBase):
    """Schema for campaign response."""
    id: str = Field(..., description="Campaign ID")
    status: CampaignStatus = Field(..., description="Campaign status")
    total_messages: int = Field(..., description="Total number of messages")
    sent_count: int = Field(..., description="Number of sent messages")
    delivered_count: int = Field(..., description="Number of delivered messages")
    failed_count: int = Field(..., description="Number of failed messages")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    started_at: Optional[datetime] = Field(None, description="Start timestamp")
    completed_at: Optional[datetime] = Field(None, description="Completion timestamp")
    user_id: str = Field(..., description="User ID")
    
    # Add computed fields
    progress_percentage: float = Field(0, description="Progress percentage")
    delivery_success_rate: float = Field(0, description="Delivery success rate")
    
    class Config:
        """Pydantic config."""
        from_attributes = True
</file>

<file path="app/services/event_bus/events.py">
"""
Event type definitions for the event bus.
"""
from enum import Enum, auto
from typing import Dict, Any, Optional
from datetime import datetime, timezone


class EventType(str, Enum):
    """Event types for the event bus."""
    
    # System events
    SYSTEM_STARTUP = "system:startup"
    SYSTEM_SHUTDOWN = "system:shutdown"
    
    # Message events
    MESSAGE_CREATED = "message:created"
    MESSAGE_UPDATED = "message:updated"
    MESSAGE_SENT = "message:sent"
    MESSAGE_DELIVERED = "message:delivered"
    MESSAGE_FAILED = "message:failed"
    MESSAGE_SCHEDULED = "message:scheduled"
    MESSAGE_RETRIED = "message:retried"
    MESSAGE_RETRY_FAILED = "message:retry_failed"
    
    # Batch events
    BATCH_CREATED = "batch:created"
    BATCH_UPDATED = "batch:updated"
    BATCH_COMPLETED = "batch:completed"

    # Campaign events
    CAMPAIGN_CREATED = "campaign:created"
    CAMPAIGN_UPDATED = "campaign:updated"
    CAMPAIGN_STARTED = "campaign:started"
    CAMPAIGN_PAUSED = "campaign:paused"
    CAMPAIGN_COMPLETED = "campaign:completed"
    CAMPAIGN_CANCELLED = "campaign:cancelled"
    CAMPAIGN_FAILED = "campaign:failed"
    
    # SMS Gateway events
    SMS_RECEIVED = "sms:received"
    SMS_SENT = "sms:sent"
    SMS_DELIVERED = "sms:delivered"
    SMS_FAILED = "sms:failed"
    
    # Webhook events
    WEBHOOK_RECEIVED = "webhook:received"
    WEBHOOK_PROCESSED = "webhook:processed"
    
    # User events
    USER_CREATED = "user:created"
    USER_UPDATED = "user:updated"
    USER_DELETED = "user:deleted"
    
    # API events
    API_REQUEST = "api:request"
    API_RESPONSE = "api:response"
    API_ERROR = "api:error"

    # Template events
    TEMPLATE_CREATED = "template:created"
    TEMPLATE_UPDATED = "template:updated"
    TEMPLATE_USED = "template:used"


class Event:
    """
    Base event class.
    
    Contains common event data and helper methods.
    """
    
    def __init__(
        self,
        event_type: EventType,
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None
    ):
        """
        Initialize event.
        
        Args:
            event_type: Event type
            data: Event data
            timestamp: Event timestamp
        """
        self.event_type = event_type
        self.data = data
        self.timestamp = timestamp or datetime.now(timezone.utc)
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert event to dictionary.
        
        Returns:
            Dict: Event data
        """
        return {
            "event_type": self.event_type,
            "data": self.data,
            "timestamp": self.timestamp.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Event":
        """
        Create event from dictionary.
        
        Args:
            data: Event data
            
        Returns:
            Event: Event instance
        """
        event_type = data.get("event_type")
        if isinstance(event_type, str):
            event_type = EventType(event_type)
        
        timestamp = data.get("timestamp")
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp)
        
        return cls(
            event_type=event_type,
            data=data.get("data", {}),
            timestamp=timestamp
        )


class MessageEvent(Event):
    """
    Message event class.
    
    Contains message-specific event data.
    """
    
    def __init__(
        self,
        event_type: EventType,
        message_id: str,
        user_id: str,
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None
    ):
        """
        Initialize message event.
        
        Args:
            event_type: Event type
            message_id: Message ID
            user_id: User ID
            data: Event data
            timestamp: Event timestamp
        """
        # Add message ID and user ID to data
        data = data.copy()
        data["message_id"] = message_id
        data["user_id"] = user_id
        
        super().__init__(event_type, data, timestamp)
    
    @property
    def message_id(self) -> str:
        """Get message ID."""
        return self.data.get("message_id", "")
    
    @property
    def user_id(self) -> str:
        """Get user ID."""
        return self.data.get("user_id", "")


class WebhookEvent(Event):
    """
    Webhook event class.
    
    Contains webhook-specific event data.
    """
    
    def __init__(
        self,
        event_type: EventType,
        webhook_id: str,
        payload: Dict[str, Any],
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None
    ):
        """
        Initialize webhook event.
        
        Args:
            event_type: Event type
            webhook_id: Webhook ID
            payload: Webhook payload
            data: Event data
            timestamp: Event timestamp
        """
        # Add webhook ID and payload to data
        data = data.copy()
        data["webhook_id"] = webhook_id
        data["payload"] = payload
        
        super().__init__(event_type, data, timestamp)
    
    @property
    def webhook_id(self) -> str:
        """Get webhook ID."""
        return self.data.get("webhook_id", "")
    
    @property
    def payload(self) -> Dict[str, Any]:
        """Get webhook payload."""
        return self.data.get("payload", {})
</file>

<file path="app/services/metrics/collector.py">
# app/services/metrics/collector.py
"""
Metrics collection service.
"""
import asyncio
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime, date, timezone, timedelta

from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.metrics")

async def initialize_metrics() -> None:
    """Initialize metrics collector."""
    logger.info("Initializing metrics collector")
    
    # Subscribe to events
    event_bus = get_event_bus()
    
    # Message events
    await event_bus.subscribe(
        EventType.MESSAGE_CREATED,
        _handle_message_created,
        "metrics.message_created"
    )
    
    await event_bus.subscribe(
        EventType.MESSAGE_SENT,
        _handle_message_sent,
        "metrics.message_sent"
    )
    
    await event_bus.subscribe(
        EventType.MESSAGE_DELIVERED,
        _handle_message_delivered,
        "metrics.message_delivered"
    )
    
    await event_bus.subscribe(
        EventType.MESSAGE_FAILED,
        _handle_message_failed,
        "metrics.message_failed"
    )
    
    # Campaign events
    await event_bus.subscribe(
        EventType.CAMPAIGN_CREATED,
        _handle_campaign_created,
        "metrics.campaign_created"
    )
    
    await event_bus.subscribe(
        EventType.CAMPAIGN_COMPLETED,
        _handle_campaign_completed,
        "metrics.campaign_completed"
    )
    
    # Template events
    await event_bus.subscribe(
        EventType.TEMPLATE_CREATED,
        _handle_template_created,
        "metrics.template_created"
    )
    
    await event_bus.subscribe(
        EventType.TEMPLATE_USED,
        _handle_template_used,
        "metrics.template_used"
    )
    
    logger.info("Metrics collector initialized")

# Event handlers
async def _handle_message_created(data: Dict[str, Any]) -> None:
    """Handle message created event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_scheduled",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for message_created: {e}")

async def _handle_message_sent(data: Dict[str, Any]) -> None:
    """Handle message sent event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_sent",
                increment=1
            )
            
            # Decrement scheduled count if it was scheduled
            scheduled = data.get("scheduled", False)
            if scheduled:
                await metrics_repo.increment_metric(
                    user_id=user_id,
                    date=today,
                    metric_name="messages_scheduled",
                    increment=-1
                )
    except Exception as e:
        logger.error(f"Error updating metrics for message_sent: {e}")

async def _handle_message_delivered(data: Dict[str, Any]) -> None:
    """Handle message delivered event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_delivered",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for message_delivered: {e}")

async def _handle_message_failed(data: Dict[str, Any]) -> None:
    """Handle message failed event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_failed",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for message_failed: {e}")

async def _handle_campaign_created(data: Dict[str, Any]) -> None:
    """Handle campaign created event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_created",
                increment=1
            )
            
            # Increment active campaigns too
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_active",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for campaign_created: {e}")

async def _handle_campaign_completed(data: Dict[str, Any]) -> None:
    """Handle campaign completed event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_completed",
                increment=1
            )
            
            # Decrement active campaigns
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_active",
                increment=-1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for campaign_completed: {e}")

async def _handle_template_created(data: Dict[str, Any]) -> None:
    """Handle template created event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="templates_created",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for template_created: {e}")

async def _handle_template_used(data: Dict[str, Any]) -> None:
    """Handle template used event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="templates_used",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for template_used: {e}")

async def get_user_metrics(
    user_id: str,
    period: str = "week"
) -> Dict[str, Any]:
    """
    Get metrics for a specific user.
    
    Args:
        user_id: User ID
        period: Time period ("day", "week", "month", "year")
        
    Returns:
        Dict[str, Any]: User metrics
    """
    from app.db.repositories.metrics import MetricsRepository
    
    # Calculate date range based on period
    end_date = datetime.now(timezone.utc).date()
    
    if period == "day":
        start_date = end_date
    elif period == "week":
        start_date = end_date - timedelta(days=7)
    elif period == "month":
        start_date = end_date - timedelta(days=30)
    elif period == "year":
        start_date = end_date - timedelta(days=365)
    else:
        # Default to week
        start_date = end_date - timedelta(days=7)
    
    # Get metrics
    async with get_repository_context(MetricsRepository) as metrics_repo:
        # Get summary metrics
        summary = await metrics_repo.get_summary_metrics(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
        
        # Get daily metrics for charting
        metrics_list = await metrics_repo.get_metrics_range(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
        
        # Format for response
        daily_data = []
        for metric in metrics_list:
            daily_data.append({
                "date": metric.date.isoformat(),
                "sent": metric.messages_sent,
                "delivered": metric.messages_delivered,
                "failed": metric.messages_failed
            })
        
        # Combine data
        result = {
            "summary": summary,
            "daily_data": daily_data,
            "period": period
        }
        
        return result

async def get_system_metrics() -> Dict[str, Any]:
    """
    Get system-wide metrics (for admin).
    
    Returns:
        Dict[str, Any]: System metrics formatted for SystemMetricsResponse
    """
    # For system metrics, we'll query the database directly
    from app.db.repositories.messages import MessageRepository
    from app.db.repositories.users import UserRepository
    from app.db.repositories.campaigns import CampaignRepository
    
    system_metrics = {
        "messages": {},
        "users": {},
        "campaigns": {}
    }
    
    # Query message stats
    async with get_repository_context(MessageRepository) as message_repo:
        from sqlalchemy import func, select
        from app.models.message import Message
        
        # Total messages
        total_query = select(func.count(Message.id))
        result = await message_repo.session.execute(total_query)
        total_messages = result.scalar_one_or_none() or 0
        
        # Messages by status
        from app.schemas.message import MessageStatus
        status_counts = {}
        for status in [MessageStatus.SENT, MessageStatus.DELIVERED, MessageStatus.FAILED]:
            status_query = select(func.count(Message.id)).where(Message.status == status)
            result = await message_repo.session.execute(status_query)
            status_counts[status] = result.scalar_one_or_none() or 0
        
        # Last 24 hours
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        recent_query = select(func.count(Message.id)).where(Message.created_at >= yesterday)
        result = await message_repo.session.execute(recent_query)
        recent_messages = result.scalar_one_or_none() or 0
        
        system_metrics["messages"] = {
            "total": total_messages,
            "sent": status_counts.get(MessageStatus.SENT, 0),
            "delivered": status_counts.get(MessageStatus.DELIVERED, 0),
            "failed": status_counts.get(MessageStatus.FAILED, 0),
            "last_24h": recent_messages
        }
    
    # Query user stats
    async with get_repository_context(UserRepository) as user_repo:
        from app.models.user import User
        
        # Total users
        total_query = select(func.count(User.id))
        result = await user_repo.session.execute(total_query)
        total_users = result.scalar_one_or_none() or 0
        
        # Active users
        active_query = select(func.count(User.id)).where(User.is_active == True)
        result = await user_repo.session.execute(active_query)
        active_users = result.scalar_one_or_none() or 0
        
        # New users today
        today_start = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
        new_today_query = select(func.count(User.id)).where(User.created_at >= today_start)
        result = await user_repo.session.execute(new_today_query)
        new_today = result.scalar_one_or_none() or 0
        
        system_metrics["users"] = {
            "total": total_users,
            "active": active_users,
            "new_today": new_today
        }
    
    # Query campaign stats
    async with get_repository_context(CampaignRepository) as campaign_repo:
        from app.models.campaign import Campaign
        
        # Total campaigns
        total_query = select(func.count(Campaign.id))
        result = await campaign_repo.session.execute(total_query)
        total_campaigns = result.scalar_one_or_none() or 0
        
        # Active campaigns
        active_query = select(func.count(Campaign.id)).where(Campaign.status == "active")
        result = await campaign_repo.session.execute(active_query)
        active_campaigns = result.scalar_one_or_none() or 0
        
        # Campaigns completed today
        today_start = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
        completed_today_query = select(func.count(Campaign.id)).where(
            Campaign.completed_at >= today_start
        )
        result = await campaign_repo.session.execute(completed_today_query)
        completed_today = result.scalar_one_or_none() or 0
        
        system_metrics["campaigns"] = {
            "total": total_campaigns,
            "active": active_campaigns,
            "completed_today": completed_today
        }
    
    return system_metrics

async def schedule_metrics_update() -> None:
    """
    Schedule regular metrics updates.
    This function is meant to be run as a background task.
    """
    from app.db.repositories.metrics import MetricsRepository
    
    while True:
        try:
            # Calculate time until the next run (midnight UTC)
            now = datetime.now(timezone.utc)
            tomorrow = (now + timedelta(days=1)).replace(
                hour=0, minute=0, second=0, microsecond=0
            )
            seconds_until_midnight = (tomorrow - now).total_seconds()
            
            # Sleep until midnight
            logger.info(f"Metrics update scheduled for {tomorrow.isoformat()}")
            await asyncio.sleep(seconds_until_midnight)
            
            # Run the update for yesterday
            yesterday = now.date() - timedelta(days=1)
            logger.info(f"Running scheduled metrics update for {yesterday.isoformat()}")
            
            async with get_repository_context(MetricsRepository) as metrics_repo:
                updated_count = await metrics_repo.update_daily_metrics(day=yesterday)
                logger.info(f"Updated metrics for {updated_count} users")
                
        except Exception as e:
            logger.error(f"Error in scheduled metrics update: {e}")
            # Sleep for a while before retrying
            await asyncio.sleep(3600)  # 1 hour
</file>

<file path="app/services/sms/retry_engine.py">
# app/services/sms/retry_engine.py
import asyncio
import logging
import uuid
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional

from app.core.config import settings
from app.db.repositories.messages import MessageRepository
from app.schemas.message import MessageStatus
from app.services.event_bus.events import EventType
from app.services.event_bus.bus import get_event_bus
from app.core.exceptions import SMSGatewayError, RetryableError
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.retry")

class RetryEngine:
    """
    Service for retrying failed messages.
    
    Periodically checks for failed messages and attempts to resend them.
    Uses proper context management for database connections to prevent leaks.
    """
    
    def __init__(self, event_bus: Any, sms_sender: Any):
        """
        Initialize retry engine with required dependencies.
        
        Args:
            event_bus: Event bus for publishing events
            sms_sender: SMS sender service for resending messages
        """
        self.event_bus = event_bus
        self.sms_sender = sms_sender
        self._running = False
        self._semaphore = asyncio.Semaphore(5)  # Limit concurrent retries
        
    async def start(self) -> None:
        """Start the retry engine."""
        if self._running:
            return
            
        self._running = True
        logger.info("Starting retry engine")
        
        while self._running:
            try:
                await self._process_retries()
            except Exception as e:
                logger.error(f"Error in retry engine: {e}", exc_info=True)
                
            # Wait before next cycle
            await asyncio.sleep(settings.RETRY_INTERVAL_SECONDS)
            
    async def stop(self) -> None:
        """Stop the retry engine."""
        self._running = False
        logger.info("Retry engine stopped")
    
    async def _process_retries(self) -> None:
        """Process messages pending retry using context managers for DB access."""
        # Get messages that need retry - using context manager
        retry_candidates = await self._get_retry_candidates()
        
        if not retry_candidates:
            logger.debug("No messages to retry")
            return
            
        logger.info(f"Found {len(retry_candidates)} messages to retry")
        
        # Process retries with concurrency limit
        tasks = []
        for message in retry_candidates:
            tasks.append(self._retry_message(message))
            
        if tasks:
            # Process retries concurrently but with limit
            for i in range(0, len(tasks), 5):  # Process in chunks of 5
                chunk = tasks[i:i+5]
                await asyncio.gather(*chunk)
                await asyncio.sleep(1)  # Short delay between chunks
    
    async def _get_retry_candidates(self) -> List[Dict[str, Any]]:
        """
        Get messages that are candidates for retry.
        
        Uses context manager to ensure database connections are properly closed.
        
        Returns:
            List[Dict]: List of messages that should be retried
        """
        # Parameters for retry candidate selection
        now = datetime.now(timezone.utc)
        max_retries = settings.RETRY_MAX_ATTEMPTS
        retry_candidates = []
        
        try:
            # Use context manager for database access
            async with get_repository_context(MessageRepository) as message_repository:
                # Query for messages that:
                # 1. Are in a failed state
                # 2. Have not exceeded max retry attempts
                # 3. Have a retryable error or no error specified
                # 4. Last retry attempt was long enough ago (based on exponential backoff)
                
                failed_messages = await message_repository.get_retryable_messages(
                    max_retries=max_retries,
                    limit=50  # Limit number of messages to process in one cycle
                )
                
                # Filter messages based on retry delay (exponential backoff)
                for message in failed_messages:
                    # Get retry attempt count (from message metadata or events)
                    retry_count = self._get_retry_count(message)
                    
                    # Calculate backoff delay - 30s, 2m, 8m, 30m, 2h, etc.
                    backoff_seconds = 30 * (2 ** retry_count)
                    
                    # Get timestamp of last attempt
                    last_attempt = message.failed_at or message.updated_at
                    
                    # Check if enough time has passed for retry
                    if now - last_attempt > timedelta(seconds=backoff_seconds):
                        retry_candidates.append(message)
            
            return retry_candidates
        
        except Exception as e:
            logger.error(f"Error getting retry candidates: {e}", exc_info=True)
            # Return empty list on error to prevent crashing the retry engine
            return []
    
    def _get_retry_count(self, message: Any) -> int:
        """
        Get the retry count for a message.
        
        Args:
            message: Message object
            
        Returns:
            int: Number of retry attempts
        """
        # Check if retry count is in metadata
        meta_data = getattr(message, 'meta_data', {}) or {}
        if isinstance(meta_data, dict) and 'retry_count' in meta_data:
            return meta_data.get('retry_count', 0)
            
        # Fallback - count events of type "retry"
        retry_events = [e for e in getattr(message, 'events', []) 
                       if getattr(e, 'event_type', '') == 'retry']
        return len(retry_events)
    
    async def _retry_message(self, message: Any) -> None:
        """
        Retry sending a message with proper context management.
        
        Uses context managers for all database operations to prevent connection leaks.
        
        Args:
            message: Message to retry
        """
        message_id = message.id
        phone_number = message.phone_number
        message_text = message.message
        custom_id = message.custom_id
        
        # Get current retry count
        retry_count = self._get_retry_count(message)
        
        try:
            logger.info(f"Retrying message {message_id} (attempt {retry_count + 1})")
            
            # Use semaphore to limit concurrent retries
            async with self._semaphore:
                # Reset status to pending for retry - using context manager
                async with get_repository_context(MessageRepository) as message_repository:
                    # Update message status
                    await message_repository.update_message_status(
                        message_id=message_id,
                        status=MessageStatus.PENDING,
                        event_type="retry",
                        data={
                            "retry_count": retry_count + 1,
                            "previous_error": message.reason
                        }
                    )
                    
                    # Update metadata to track retry count
                    meta_data = getattr(message, 'meta_data', {}) or {}
                    if isinstance(meta_data, dict):
                        meta_data['retry_count'] = retry_count + 1
                        await message_repository.update(
                            id=message_id,
                            obj_in={"meta_data": meta_data}
                        )
                
                # Attempt to send again
                result = await self.sms_sender._send_to_gateway(
                    phone_number=phone_number,
                    message_text=message_text,
                    custom_id=custom_id or str(uuid.uuid4())
                )
                
                # Update message status with new context manager
                async with get_repository_context(MessageRepository) as message_repository:
                    await message_repository.update_message_status(
                        message_id=message_id,
                        status=result.get("status", MessageStatus.PENDING),
                        event_type="retry_success",
                        gateway_message_id=result.get("gateway_message_id"),
                        data=result
                    )
                
                # Publish event - doesn't need database connection
                await self.event_bus.publish(
                    EventType.MESSAGE_RETRIED,
                    {
                        "message_id": message_id,
                        "phone_number": phone_number,
                        "retry_count": retry_count + 1,
                        "status": result.get("status", MessageStatus.PENDING)
                    }
                )
                
                logger.info(f"Successfully retried message {message_id}")
                
        except Exception as e:
            logger.error(f"Error retrying message {message_id}: {e}")
            
            # Update status to failed with incremented retry count - in a new context
            try:
                error_message = str(e)
                is_retryable = isinstance(e, RetryableError)
                
                async with get_repository_context(MessageRepository) as message_repository:
                    await message_repository.update_message_status(
                        message_id=message_id,
                        status=MessageStatus.FAILED,
                        event_type="retry_failed",
                        reason=error_message,
                        data={
                            "retry_count": retry_count + 1,
                            "retryable": is_retryable
                        }
                    )
                
                # Publish event
                await self.event_bus.publish(
                    EventType.MESSAGE_RETRY_FAILED,
                    {
                        "message_id": message_id,
                        "phone_number": phone_number,
                        "retry_count": retry_count + 1,
                        "error": error_message,
                        "retryable": is_retryable
                    }
                )
            except Exception as update_error:
                logger.error(f"Failed to update error status: {update_error}")


# Singleton instance
_retry_engine = None

async def get_retry_engine():
    """
    Get the singleton retry engine instance.
    
    Note: This doesn't maintain any long-lived database connections,
    as the repository is now created within context managers for each operation.
    """
    global _retry_engine
    
    if _retry_engine is None:
        from app.services.event_bus.bus import get_event_bus
        from app.services.sms.sender import get_sms_sender
        
        event_bus = get_event_bus()
        sms_sender = await get_sms_sender()
        
        _retry_engine = RetryEngine(
            event_bus=event_bus,
            sms_sender=sms_sender
        )
        
    return _retry_engine
</file>

<file path="requirements.txt">
aiosqlite==0.21.0
alembic==1.15.2
android-sms-gateway==2.0.0
annotated-types==0.7.0
anyio==4.9.0
asgi-lifespan==2.1.0
asyncpg==0.30.0
bcrypt==4.3.0
certifi==2025.1.31
cffi==1.17.1
click==8.1.8
colorama==0.4.6
coverage==7.8.0
cryptography==44.0.2
dnspython==2.7.0
ecdsa==0.19.1
email_validator==2.2.0
fastapi==0.115.12
greenlet==3.2.1
h11==0.14.0
httpcore==1.0.8
httptools==0.6.4
httpx==0.28.1
idna==3.10
iniconfig==2.1.0
Mako==1.3.10
MarkupSafe==3.0.2
packaging==25.0
passlib==1.7.4
phonenumbers==9.0.3
pluggy==1.5.0
psycopg2-binary==2.9.10
pyasn1==0.4.8
pycparser==2.22
pydantic==2.11.3
pydantic-settings==2.9.1
pydantic_core==2.33.1
PyJWT==2.10.1
pytest==8.3.5
pytest-asyncio==0.26.0
pytest-cov==6.1.1
pytest-mock==3.14.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-jose==3.4.0
python-multipart==0.0.20
pytz==2025.2
PyYAML==6.0.2
rsa==4.9.1
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.40
starlette==0.46.2
typing-inspection==0.4.0
typing_extensions==4.13.2
uvicorn==0.34.2
watchfiles==1.0.5
websockets==15.0.1
</file>

<file path="tests/conftest.py">
import asyncio
import os
from datetime import datetime
from collections.abc import AsyncGenerator

import pytest
import pytest_asyncio
from httpx import AsyncClient
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from app.main import app
from app.db.repositories.users import UserRepository
from app.db.repositories.templates import TemplateRepository
from app.core.security import get_password_hash
from app.models import base as models
from app.schemas.user import User
from app.api.v1 import dependencies
from app.models.user import User as UserModel


# Use test DB from env or fallback to SQLite
TEST_DATABASE_URL = os.getenv("DATABASE_URL", "sqlite+aiosqlite:///:memory:")
async_engine = create_async_engine(TEST_DATABASE_URL, echo=False, future=True)
async_session_factory = sessionmaker(async_engine, class_=AsyncSession, expire_on_commit=False)

TEST_USER = {
    "email": "test@example.com",
    "password": "Test1234!",
    "full_name": "Test User",
    "role": "user"
}

TEST_TEMPLATES = [
    {
        "name": "Welcome Message",
        "content": "Hi {{name}}, welcome to our service! We're glad you've joined us.",
        "description": "Template for welcoming new users"
    },
    {
        "name": "OTP Verification",
        "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
        "description": "Template for sending OTP codes"
    },
    {
        "name": "Appointment Reminder",
        "content": "Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule.",
        "description": "Template for appointment reminders"
    }
]

@pytest_asyncio.fixture(autouse=True)
def override_auth():
    def fake_user():
        return User(
            id="test-user-id",
            email="test@example.com",
            is_active=True,
            is_superuser=False,
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
    app.dependency_overrides[dependencies.get_current_user] = lambda: fake_user()
    yield
    app.dependency_overrides.clear()

@pytest.fixture(scope="session", autouse=True)
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest_asyncio.fixture(scope="session", autouse=True)
async def initialize_test_db():
    print("⚙️  Initializing test DB and seeding data...")
    async with async_engine.begin() as conn:
        await conn.run_sync(models.Base.metadata.drop_all)
        await conn.run_sync(models.Base.metadata.create_all)

    async with async_session_factory() as session:
        user_repo = UserRepository(session)
        existing_user = await user_repo.get_by_email(TEST_USER["email"])
        if not existing_user:
            user = UserModel(
                id="test-user-id",
                email=TEST_USER["email"],
                hashed_password=get_password_hash(TEST_USER["password"]),
                full_name=TEST_USER["full_name"],
                role=TEST_USER["role"],
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(user)
        else:
            user = existing_user

        template_repo = TemplateRepository(session)
        for template in TEST_TEMPLATES:
            await template_repo.create_template(
                name=template["name"],
                content=template["content"],
                description=template["description"],
                user_id=user.id
            )

        await session.commit()
    
            # --- Add existing message and task for tests that depend on them ---
        from app.db.repositories.messages import MessageRepository
        from datetime import timezone

        now = datetime.now(timezone.utc)
        message_repo = MessageRepository(session)


        await message_repo.create_message(
            phone_number="+1234567890",
            message_text="Test message seeded for unit tests",
            user_id=user.id,
            custom_id="existing-msg-id",
            metadata={}
        )


        # Create a batch using repository logic
        batch = await message_repo.create_batch(
            user_id=user.id,
            name="Test Batch",
            total=1  # One message in batch
        )
        # Override ID for the test that expects it
        batch.id = "existing-task-id"

        # Add a message linked to this batch
        await message_repo.create_message(
            phone_number="+1234567890",
            message_text="Hello from seeded batch",
            user_id=user.id,
            custom_id="msg-in-batch",
            metadata={"batch_id": batch.id}
        )

        await session.commit()

    print(f"✅ Test DB seeded at {TEST_DATABASE_URL}")

@pytest_asyncio.fixture()
async def async_client() -> AsyncGenerator[AsyncClient, None]:
    from httpx import ASGITransport
    transport = ASGITransport(app=app)
    async with AsyncClient(base_url="http://testserver", transport=transport) as client:
        yield client

@pytest_asyncio.fixture()
async def db_session() -> AsyncGenerator[AsyncSession, None]:
    async with async_session_factory() as session:
        yield session
</file>

<file path="app/api/v1/endpoints/campaigns.py">
# app/api/v1/endpoints/campaigns.py
import csv
import io
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, UploadFile, File, Query, Path, status
from fastapi.responses import JSONResponse
import logging

from app.api.v1.dependencies import get_current_user, get_rate_limiter
from app.core.exceptions import ValidationError, NotFoundError
from app.schemas.campaign import (
    CampaignCreate,
    CampaignCreateFromCSV,
    CampaignUpdate,
    CampaignResponse,
    CampaignStatus,
)
from app.schemas.user import User
from app.schemas.message import MessageResponse, CampaignBulkDeleteRequest, BulkDeleteResponse
from app.utils.pagination import PaginationParams, paginate_response, PaginatedResponse, PageInfo
from app.services.campaigns.processor import get_campaign_processor
from app.db.session import get_repository_context

router = APIRouter()
logger = logging.getLogger("inboxerr.endpoint")



@router.post("/", response_model=CampaignResponse, status_code=status.HTTP_201_CREATED)
async def create_campaign(
    campaign: CampaignCreate,
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Create a new campaign.
    
    This creates a campaign in draft status. Messages can be added later.
    """
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "create_campaign")
    
    try:
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Create campaign
            result = await campaign_repo.create_campaign(
                name=campaign.name,
                description=campaign.description,
                user_id=current_user.id,
                scheduled_start_at=campaign.scheduled_start_at,
                scheduled_end_at=campaign.scheduled_end_at,
                settings=campaign.settings
            )
            
            return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating campaign: {str(e)}")


@router.post("/from-csv", response_model=CampaignResponse, status_code=status.HTTP_201_CREATED)
async def create_campaign_from_csv(
    file: UploadFile = File(...),
    campaign_data: str = Query(..., description="Campaign data as JSON string"),
    delimiter: str = Query(",", description="CSV delimiter"),
    has_header: bool = Query(True, description="Whether CSV has a header row"),
    phone_column: str = Query("phone", description="Column name containing phone numbers"),
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Create a campaign and add phone numbers from CSV.
    
    This creates a campaign and immediately adds all phone numbers from the CSV.
    The campaign will remain in draft status until explicitly started.
    """
    import json
    
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "create_campaign")
    
    try:
        # Parse campaign data
        campaign_dict = json.loads(campaign_data)
        campaign_data = CampaignCreateFromCSV(**campaign_dict)
        
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Create campaign
            campaign = await campaign_repo.create_campaign(
                name=campaign_data.name,
                description=campaign_data.description,
                user_id=current_user.id,
                scheduled_start_at=campaign_data.scheduled_start_at,
                scheduled_end_at=campaign_data.scheduled_end_at,
                settings=campaign_data.settings
            )
            
            # Read CSV file
            contents = await file.read()
            csv_file = io.StringIO(contents.decode('utf-8'))
            
            # Parse CSV
            csv_reader = csv.reader(csv_file, delimiter=delimiter)
            
            # Skip header if present
            if has_header:
                header = next(csv_reader)
                try:
                    phone_index = header.index(phone_column)
                except ValueError:
                    raise ValidationError(
                        message=f"Column '{phone_column}' not found in CSV header",
                        details={"available_columns": header}
                    )
            else:
                phone_index = 0  # Assume first column has phone numbers
            
            # Extract phone numbers
            phone_numbers = []
            for row in csv_reader:
                if row and len(row) > phone_index:
                    phone = row[phone_index].strip()
                    if phone:
                        phone_numbers.append(phone)
            
            if not phone_numbers:
                raise ValidationError(message="No valid phone numbers found in CSV")
            
            # Add phone numbers to campaign
            added_count = await campaign_repo.add_messages_to_campaign(
                campaign_id=campaign.id,
                phone_numbers=phone_numbers,
                message_text=campaign_data.message_template,
                user_id=current_user.id
            )
            
            # Refresh campaign
            campaign = await campaign_repo.get_by_id(campaign.id)
            
            return campaign
        
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid campaign data JSON")
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating campaign: {str(e)}")


@router.get("/", response_model=PaginatedResponse[CampaignResponse])
async def list_campaigns(
    pagination: PaginationParams = Depends(),
    status: Optional[str] = Query(None, description="Filter by campaign status"),
    current_user: User = Depends(get_current_user),
):
    """
    List campaigns for the current user.
    
    Returns a paginated list of campaigns.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Get campaigns with pagination
            campaigns, total = await campaign_repo.get_campaigns_for_user(
                user_id=current_user.id,
                status=status,
                skip=pagination.skip,
                limit=pagination.limit
            )
            
            # Calculate pagination info
            total_pages = (total + pagination.limit - 1) // pagination.limit
            
            # Return paginated response
            return paginate_response(campaigns, total, pagination)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing campaigns: {str(e)}")


@router.get("/{campaign_id}", response_model=CampaignResponse)
async def get_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
):
   """
   Get details of a specific campaign.
   """
   try:
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           # Get campaign
           campaign = await campaign_repo.get_by_id(campaign_id)
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           # Check authorization
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to access this campaign")
           
           return campaign
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error retrieving campaign: {str(e)}")


@router.put("/{campaign_id}", response_model=CampaignResponse)
async def update_campaign(
   campaign_update: CampaignUpdate,
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
):
   """
   Update campaign details.
   
   Only draft campaigns can be fully updated. Active campaigns can only have their description updated.
   """
   try:
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           # Get campaign
           campaign = await campaign_repo.get_by_id(campaign_id)
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           # Check authorization
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to update this campaign")
           
           # Check if campaign can be updated
           if campaign.status != "draft" and any([
               campaign_update.scheduled_start_at is not None,
               campaign_update.scheduled_end_at is not None,
               campaign_update.name is not None
           ]):
               raise HTTPException(
                   status_code=400, 
                   detail="Only draft campaigns can have name or schedule updated"
               )
           
           # Convert to dict and remove None values
           update_data = {k: v for k, v in campaign_update.dict().items() if v is not None}
           
           # Update campaign
           updated = await campaign_repo.update(id=campaign_id, obj_in=update_data)
           if not updated:
               raise HTTPException(status_code=500, detail="Failed to update campaign")
           
           return updated
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error updating campaign: {str(e)}")


@router.post("/{campaign_id}/start", response_model=CampaignResponse)
async def start_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
   campaign_processor = Depends(get_campaign_processor),
):
   """
   Start a campaign.
   
   This will change the campaign status to active and begin sending messages.
   """
   try:
       # Start campaign - campaign_processor already uses context managers internally
       success = await campaign_processor.start_campaign(
           campaign_id=campaign_id,
           user_id=current_user.id
       )
       
       if not success:
           raise HTTPException(status_code=400, detail="Failed to start campaign")
       
       # Get updated campaign - use context manager for this separate operation
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           return campaign
       
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error starting campaign: {str(e)}")


@router.post("/{campaign_id}/pause", response_model=CampaignResponse)
async def pause_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
   campaign_processor = Depends(get_campaign_processor),
):
   """
   Pause a campaign.
   
   This will change the campaign status to paused and stop sending messages.
   The campaign can be resumed later.
   """
   try:
       # Pause campaign - campaign_processor already uses context managers internally
       success = await campaign_processor.pause_campaign(
           campaign_id=campaign_id,
           user_id=current_user.id
       )
       
       if not success:
           raise HTTPException(status_code=400, detail="Failed to pause campaign")
       
       # Get updated campaign - use context manager for this separate operation
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           return campaign
       
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error pausing campaign: {str(e)}")


@router.post("/{campaign_id}/cancel", response_model=CampaignResponse)
async def cancel_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
   campaign_processor = Depends(get_campaign_processor),
):
   """
   Cancel a campaign.
   
   This will change the campaign status to cancelled and stop sending messages.
   The campaign cannot be resumed after cancellation.
   """
   try:
       # Cancel campaign - campaign_processor already uses context managers internally
       success = await campaign_processor.cancel_campaign(
           campaign_id=campaign_id,
           user_id=current_user.id
       )
       
       if not success:
           raise HTTPException(status_code=400, detail="Failed to cancel campaign")
       
       # Get updated campaign - use context manager for this separate operation
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           return campaign
       
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error cancelling campaign: {str(e)}")


@router.get("/{campaign_id}/messages", response_model=PaginatedResponse[MessageResponse])
async def get_campaign_messages(
   campaign_id: str = Path(..., description="Campaign ID"),
   pagination: PaginationParams = Depends(),
   status: Optional[str] = Query(None, description="Filter by message status"),
   current_user: User = Depends(get_current_user),
):
   """
   Get messages for a campaign.
   
   Returns a paginated list of messages for the specified campaign.
   """
   try:
       # Use repository context for proper connection management
       from app.db.repositories.campaigns import CampaignRepository
       from app.db.repositories.messages import MessageRepository
       
       # First check if campaign exists and belongs to user
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to access this campaign")
       
       # Get messages for campaign - in a separate context to avoid long transactions
       async with get_repository_context(MessageRepository) as message_repo:
           messages, total = await message_repo.get_messages_for_campaign(
               campaign_id=campaign_id,
               status=status,
               skip=pagination.skip,
               limit=pagination.limit
           )

           # Calculate proper pagination info
           total_pages = (total + pagination.limit - 1) // pagination.limit

           # Create proper PageInfo object
           page_info = PageInfo(
               current_page=pagination.page,
               total_pages=total_pages,
               page_size=pagination.limit,
               total_items=total,
               has_previous=pagination.page > 1,
               has_next=pagination.page < total_pages
           )
           
           # Return PaginatedResponse object
           return PaginatedResponse(
               items=messages,  # Direct Message objects - FastAPI will serialize them
               page_info=page_info
           )
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error retrieving campaign messages: {str(e)}")


@router.delete("/{campaign_id}", status_code=204)
async def delete_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
):
   """
   Delete a campaign.
   
   Only draft campaigns can be deleted. Active, paused, or completed campaigns cannot be deleted.
   """
   try:
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           # Get campaign
           campaign = await campaign_repo.get_by_id(campaign_id)
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           # Check authorization
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to delete this campaign")
           
           # Check if campaign can be deleted
           if campaign.status != "draft":
               raise HTTPException(
                   status_code=400, 
                   detail="Only draft campaigns can be deleted"
               )
           
           # Delete campaign
           success = await campaign_repo.delete(id=campaign_id)
           if not success:
               raise HTTPException(status_code=500, detail="Failed to delete campaign")
           
           return JSONResponse(status_code=204, content=None)
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error deleting campaign: {str(e)}")
   

@router.delete("/{campaign_id}/messages/bulk", response_model=BulkDeleteResponse)
async def bulk_delete_campaign_messages(
    request: CampaignBulkDeleteRequest,
    campaign_id: str = Path(..., description="Campaign ID"),
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Bulk delete messages from a campaign with event safety and server stability.
    
    This endpoint efficiently deletes multiple messages belonging to a specific campaign
    with optional filtering by status and date range. Enhanced with delivery event safety
    checking and batched processing for high-volume operations handling 10K-30K messages.
    
    **Event Safety Features:**
    - Pre-deletion check for delivery events
    - Requires explicit confirmation to delete tracking data
    - Clear warnings about data loss implications
    - Two-phase deletion (events first, then messages)
    
    **Server Stability Features:**
    - Batched processing prevents server overload
    - Configurable batch sizes for different load scenarios
    - Inter-batch delays prevent database lock contention
    - Graceful handling of partial failures
    
    **Business Safety Features:**
    - Campaign ownership validation
    - User authorization checks  
    - Active campaign protection
    - Comprehensive audit logging
    
    **Performance:**
    - Single SQL query per batch
    - Optimized for large datasets
    - 30K deletions in batches for stability
    
    **Business Use Cases:**
    - Clean up failed messages from campaigns
    - Remove test messages before campaign launch
    - Compliance-driven message deletion (with event confirmation)
    - Campaign optimization and cleanup
    
    Args:
        campaign_id: ID of the campaign containing messages to delete
        request: Bulk delete request with filters, confirmation, and force options
        current_user: Authenticated user (injected by dependency)
        rate_limiter: Rate limiting for bulk operations (injected by dependency)
    
    Returns:
        BulkDeleteResponse: Detailed results including event safety information
        
    Raises:
        HTTPException 400: Invalid request, campaign state, or missing confirmation
        HTTPException 403: User not authorized for this campaign
        HTTPException 404: Campaign not found
        HTTPException 429: Rate limit exceeded
        HTTPException 500: Database or internal server error
    """
    import time
    
    # Check rate limits for bulk operations
    await rate_limiter.check_rate_limit(current_user.id, "bulk_delete_campaign")
    
    start_time = time.time()
    
    try:
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        from app.db.repositories.messages import MessageRepository
        
        # First validate campaign exists and user has access
        async with get_repository_context(CampaignRepository) as campaign_repo:
            campaign = await campaign_repo.get_by_id(campaign_id)
            
            if not campaign:
                raise NotFoundError(message=f"Campaign {campaign_id} not found")
            
            # Check authorization
            if campaign.user_id != current_user.id:
                raise HTTPException(
                    status_code=403, 
                    detail="Not authorized to delete messages from this campaign"
                )
            
            # Safety check - prevent deletion from active campaigns unless force delete
            if campaign.status == "active" and not request.force_delete:
                raise HTTPException(
                    status_code=400,
                    detail="Cannot bulk delete messages from active campaign. Pause the campaign first or use force_delete."
                )
        
        # Perform bulk deletion with event safety
        async with get_repository_context(MessageRepository) as message_repo:
            # Convert datetime objects to ISO strings for repository method
            from_date_str = request.from_date.isoformat() if request.from_date else None
            to_date_str = request.to_date.isoformat() if request.to_date else None
            
            # Execute bulk deletion with enhanced safety
            deleted_count, failed_message_ids, metadata = await message_repo.bulk_delete_campaign_messages(
                campaign_id=campaign_id,
                user_id=current_user.id,
                status=request.status.value if request.status else None,
                from_date=from_date_str,
                to_date=to_date_str,
                limit=request.limit,
                force_delete=request.force_delete,
                batch_size=request.batch_size
            )
            
            # Calculate execution time
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # Build applied filters for audit trail
            filters_applied = {}
            if request.status:
                filters_applied["status"] = request.status.value
            if request.from_date:
                filters_applied["from_date"] = from_date_str
            if request.to_date:
                filters_applied["to_date"] = to_date_str
            filters_applied["limit"] = request.limit
            filters_applied["force_delete"] = request.force_delete
            filters_applied["batch_size"] = request.batch_size
            
            # Build response with enhanced metadata
            response = BulkDeleteResponse(
                deleted_count=deleted_count,
                campaign_id=campaign_id,
                failed_count=len(failed_message_ids),
                errors=[f"Failed to delete message: {msg_id}" for msg_id in failed_message_ids],
                operation_type="campaign",
                filters_applied=filters_applied,
                execution_time_ms=execution_time_ms,
                requires_confirmation=metadata.get("requires_confirmation", False),
                events_count=metadata.get("events_count"),
                events_deleted=metadata.get("events_deleted", 0),
                safety_warnings=metadata.get("safety_warnings", []),
                batch_info=metadata.get("batch_info")
            )
            
            # Log successful operation for audit
            logger.info(
                f"User {current_user.id} bulk deleted {deleted_count} messages "
                f"from campaign {campaign_id} in {execution_time_ms}ms "
                f"with filters: {filters_applied}. Events deleted: {metadata.get('events_deleted', 0)}"
            )
            
            return response
            
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        # Log error and return generic error message
        logger.error(f"Error in bulk_delete_campaign_messages: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Error performing bulk deletion: {str(e)}"
        )
</file>

<file path="app/core/config.py">
"""
Application settings and configuration management.
"""
from typing import List, Optional, Union
from pydantic import AnyHttpUrl, validator, field_validator
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """Application settings."""
    # Base
    PROJECT_NAME: str = "Inboxerr Backend"
    PROJECT_DESCRIPTION: str = "API backend for SMS management and delivery"
    VERSION: str = "0.1.0"
    API_PREFIX: str = "/api/v1"
    DEBUG: bool = False
    
    # CORS
    BACKEND_CORS_ORIGINS: List[Union[str, AnyHttpUrl]] = []

    @field_validator("BACKEND_CORS_ORIGINS", mode='before')
    def assemble_cors_origins(cls, v: Union[str, List[str]]) -> Union[List[str], str]:
        """Parse CORS origins from string or list."""
        if isinstance(v, str) and not v.startswith("["):
            return [i.strip() for i in v.split(",")]
        elif isinstance(v, (list, str)):
            return v
        raise ValueError(v)
    
    # Authentication
    SECRET_KEY: str = "CHANGEME_IN_PRODUCTION"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24  # 1 day
    API_KEY_HEADER: str = "X-API-Key"

    # Database
    DATABASE_URL: str = "postgresql+asyncpg://postgres:admin@localhost:5432/inboxerr"
    
    # SMS Gateway
    SMS_GATEWAY_URL: str = "https://endpointnumber1.work.gd/api/3rdparty/v1"
    SMS_GATEWAY_LOGIN: str = ""
    SMS_GATEWAY_PASSWORD: str = ""
    
    # Webhook
    API_BASE_URL: str = "http://localhost:8000"  # Base URL for webhooks
    WEBHOOK_SIGNATURE_KEY: Optional[str] = None
    WEBHOOK_TIMESTAMP_TOLERANCE: int = 300  # 5 minutes
    
    # SMS Processing
    BATCH_SIZE: int = 100
    DELAY_BETWEEN_SMS: float = 0.3  # seconds
    RETRY_ENABLED: bool = False
    RETRY_MAX_ATTEMPTS: int = 3
    RETRY_INTERVAL_SECONDS: int = 60
    
    # Metrics
    METRICS_ENABLED: bool = True

    # Logging
    LOG_LEVEL: str = "INFO"
    
    class Config:
        """Pydantic config."""
        case_sensitive = True
        env_file = ".env"

    # Mock settings for testing purposes
    SMS_GATEWAY_MOCK: bool = True  # For development and testing

# Create singleton settings instance
settings = Settings()
</file>

<file path="app/services/webhooks/manager.py">
# app/services/webhooks/manager.py
import logging
import hmac
import hashlib
import time
import json
from typing import Dict, Any, Optional, List, Tuple
import httpx
from fastapi.encoders import jsonable_encoder
from datetime import datetime, timezone 

from app.core.config import settings
from app.core.exceptions import SMSGatewayError
from app.db.repositories.messages import MessageRepository
from app.db.repositories.webhooks import WebhookRepository
from app.schemas.message import MessageStatus
from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.services.webhooks.models import (
    WebhookPayload, SmsReceivedPayload, SmsSentPayload, 
    SmsDeliveredPayload, SmsFailedPayload, SystemPingPayload
)
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.webhooks")

# Track registered webhooks
_registered_webhooks: Dict[str, str] = {}  # event_type -> webhook_id
_initialized = False

async def initialize_webhook_manager() -> None:
    """Initialize the webhook manager and register with SMS Gateway."""
    global _initialized
    
    if _initialized:
        return
        
    logger.info("Initializing webhook manager")
    
    # Register webhooks for each event type
    events_to_register = [
        "sms:sent", 
        "sms:delivered", 
        "sms:failed"
    ]
    
    for event_type in events_to_register:
        webhook_id = await register_webhook_with_gateway(event_type)
        if webhook_id:
            _registered_webhooks[event_type] = webhook_id
    
    _initialized = True
    logger.info(f"Webhook manager initialized, registered webhooks: {len(_registered_webhooks)}")

async def shutdown_webhook_manager() -> None:
    """Clean up webhook manager resources."""
    logger.info("Shutting down webhook manager")
    
    # Unregister all webhooks
    for event_type, webhook_id in _registered_webhooks.items():
        await unregister_webhook_from_gateway(webhook_id)
    
    _registered_webhooks.clear()
    logger.info("Webhook manager shutdown complete")

async def register_webhook_with_gateway(event_type: str) -> Optional[str]:
    """
    Register a webhook for a specific event type.
    
    Args:
        event_type: Event type to register for
        
    Returns:
        str: Webhook ID if registration successful
    """
    if not settings.SMS_GATEWAY_URL or not settings.SMS_GATEWAY_LOGIN or not settings.SMS_GATEWAY_PASSWORD:
        logger.warning("SMS Gateway credentials not configured, skipping webhook registration")
        return None
    
    # Webhook URL for the Gateway to call
    webhook_url = f"{settings.API_BASE_URL}{settings.API_PREFIX}/webhooks/gateway"
    
    try:
        # Create httpx client with authentication
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{settings.SMS_GATEWAY_URL}/webhooks",
                auth=(settings.SMS_GATEWAY_LOGIN, settings.SMS_GATEWAY_PASSWORD),
                json={
                    "id": f"inboxerr-{event_type}",  # Custom ID for tracking
                    "url": webhook_url,
                    "event": event_type
                },
                timeout=10.0
            )
            
            if response.status_code in (200, 201):
                webhook_data = response.json()
                webhook_id = webhook_data.get("id")
                logger.info(f"Successfully registered webhook for {event_type}: {webhook_id}")
                return webhook_id
            else:
                logger.error(f"Failed to register webhook for {event_type}: {response.status_code} - {response.text}")
                return None
                
    except Exception as e:
        logger.error(f"Error registering webhook for {event_type}: {e}")
        return None

async def unregister_webhook_from_gateway(webhook_id: str) -> bool:
    """
    Unregister a webhook from SMS Gateway.
    
    Args:
        webhook_id: Webhook ID
        
    Returns:
        bool: True if unregistration successful
    """
    if not settings.SMS_GATEWAY_URL or not settings.SMS_GATEWAY_LOGIN or not settings.SMS_GATEWAY_PASSWORD:
        logger.warning("SMS Gateway credentials not configured, skipping webhook unregistration")
        return False
    
    try:
        # Create httpx client with authentication
        async with httpx.AsyncClient() as client:
            response = await client.delete(
                f"{settings.SMS_GATEWAY_URL}/webhooks/{webhook_id}",
                auth=(settings.SMS_GATEWAY_LOGIN, settings.SMS_GATEWAY_PASSWORD),
                timeout=10.0
            )
            
            if response.status_code in (200, 204):
                logger.info(f"Successfully unregistered webhook: {webhook_id}")
                return True
            else:
                logger.error(f"Failed to unregister webhook: {response.status_code} - {response.text}")
                return False
                
    except Exception as e:
        logger.error(f"Error unregistering webhook: {e}")
        return False

async def process_gateway_webhook(raw_body: bytes, headers: Dict[str, str]) -> Tuple[bool, Dict[str, Any]]:
    """
    Process a webhook received from the SMS Gateway with enhanced error handling.
    
    Uses context managers for all database operations to prevent connection leaks.
    
    Args:
        raw_body: Raw request body
        headers: Request headers
        
    Returns:
        Tuple[bool, Dict]: (success, processed_data)
    """
    # Decode raw body for payload processing
    try:
        payload_str = raw_body.decode('utf-8')
    except UnicodeDecodeError:
        logger.error("Failed to decode webhook payload")
        return False, {"error": "Invalid payload encoding"}
    
    try:
        # Parse JSON
        try:
            payload_dict = json.loads(payload_str)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in webhook: {e}")
            return False, {"error": "Invalid JSON payload", "details": str(e)}
        
        # Verify webhook signature if enabled
        if settings.WEBHOOK_SIGNATURE_KEY:
            signature_valid, signature_error = verify_webhook_signature(payload_str, headers)
            if not signature_valid:
                logger.warning(f"Invalid webhook signature: {signature_error}")
                return False, {"error": "Invalid signature", "details": signature_error}
        
        # Validate basic payload structure
        try:
            base_payload = WebhookPayload(**payload_dict)
        except Exception as e:
            logger.error(f"Invalid webhook payload structure: {e}")
            return False, {"error": "Invalid payload structure", "details": str(e)}
        
        event_type = base_payload.event
        gateway_id = base_payload.id
        
        logger.info(f"Processing webhook event: {event_type}, gateway ID: {gateway_id}")
        
        # Record the incoming webhook event in database using context manager
        webhook_event_id = None
        try:
            async with get_repository_context(WebhookRepository) as webhook_repo:
                # Create webhook event record
                webhook_event = await webhook_repo.create_webhook_event(
                    event_type=event_type,
                    payload=payload_dict,
                    phone_number=payload_dict.get("payload", {}).get("phoneNumber"),
                    gateway_message_id=gateway_id
                )
                if webhook_event:
                    webhook_event_id = webhook_event.id
        except Exception as e:
            # Log error but continue processing - this is just for auditing
            logger.error(f"Error recording webhook event: {e}")
        
        # Process based on event type
        try:
            if event_type == "sms:received":
                payload = SmsReceivedPayload(**payload_dict["payload"])
                result = await process_sms_received(base_payload, payload)
            elif event_type == "sms:sent":
                payload = SmsSentPayload(**payload_dict["payload"])
                result = await process_sms_sent(base_payload, payload)
            elif event_type == "sms:delivered":
                payload = SmsDeliveredPayload(**payload_dict["payload"])
                result = await process_sms_delivered(base_payload, payload)
            elif event_type == "sms:failed":
                payload = SmsFailedPayload(**payload_dict["payload"])
                result = await process_sms_failed(base_payload, payload)
            elif event_type == "system:ping":
                payload = SystemPingPayload(**payload_dict["payload"])
                result = await process_system_ping(base_payload, payload)
            else:
                logger.warning(f"Unknown webhook event type: {event_type}")
                return False, {"error": "Unknown event type", "event_type": event_type}
                
            # Log successful processing
            logger.info(f"Successfully processed webhook event: {event_type}, gateway ID: {gateway_id}")
            
            # Mark event as processed if we created one - using context manager
            if webhook_event_id:
                try:
                    async with get_repository_context(WebhookRepository) as webhook_repo:
                        await webhook_repo.mark_event_processed(event_id=webhook_event_id)
                except Exception as e:
                    logger.error(f"Error marking webhook event as processed: {e}")
            
            return True, result
            
        except Exception as e:
            logger.error(f"Error processing webhook event {event_type}: {e}", exc_info=True)
            
            # Try to handle specific event processing errors gracefully
            error_details = {"error_type": type(e).__name__, "gateway_id": gateway_id}
            
            # Publish error event
            try:
                event_bus = get_event_bus()
                await event_bus.publish(
                    EventType.WEBHOOK_PROCESSED,
                    {
                        "success": False,
                        "event_type": event_type,
                        "gateway_id": gateway_id,
                        "error": str(e),
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                )
            except Exception as publish_error:
                logger.error(f"Error publishing webhook error event: {publish_error}")
                
            return False, {"error": f"Error processing {event_type} event", "details": str(e), **error_details}
            
    except Exception as e:
        logger.error(f"Unexpected error in webhook processing: {e}", exc_info=True)
        return False, {"error": "Unexpected error", "details": str(e)}


def verify_webhook_signature(payload: str, headers: Dict[str, str]) -> Tuple[bool, Optional[str]]:
    """
    Verify webhook signature from SMS Gateway with detailed error reporting.
    
    Args:
        payload: Webhook payload string
        headers: Request headers
        
    Returns:
        Tuple[bool, Optional[str]]: (is_valid, error_message_or_none)
    """
    signature = headers.get("X-Signature")
    timestamp = headers.get("X-Timestamp")
    
    if not signature:
        return False, "Missing X-Signature header"
    
    if not timestamp:
        return False, "Missing X-Timestamp header"
    
    # Verify timestamp is recent (within tolerance)
    try:
        ts = int(timestamp)
        current_time = int(time.time())
        if abs(current_time - ts) > settings.WEBHOOK_TIMESTAMP_TOLERANCE:
            return False, f"Timestamp too old: {timestamp} (current: {current_time})"
    except (ValueError, TypeError):
        return False, f"Invalid timestamp format: {timestamp}"
    
    # Calculate expected signature
    message = (payload + timestamp).encode()
    expected_signature = hmac.new(
        settings.WEBHOOK_SIGNATURE_KEY.encode(),
        message,
        hashlib.sha256
    ).hexdigest()
    
    # Compare signatures (constant-time comparison)
    is_valid = hmac.compare_digest(expected_signature, signature)
    
    return is_valid, None if is_valid else "Signature mismatch"


async def process_sms_received(base_payload: WebhookPayload, payload: SmsReceivedPayload) -> Dict[str, Any]:
    """Process SMS received event."""
    # For inbound messages - not the main focus for now
    logger.info(f"Received SMS: {payload.phone_number} -> '{payload.message}'")
    
    # Publish event for other components - no DB operations needed
    event_bus = get_event_bus()
    await event_bus.publish(
        EventType.SMS_RECEIVED,
        {
            "gateway_id": base_payload.id,
            "device_id": base_payload.device_id,
            "phone_number": payload.phone_number,
            "message": payload.message,
            "timestamp": payload.received_at.isoformat()
        }
    )
    
    return {
        "status": "processed",
        "event": "sms:received",
        "phone_number": payload.phone_number
    }


async def process_sms_sent(base_payload: WebhookPayload, payload: SmsSentPayload) -> Dict[str, Any]:
    """
    Process SMS sent event with proper context management.
    
    Args:
        base_payload: Base webhook payload
        payload: SMS sent payload
        
    Returns:
        Dict[str, Any]: Processing result
    """
    logger.info(f"SMS sent to {payload.phone_number}, gateway ID: {base_payload.id}")
    
    # Extract gateway message ID
    gateway_id = base_payload.id
    user_id = None
    message_id = None
    
    # Find message by gateway ID - using context manager
    try:
        async with get_repository_context(MessageRepository) as message_repo:
            message = await message_repo.get_by_gateway_id(gateway_id)
            if not message:
                # This could be normal if we didn't originate this message
                logger.info(f"No matching message found for gateway ID: {gateway_id}")
                return {
                    "status": "acknowledged",
                    "event": "sms:sent",
                    "message_found": False
                }
            
            # Store these for use outside the context
            message_id = message.id
            user_id = message.user_id
            
            # Update message status
            updated_message = await message_repo.update_message_status(
                message_id=message_id,
                status=MessageStatus.SENT,
                event_type="webhook",
                gateway_message_id=gateway_id,
                data=jsonable_encoder(base_payload)
            )
            
            if not updated_message:
                logger.warning(f"Failed to update message status for ID: {message_id}")
                return {
                    "status": "error",
                    "event": "sms:sent",
                    "message_id": message_id,
                    "error": "Failed to update message status"
                }
    except Exception as e:
        logger.error(f"Error processing sms:sent webhook: {e}")
        return {
            "status": "error",
            "event": "sms:sent",
            "error": str(e)
        }
    
    # Publish event - outside the database context
    try:
        event_bus = get_event_bus()
        await event_bus.publish(
            EventType.MESSAGE_SENT,
            {
                "message_id": message_id,
                "gateway_id": gateway_id,
                "phone_number": payload.phone_number,
                "user_id": user_id,
                "timestamp": payload.sent_at.isoformat()
            }
        )
    except Exception as e:
        logger.error(f"Error publishing event for sms:sent: {e}")
    
    return {
        "status": "processed",
        "event": "sms:sent",
        "message_id": message_id,
        "phone_number": payload.phone_number
    }


async def process_sms_delivered(base_payload: WebhookPayload, payload: SmsDeliveredPayload) -> Dict[str, Any]:
    """
    Process SMS delivered event with proper context management.
    
    Args:
        base_payload: Base webhook payload
        payload: SMS delivered payload
        
    Returns:
        Dict[str, Any]: Processing result
    """
    logger.info(f"SMS delivered to {payload.phone_number}, gateway ID: {base_payload.id}")
    
    # Extract gateway message ID
    gateway_id = base_payload.id
    user_id = None
    message_id = None
    
    # Find message by gateway ID - using context manager
    try:
        async with get_repository_context(MessageRepository) as message_repo:
            message = await message_repo.get_by_gateway_id(gateway_id)
            if not message:
                logger.info(f"No matching message found for gateway ID: {gateway_id}")
                return {
                    "status": "acknowledged",
                    "event": "sms:delivered",
                    "message_found": False
                }
            
            # Store these for use outside the context
            message_id = message.id
            user_id = message.user_id
            
            # Update message status
            updated_message = await message_repo.update_message_status(
                message_id=message_id,
                status=MessageStatus.DELIVERED,
                event_type="webhook",
                gateway_message_id=gateway_id,
                data=jsonable_encoder(base_payload)
            )
            
            if not updated_message:
                logger.warning(f"Failed to update message status for ID: {message_id}")
                return {
                    "status": "error",
                    "event": "sms:delivered",
                    "message_id": message_id,
                    "error": "Failed to update message status"
                }
    except Exception as e:
        logger.error(f"Error processing sms:delivered webhook: {e}")
        return {
            "status": "error",
            "event": "sms:delivered",
            "error": str(e)
        }
    
    # Publish event - outside the database context
    try:
        event_bus = get_event_bus()
        await event_bus.publish(
            EventType.MESSAGE_DELIVERED,
            {
                "message_id": message_id,
                "gateway_id": gateway_id,
                "phone_number": payload.phone_number,
                "user_id": user_id,
                "timestamp": payload.delivered_at.isoformat()
            }
        )
    except Exception as e:
        logger.error(f"Error publishing event for sms:delivered: {e}")
    
    return {
        "status": "processed",
        "event": "sms:delivered",
        "message_id": message_id,
        "phone_number": payload.phone_number
    }


async def process_sms_failed(base_payload: WebhookPayload, payload: SmsFailedPayload) -> Dict[str, Any]:
    """
    Process SMS failed event with proper context management.
    
    Args:
        base_payload: Base webhook payload
        payload: SMS failed payload
        
    Returns:
        Dict[str, Any]: Processing result
    """
    logger.info(f"SMS failed for {payload.phone_number}, reason: {payload.reason}, gateway ID: {base_payload.id}")
    
    # Extract gateway message ID and failure reason
    gateway_id = base_payload.id
    reason = payload.reason
    user_id = None
    message_id = None
    
    # Find message by gateway ID - using context manager
    try:
        async with get_repository_context(MessageRepository) as message_repo:
            message = await message_repo.get_by_gateway_id(gateway_id)
            if not message:
                logger.info(f"No matching message found for gateway ID: {gateway_id}")
                return {
                    "status": "acknowledged",
                    "event": "sms:failed",
                    "message_found": False
                }
            
            # Store these for use outside the context
            message_id = message.id
            user_id = message.user_id
            
            # Update message status
            updated_message = await message_repo.update_message_status(
                message_id=message_id,
                status=MessageStatus.FAILED,
                event_type="webhook",
                reason=reason,
                gateway_message_id=gateway_id,
                data=jsonable_encoder(base_payload)
            )
            
            if not updated_message:
                logger.warning(f"Failed to update message status for ID: {message_id}")
                return {
                    "status": "error",
                    "event": "sms:failed",
                    "message_id": message_id,
                    "error": "Failed to update message status"
                }
    except Exception as e:
        logger.error(f"Error processing sms:failed webhook: {e}")
        return {
            "status": "error",
            "event": "sms:failed",
            "error": str(e)
        }
    
    # Publish event - outside the database context
    try:
        event_bus = get_event_bus()
        await event_bus.publish(
            EventType.MESSAGE_FAILED,
            {
                "message_id": message_id,
                "gateway_id": gateway_id,
                "phone_number": payload.phone_number,
                "user_id": user_id,
                "reason": reason,
                "timestamp": payload.failed_at.isoformat()
            }
        )
    except Exception as e:
        logger.error(f"Error publishing event for sms:failed: {e}")
    
    return {
        "status": "processed",
        "event": "sms:failed",
        "message_id": message_id,
        "phone_number": payload.phone_number,
        "reason": reason
    }


async def process_system_ping(base_payload: WebhookPayload, payload: SystemPingPayload) -> Dict[str, Any]:
    """Process system ping event."""
    logger.info(f"System ping received from device: {base_payload.device_id}")
    
    # Simple acknowledgment - no database operations needed
    return {
        "status": "acknowledged",
        "event": "system:ping",
        "device_id": base_payload.device_id
    }


async def fetch_registered_webhooks_from_gateway() -> List[Dict[str, Any]]:
    """
    Fetch registered webhooks from the SMS Gateway.
    Returns a list of registered webhooks or raises an error.
    """
    if not settings.SMS_GATEWAY_URL or not settings.SMS_GATEWAY_LOGIN or not settings.SMS_GATEWAY_PASSWORD:
        raise SMSGatewayError("SMS Gateway credentials not configured", code="SMS_GATEWAY_CONFIG_MISSING")

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{settings.SMS_GATEWAY_URL}/webhooks",
                auth=(settings.SMS_GATEWAY_LOGIN, settings.SMS_GATEWAY_PASSWORD),
                timeout=10.0
            )
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error(f"Error fetching registered webhooks: {e}")
        raise
</file>

<file path="app/schemas/message.py">
"""
Pydantic schemas for message-related API operations.
"""
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from enum import Enum
from pydantic import BaseModel, Field, validator
from app.schemas.campaign import CampaignResponse


class MessageStatus(str, Enum):
    """Possible message statuses."""
    PENDING = "pending"
    PROCESSED = "processed"
    SENT = "sent"
    DELIVERED = "delivered"
    FAILED = "failed"
    SCHEDULED = "scheduled"
    CANCELLED = "cancelled"


class MessageCreate(BaseModel):
    """Schema for creating a new message."""
    phone_number: str = Field(..., description="Recipient phone number in E.164 format")
    message: str = Field(..., description="Message content")
    scheduled_at: Optional[datetime] = Field(None, description="Schedule message for future delivery")
    custom_id: Optional[str] = Field(None, description="Custom ID for tracking")
    
    @validator("phone_number")
    def validate_phone_number(cls, v):
        """Validate phone number format."""
        # Basic validation - will be handled more thoroughly in the service
        if not v or not (v.startswith("+") and len(v) >= 8):
            raise ValueError("Phone number must be in E.164 format (e.g. +1234567890)")
        return v
    
    @validator("message")
    def validate_message(cls, v):
        """Validate message content."""
        if not v or len(v.strip()) == 0:
            raise ValueError("Message cannot be empty")
        if len(v) > 1600:  # Allow for multi-part SMS
            raise ValueError("Message exceeds maximum length of 1600 characters")
        return v


class MessageResponse(BaseModel):
    """Schema for message response."""
    id: str = Field(..., description="Message ID")
    custom_id: Optional[str] = Field(None, description="Custom ID if provided")
    phone_number: str = Field(..., description="Recipient phone number")
    message: str = Field(..., description="Message content")
    status: MessageStatus = Field(..., description="Current message status")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    scheduled_at: Optional[datetime] = Field(None, description="Scheduled delivery time")
    sent_at: Optional[datetime] = Field(None, description="Time when message was sent")
    delivered_at: Optional[datetime] = Field(None, description="Time when message was delivered")
    failed_at: Optional[datetime] = Field(None, description="Time when message failed")
    reason: Optional[str] = Field(None, description="Failure reason if applicable")
    gateway_message_id: Optional[str] = Field(None, description="ID from SMS gateway")
    user_id: str = Field(..., description="User who sent the message")
    meta_data: Optional[Dict[str, Any]] = Field(default={}, description="Additional metadata")
    campaign: Optional[CampaignResponse] = Field(None, description="Campaign information if message belongs to a campaign")
    parts_count: Optional[int] = Field(None, description="Number of SMS parts")


    
    class Config:
        """Pydantic config."""
        from_attributes = True


class MessageStatusUpdate(BaseModel):
    """Schema for updating message status."""
    status: MessageStatus = Field(..., description="New message status")
    reason: Optional[str] = Field(None, description="Reason for status change (required for FAILED)")
    
    @validator("reason")
    def validate_reason(cls, v, values):
        """Validate reason field."""
        if values.get("status") == MessageStatus.FAILED and not v:
            raise ValueError("Reason is required when status is FAILED")
        return v


class BatchOptions(BaseModel):
    """Options for batch processing."""
    delay_between_messages: Optional[float] = Field(0.3, description="Delay between messages in seconds")
    fail_on_first_error: Optional[bool] = Field(False, description="Stop processing on first error")
    retry_failed: Optional[bool] = Field(True, description="Automatically retry failed messages")


class BatchMessageRequest(BaseModel):
    """Schema for batch message request."""
    messages: List[MessageCreate] = Field(..., description="List of messages to send")
    options: Optional[BatchOptions] = Field(default=None, description="Batch processing options")
    
    @validator("messages")
    def validate_messages(cls, v):
        """Validate message list."""
        if not v:
            raise ValueError("Message list cannot be empty")
        if len(v) > 1000:
            raise ValueError("Maximum batch size is 1000 messages")
        return v


class BatchMessageResponse(BaseModel):
    """Schema for batch message response."""
    batch_id: str = Field(..., description="Batch ID for tracking")
    total: int = Field(..., description="Total number of messages in batch")
    processed: int = Field(..., description="Number of messages processed")
    successful: int = Field(..., description="Number of successful messages")
    failed: int = Field(..., description="Number of failed messages")
    status: str = Field(..., description="Overall batch status")
    created_at: datetime = Field(..., description="Batch creation timestamp")
    messages: Optional[List[MessageResponse]] = Field(None, description="List of message responses")

class CampaignBulkDeleteRequest(BaseModel):
    """Schema for campaign-scoped bulk delete request."""
    status: Optional[MessageStatus] = Field(None, description="Filter by message status (e.g., 'failed', 'sent')")
    from_date: Optional[datetime] = Field(None, description="Delete messages from this date onwards (ISO format)")
    to_date: Optional[datetime] = Field(None, description="Delete messages up to this date (ISO format)")
    limit: int = Field(default=1000, le=10000, description="Maximum number of messages to delete (max 10,000)")
    confirm_delete: bool = Field(default=True, description="Confirmation flag - must be true to proceed")
    force_delete: bool = Field(default=False, description="Force delete messages with delivery events")
    confirmation_token: Optional[str] = Field(None, description="Required when force_delete=True - must be 'CONFIRM'")
    batch_size: int = Field(default=1000, le=5000, description="Process deletions in batches for server stability")
    
    @validator("limit")
    def validate_limit(cls, v):
        """Validate deletion limit for safety."""
        if v <= 0:
            raise ValueError("Limit must be greater than 0")
        if v > 10000:
            raise ValueError("Maximum limit is 10,000 messages per operation")
        return v
    
    @validator("batch_size")
    def validate_batch_size(cls, v):
        """Validate batch size for server stability."""
        if v <= 0:
            raise ValueError("Batch size must be greater than 0")
        if v > 5000:
            raise ValueError("Maximum batch size is 5,000 for server stability")
        return v
    
    @validator("confirmation_token")
    def validate_confirmation_token(cls, v, values):
        """Validate confirmation token when force delete is enabled."""
        if values.get("force_delete") and v != "CONFIRM":
            raise ValueError("confirmation_token must be 'CONFIRM' when force_delete is true")
        return v
    
    @validator("confirm_delete")
    def validate_confirmation(cls, v):
        """Ensure user confirms the bulk deletion."""
        if not v:
            raise ValueError("confirm_delete must be true to proceed with bulk deletion")
        return v
    
    @validator("from_date", "to_date")
    def validate_dates(cls, v):
        """Validate date format and timezone."""
        if v is not None:
            # Ensure datetime is timezone-aware
            if v.tzinfo is None:
                raise ValueError("Date must be timezone-aware (include timezone information)")
        return v

class GlobalBulkDeleteRequest(BaseModel):
    """Schema for global bulk delete request (by message IDs)."""
    message_ids: List[str] = Field(..., description="List of message IDs to delete")
    campaign_id: Optional[str] = Field(None, description="Optional campaign context for validation")
    confirm_delete: bool = Field(default=True, description="Confirmation flag - must be true to proceed")
    force_delete: bool = Field(default=False, description="Force delete messages with delivery events")
    confirmation_token: Optional[str] = Field(None, description="Required when force_delete=True - must be 'CONFIRM'")
    
    @validator("message_ids")
    def validate_message_ids(cls, v):
        """Validate message ID list."""
        if not v:
            raise ValueError("Message IDs list cannot be empty")
        if len(v) > 1000:
            raise ValueError("Maximum 1,000 message IDs per global bulk operation")
        
        # Check for duplicates
        if len(v) != len(set(v)):
            raise ValueError("Duplicate message IDs found in request")
        
        return v
    
    @validator("confirmation_token")
    def validate_confirmation_token(cls, v, values):
        """Validate confirmation token when force delete is enabled."""
        if values.get("force_delete") and v != "CONFIRM":
            raise ValueError("confirmation_token must be 'CONFIRM' when force_delete is true")
        return v
    
    @validator("confirm_delete")
    def validate_confirmation(cls, v):
        """Ensure user confirms the bulk deletion."""
        if not v:
            raise ValueError("confirm_delete must be true to proceed with bulk deletion")
        return v


class BulkDeleteResponse(BaseModel):
    """Schema for bulk delete operation response."""
    deleted_count: int = Field(..., description="Number of messages successfully deleted")
    campaign_id: Optional[str] = Field(None, description="Campaign ID if campaign-scoped operation")
    failed_count: int = Field(default=0, description="Number of messages that failed to delete")
    errors: List[str] = Field(default=[], description="List of error messages if any failures occurred")
    operation_type: str = Field(..., description="Type of bulk operation ('campaign' or 'global')")
    filters_applied: Dict[str, Any] = Field(default={}, description="Filters that were applied during deletion")
    execution_time_ms: Optional[int] = Field(None, description="Operation execution time in milliseconds")
    requires_confirmation: bool = Field(default=False, description="Whether force delete is needed due to existing events")
    events_count: Optional[int] = Field(None, description="Number of delivery events that would be deleted")
    events_deleted: int = Field(default=0, description="Number of delivery events actually deleted")
    safety_warnings: List[str] = Field(default=[], description="Safety warnings about delivery event deletion")
    batch_info: Optional[Dict[str, Any]] = Field(None, description="Batch processing information for large operations")
    
    class Config:
        """Pydantic config."""
        from_attributes = True
        schema_extra = {
            "example": {
                "deleted_count": 2847,
                "campaign_id": "camp_abc123",
                "failed_count": 0,
                "errors": [],
                "operation_type": "campaign",
                "filters_applied": {
                    "status": "failed",
                    "from_date": "2024-01-01T00:00:00Z"
                },
                "execution_time_ms": 3421,
                "requires_confirmation": False,
                "events_count": None,
                "events_deleted": 0,
                "safety_warnings": [],
                "batch_info": {
                    "batches_processed": 3,
                    "batch_size": 1000
                }
            }
        }


class BulkDeleteProgress(BaseModel):
    """Schema for tracking bulk delete operation progress (future use)."""
    operation_id: str = Field(..., description="Unique operation identifier")
    status: str = Field(..., description="Operation status ('pending', 'processing', 'completed', 'failed')")
    progress_percentage: int = Field(..., description="Progress percentage (0-100)")
    messages_processed: int = Field(..., description="Number of messages processed so far")
    total_messages: int = Field(..., description="Total number of messages to process")
    estimated_completion: Optional[datetime] = Field(None, description="Estimated completion time")
    errors: List[str] = Field(default=[], description="Any errors encountered during processing")
    
    class Config:
        """Pydantic config."""
        from_attributes = True
</file>

<file path="app/services/sms/sender.py">
"""
SMS sender service for interacting with the Android SMS Gateway.
"""
import asyncio
import logging
import uuid
import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Tuple, Union
from httpx import HTTPStatusError

from app.core.config import settings
from app.core.exceptions import ValidationError, SMSGatewayError, RetryableError, SMSAuthError
from app.utils.phone import validate_phone
from app.db.repositories.templates import TemplateRepository
from app.db.repositories.messages import MessageRepository
from app.schemas.message import MessageCreate, MessageStatus, BatchMessageRequest, BatchOptions
from app.services.event_bus.events import EventType
from app.db.session import get_repository_context, get_repository, get_session

# Lazy import of android_sms_gateway to avoid import errors if not installed
try:
    from android_sms_gateway import client, domain
    SMS_GATEWAY_AVAILABLE = True
except ImportError:
    SMS_GATEWAY_AVAILABLE = False


logger = logging.getLogger("inboxerr.sms")


class SMSSender:
    """
    Service for sending SMS messages through the Android SMS Gateway.
    """
    
    def __init__(
        self,
        event_bus: Any
    ):
        """
        Initialize SMS sender service.
        
        Args:
            message_repository: Repository for message storage
            event_bus: Event bus for publishing events
        """
        self.event_bus = event_bus
        self._semaphore = asyncio.Semaphore(10)  # Limit concurrent requests
        self._last_send_time = 0
        
        # Check if gateway client is available
        if not SMS_GATEWAY_AVAILABLE:
            logger.warning("Android SMS Gateway client not installed. SMS sending will be simulated.")
    
    async def send_message(
        self,
        *,
        phone_number: str,
        message_text: str,
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        custom_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        campaign_id: Optional[str] = None,
        priority: int = 0,
        ttl: Optional[int] = None,
        sim_number: Optional[int] = None,
        is_encrypted: bool = False
    ) -> Dict[str, Any]:
        """
        Send a single SMS message.
        
        Args:
            phone_number: Recipient phone number
            message_text: Message content
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            custom_id: Optional custom ID for tracking
            metadata: Optional additional data
            campaign_id: Optional campaign ID
            priority: Message priority (0-127, ≥100 bypasses limits)
            ttl: Time-to-live in seconds
            sim_number: SIM card to use (1-3)
            is_encrypted: Whether message is encrypted
            
        Returns:
            Dict: Message details with status
            
        Raises:
            ValidationError: If phone number is invalid
            SMSGatewayError: If there's an error sending the message
        """
        # Validate phone number
        is_valid, formatted_number, error, _ = validate_phone(phone_number)
        if not is_valid:
            raise ValidationError(message=f"Invalid phone number: {error}")
        
        # Generate id to track message in the system.
        custom_id = custom_id or str(uuid.uuid4())

        # Use context manager for repository
        async with get_repository_context(MessageRepository) as repo:
            # Create message in database
            db_message = await repo.create_message(
                phone_number=formatted_number,
                message_text=message_text,
                user_id=user_id,
                custom_id=custom_id,
                scheduled_at=scheduled_at,
                metadata=metadata or {},
                campaign_id=campaign_id
            )
            
            # ✅ ALWAYS publish MESSAGE_CREATED event (this was missing!)
            await self.event_bus.publish(
                EventType.MESSAGE_CREATED,
                {
                    "message_id": db_message.id,
                    "phone_number": formatted_number,
                    "user_id": user_id,
                    "campaign_id": campaign_id
                }
            )
            
            # If scheduled for future, return message details
            if scheduled_at and scheduled_at > datetime.now(timezone.utc):
                logger.info(f"Message {db_message.id} scheduled for {scheduled_at}")
                
                # Publish scheduled event
                await self.event_bus.publish(
                    EventType.MESSAGE_SCHEDULED,
                    {
                        "message_id": db_message.id,
                        "phone_number": formatted_number,
                        "scheduled_at": scheduled_at.isoformat(),
                        "user_id": user_id
                    }
                )
                
                return db_message.dict()
            
            # Otherwise, send immediately
            try:
                result = await self._send_to_gateway(
                    phone_number=formatted_number,
                    message_text=message_text,
                    custom_id=db_message.custom_id,
                    priority=priority,
                    ttl=ttl,
                    sim_number=sim_number,
                    is_encrypted=is_encrypted
                )
                
                # Update message status
                await repo.update_message_status(
                    message_id=db_message.id,
                    status=result.get("status", MessageStatus.PENDING),
                    event_type="gateway_response",
                    gateway_message_id=result.get("gateway_message_id"),
                    data=result
                )
                
                # ✅ Publish MESSAGE_SENT event (this was missing!)
                await self.event_bus.publish(
                    EventType.MESSAGE_SENT,
                    {
                        "message_id": db_message.id,
                        "phone_number": formatted_number,
                        "user_id": user_id,
                        "gateway_message_id": result.get("gateway_message_id")
                    }
                )
                
                # Get updated message
                updated_message = await repo.get_by_id(db_message.id)
                return updated_message.dict()
                
            except Exception as e:
                # Handle error
                error_status = MessageStatus.FAILED
                error_message = str(e)
                logger.error(f"Error sending message {db_message.id}: {error_message}")
                
                # Update message status
                await repo.update_message_status(
                    message_id=db_message.id,
                    status=error_status,
                    event_type="send_error",
                    reason=error_message,
                    data={"error": error_message}
                )
                
                # ✅ Publish MESSAGE_FAILED event (this was missing!)
                await self.event_bus.publish(
                    EventType.MESSAGE_FAILED,
                    {
                        "message_id": db_message.id,
                        "phone_number": formatted_number,
                        "user_id": user_id,
                        "reason": error_message
                    }
                )
                
                # Re-raise as SMSGatewayError
                if isinstance(e, RetryableError):
                    raise SMSGatewayError(message=error_message, code="GATEWAY_ERROR", status_code=503)
                else:
                    raise SMSGatewayError(message=error_message, code="GATEWAY_ERROR")

    async def send_batch(
        self,
        *,
        messages: List[MessageCreate],
        user_id: str,
        options: Optional[BatchOptions] = None,
        campaign_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a batch of SMS messages.
        
        Args:
            messages: List of messages to send
            user_id: User ID
            options: Optional batch processing options
            campaign_id: Optional campaign ID
            
        Returns:
            Dict: Batch details with status
            
        Raises:
            ValidationError: If any phone number is invalid
            SMSGatewayError: If there's an error sending the messages
        """
        if not messages:
            raise ValidationError(message="No messages provided")
        
        # Set default options
        if not options:
            options = BatchOptions(
                delay_between_messages=0.3,
                fail_on_first_error=False,
                retry_failed=True
            )
        
        # Create batch in database using context manager
        async with get_repository_context(MessageRepository) as repo:
            batch = await repo.create_batch(
                user_id=user_id,
                name=f"Batch {datetime.now(timezone.utc).isoformat()}",
                total=len(messages)
            )
            
            # Process in background
            asyncio.create_task(
                self._process_batch(
                    messages=messages,
                    user_id=user_id,
                    batch_id=batch.id,
                    campaign_id=campaign_id,
                    options=options
                )
            )
            
            # Return batch details
            return {
                "batch_id": batch.id,
                "total": batch.total,
                "processed": 0,
                "successful": 0,
                "failed": 0,
                "status": batch.status,
                "created_at": batch.created_at
            }

    async def _process_batch(
        self,
        *,
        messages: List[MessageCreate],
        user_id: str,
        batch_id: str,
        campaign_id: Optional[str] = None,
        options: BatchOptions
    ) -> None:
        """
        Process a batch of messages with improved concurrency handling.
        
        Args:
            messages: List of messages to send
            user_id: User ID
            batch_id: Batch ID
            campaign_id: Optional campaign ID
            options: Batch processing options
        """
        processed = 0
        successful = 0
        failed = 0
        
        # Create a semaphore to limit concurrent processing
        send_semaphore = asyncio.Semaphore(5)  # Limit concurrent sends
        
        try:
            # Calculate optimal chunk size - smaller for better reliability
            total_messages = len(messages)
            chunk_size = min(20, max(5, total_messages // 10))  # Smaller chunks
            
            logger.info(f"Processing batch {batch_id} with {total_messages} messages in chunks of {chunk_size}")
            
            # Process in chunks for better performance
            for i in range(0, total_messages, chunk_size):
                chunk = messages[i:i+chunk_size]

                # Process all messages in this chunk concurrently
                tasks = [
                    self._process_single_message_safely(
                        message=msg,
                        user_id=user_id,
                        batch_id=batch_id,
                        campaign_id=campaign_id,
                        semaphore=send_semaphore
                    )
                    for msg in chunk
                ]
            
                # Wait for all tasks in this chunk
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # Count successes and failures
                chunk_successful = sum(1 for r in results if r is True)
                chunk_failed = sum(1 for r in results if r is False or isinstance(r, Exception))

                # Update batch progress with a proper context-managed repository
                async with get_repository_context(MessageRepository) as repo:
                    await repo.update_batch_progress(
                        batch_id=batch_id,
                        increment_processed=len(chunk),
                        increment_successful=chunk_successful,
                        increment_failed=chunk_failed
                    )

                # Update counters
                processed += len(chunk)
                successful += chunk_successful
                failed += chunk_failed

                # Report progress
                progress_pct = (processed / total_messages) * 100
                logger.info(f"Batch {batch_id} progress: {progress_pct:.1f}% ({processed}/{total_messages})")
                
                # Add delay between chunks
                if i + chunk_size < total_messages:
                    await asyncio.sleep(options.delay_between_messages * 2)  # Double the delay for stability
            
            # Final update with status
            final_status = MessageStatus.PROCESSED
            if processed == 0:
                final_status = MessageStatus.FAILED
            elif failed > 0:
                final_status = "partial"
            
            async with get_repository_context(MessageRepository) as repo:
                await repo.update_batch_progress(
                    batch_id=batch_id,
                    status=final_status
                )
            
            # Publish event - this doesn't need a database connection
            await self.event_bus.publish(
                EventType.BATCH_COMPLETED,
                {
                    "batch_id": batch_id,
                    "campaign_id": campaign_id,
                    "total": total_messages,
                    "processed": processed,
                    "successful": successful,
                    "failed": failed,
                    "status": final_status,
                    "user_id": user_id
                }
            )
            
        except Exception as e:
            logger.error(f"Batch processing error: {e}", exc_info=True)
            
            # Update batch status to error state with proper connection handling
            try:
                async with get_repository_context(MessageRepository) as repo:
                    await repo.update_batch_progress(
                        batch_id=batch_id,
                        status=MessageStatus.FAILED
                    )
            except Exception as update_error:
                logger.error(f"Failed to update batch status after error: {update_error}")

    # This method looks good with your changes!
    async def _process_single_message_safely(
        self,
        *,
        message: MessageCreate,
        user_id: str,
        batch_id: str,
        campaign_id: Optional[str] = None,
        semaphore: asyncio.Semaphore
    ) -> bool:
        """
        Process a single message with proper connection handling.
        
        Args:
            message: Message to process
            user_id: User ID
            batch_id: Batch ID
            campaign_id: Optional campaign ID
            semaphore: Semaphore for concurrency control
            
        Returns:
            bool: True if successful, False if failed
        """
        # Use the context manager to ensure proper session lifecycle
        try:
            async with get_repository_context(MessageRepository) as repo:
                # Build metadata
                metadata = {"batch_id": batch_id}
                if campaign_id:
                    metadata["campaign_id"] = campaign_id
                
                # Create message in database
                db_message = await repo.create_message(
                    phone_number=message.phone_number,
                    message_text=message.message,
                    user_id=user_id,
                    custom_id=message.custom_id or str(uuid.uuid4()),
                    scheduled_at=message.scheduled_at,
                    metadata=metadata,
                    batch_id=batch_id,
                    campaign_id=campaign_id
                )

                # Skip scheduled messages
                if db_message.scheduled_at and db_message.scheduled_at > datetime.now(timezone.utc):
                    return True
                
                # Process this message with rate limiting
                async with semaphore:
                    # Send message
                    result = await self._send_to_gateway(
                        phone_number=db_message.phone_number,
                        message_text=db_message.message,
                        custom_id=db_message.custom_id
                    )
                    
                    # Update message status in the same session
                    await repo.update_message_status(
                        message_id=db_message.id,
                        status=result.get("status", MessageStatus.PENDING),
                        event_type="gateway_response",
                        gateway_message_id=result.get("gateway_message_id"),
                        data=result
                    )
                    
                    return True
                    
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            
            # Try to record the failure if we have a message
            if 'db_message' in locals():
                try:
                    async with get_repository_context(MessageRepository) as err_repo:
                        await err_repo.update_message_status(
                            message_id=db_message.id,
                            status=MessageStatus.FAILED,
                            event_type="send_error",
                            reason=str(e),
                            data={"error": str(e)}
                        )
                except Exception as update_error:
                    logger.error(f"Failed to update error status: {update_error}")
            
            return False

    async def _process_single_message(
        self,
        *,
        message: MessageCreate,
        user_id: str,
        batch_id: Optional[str] = None,
        campaign_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process a single message within a batch.
        
        Args:
            message: Message to send
            user_id: User ID
            batch_id: Optional batch ID
            campaign_id: Optional campaign ID
            
        Returns:
            Dict: Result of message processing
            
        Raises:
            Exception: Any error during processing
        """
        # Build metadata
        metadata = {}
        if batch_id:
            metadata["batch_id"] = batch_id
        if campaign_id:
            metadata["campaign_id"] = campaign_id
            
        # Set priority based on campaign
        # Campaigns get slightly higher priority but still below urgent messages
        priority = 50 if campaign_id else 0
            
        # Send message
        return await self.send_message(
            phone_number=message.phone_number,
            message_text=message.message,
            user_id=user_id,
            scheduled_at=message.scheduled_at,
            custom_id=message.custom_id,
            metadata=metadata,
            campaign_id=campaign_id,
            priority=priority
        )
    
    async def send_messages_bulk(
        self,
        *,
        messages: List[Dict[str, Any]],
        user_id: str,
        campaign_id: Optional[str] = None,
        batch_id: Optional[str] = None,
        chunk_size: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Send multiple messages efficiently in bulk.
        
        Args:
            messages: List of message dictionaries with recipient and content
            user_id: User ID
            campaign_id: Optional campaign ID
            batch_id: Optional batch ID
            chunk_size: Number of messages to process in each chunk
            
        Returns:
            List[Dict]: List of results for each message
        """
        results = []
        
        # Process in chunks
        for i in range(0, len(messages), chunk_size):
            chunk = messages[i:i+chunk_size]
            chunk_results = await self._process_message_chunk(
                messages=chunk,
                user_id=user_id,
                campaign_id=campaign_id,
                batch_id=batch_id
            )
            results.extend(chunk_results)
            
            # Small delay between chunks to prevent overloading
            if i + chunk_size < len(messages):
                await asyncio.sleep(1)
        
        return results
    
    async def _process_message_chunk(
        self, 
        messages: List[Dict[str, Any]],
        user_id: str,
        campaign_id: Optional[str] = None,
        batch_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Process a chunk of messages concurrently.
        
        Args:
            messages: List of message dictionaries to process
            user_id: User ID
            campaign_id: Optional campaign ID
            batch_id: Optional batch ID
            
        Returns:
            List[Dict]: Results for each message
        """
        # Create tasks for all messages
        tasks = []
        created_messages = []
        
        # Use a single session for all initial creation
        async with get_repository_context(MessageRepository) as repo:
            for msg in messages:
                try:
                    # Create database entries first to get IDs
                    db_message = await repo.create_message(
                        phone_number=msg["phone_number"],
                        message_text=msg["message_text"],
                        user_id=user_id,
                        custom_id=msg.get("custom_id"),
                        scheduled_at=msg.get("scheduled_at"),
                        metadata=msg.get("metadata", {}),
                        campaign_id=campaign_id
                    )
                    
                    created_messages.append(db_message)
                    
                except Exception as e:
                    logger.error(f"Error creating message: {e}")
        
        # Process messages with separate tasks
        for db_message in created_messages:
            # Skip if scheduled for the future
            if db_message.scheduled_at and db_message.scheduled_at > datetime.now(timezone.utc):
                tasks.append(asyncio.create_task(
                    asyncio.sleep(0)  # Dummy task for scheduled messages
                ))
                continue
                
            # Create task to send via gateway
            task = asyncio.create_task(
                self._send_message_with_error_handling(
                    db_message=db_message,
                    phone_number=db_message.phone_number,
                    message_text=db_message.message,
                    priority=0,  # Default priority
                    ttl=None,
                    sim_number=None,
                    is_encrypted=False
                )
            )
            tasks.append(task)
        
        # Wait for all tasks to complete
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return [r for r in results if not isinstance(r, Exception)]
        
        return []
    
    async def _send_message_with_error_handling(
        self,
        *,
        db_message: Any,
        phone_number: str,
        message_text: str,
        priority: int = 0,
        ttl: Optional[int] = None,
        sim_number: Optional[int] = None,
        is_encrypted: bool = False
    ) -> Dict[str, Any]:
        """
        Send message with error handling and status updates.
        
        Args:
            db_message: Database message object
            phone_number: Recipient phone number
            message_text: Message content
            priority: Message priority
            ttl: Time-to-live in seconds
            sim_number: SIM card to use
            is_encrypted: Whether message is encrypted
            
        Returns:
            Dict: Result of message sending
        """
        try:
            # Send to gateway
            result = await self._send_to_gateway(
                phone_number=phone_number,
                message_text=message_text,
                custom_id=db_message.custom_id,
                priority=priority,
                ttl=ttl,
                sim_number=sim_number,
                is_encrypted=is_encrypted
            )
            
            # Update status with a fresh session
            async with get_repository_context(MessageRepository) as repo:
                # Update message status
                await repo.update_message_status(
                    message_id=db_message.id,
                    status=result.get("status", MessageStatus.PENDING),
                    event_type="gateway_response",
                    gateway_message_id=result.get("gateway_message_id"),
                    data=result
                )
            
            return result
            
        except Exception as e:
            # Update status to failed with a fresh session
            try:
                async with get_repository_context(MessageRepository) as repo:
                    await repo.update_message_status(
                        message_id=db_message.id,
                        status=MessageStatus.FAILED,
                        event_type="send_error",
                        reason=str(e),
                        data={"error": str(e)}
                    )
            except Exception as update_error:
                # If even the error update fails, just log it
                logger.error(f"Failed to update error status for message {db_message.id}: {update_error}")
            
            # Log and re-raise
            logger.error(f"Error sending message {db_message.id}: {e}")
            raise
    
    async def schedule_batch_from_numbers(
        self,
        *,
        phone_numbers: List[str],
        message_text: str,
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        campaign_id: Optional[str] = None
    ) -> str:
        """
        Schedule a batch of messages from a list of phone numbers.
        
        Args:
            phone_numbers: List of phone numbers
            message_text: Message content
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            campaign_id: Optional campaign ID
            
        Returns:
            str: Batch ID
            
        Raises:
            ValidationError: If any phone number is invalid
        """
        if not phone_numbers:
            raise ValidationError(message="No phone numbers provided")
        
        # Create messages
        messages = []
        for phone in phone_numbers:
            # Basic validation
            is_valid, formatted_number, error, _ = validate_phone(phone)
            if is_valid:
                messages.append(
                    MessageCreate(
                        phone_number=formatted_number,
                        message=message_text,
                        scheduled_at=scheduled_at,
                        custom_id=str(uuid.uuid4())
                    )
                )
        
        if not messages:
            raise ValidationError(message="No valid phone numbers found")
        
        # Create and process batch
        result = await self.send_batch(
            messages=messages,
            user_id=user_id,
            campaign_id=campaign_id,
            options=BatchOptions(
                delay_between_messages=settings.DELAY_BETWEEN_SMS,
                fail_on_first_error=False,
                retry_failed=True
            )
        )
        
        return result["batch_id"]
    
    async def get_message(self, message_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """
        Get message details.
        
        Args:
            message_id: Message ID or custom ID
            user_id: User ID for authorization
            
        Returns:
            Dict: Message details or None if not found
        """
        async with get_repository_context(MessageRepository) as repo:
            # Try to get by ID first
            message = await repo.get_by_id(message_id)
            
            # If not found, try custom ID
            if not message:
                message = await repo.get_by_custom_id(message_id)
                
            # If not found, try gateway ID
            if not message:
                message = await repo.get_by_gateway_id(message_id)
            
            # Check authorization
            if message and str(message.user_id) != str(user_id):
                return None
            
            return message.dict() if message else None
    
    async def list_messages(
        self,
        *,
        filters: Dict[str, Any],
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        List messages with filtering and pagination.
        
        Args:
            filters: Filter criteria
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Dict], int]: List of messages and total count
        """
        # Extract user_id from filters
        user_id = filters.pop("user_id", None)
        if not user_id:
            return [], 0
        
        async with get_repository_context(MessageRepository) as repo:
            # Get messages
            messages, total = await repo.list_messages_for_user(
                user_id=user_id,
                status=filters.get("status"),
                phone_number=filters.get("phone_number"),
                from_date=filters.get("from_date"),
                to_date=filters.get("to_date"),
                campaign_id=filters.get("campaign_id"),  # Support filtering by campaign
                skip=skip,
                limit=limit
            )
            
            # Convert to dict
            message_dicts = [message.dict() for message in messages]
            
            return message_dicts, total
    
    async def update_message_status(
        self,
        *,
        message_id: str,
        status: str,
        reason: Optional[str] = None,
        user_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        Update message status.
        
        Args:
            message_id: Message ID
            status: New status
            reason: Reason for status change
            user_id: User ID for authorization
            
        Returns:
            Dict: Updated message or None if not found
        """
        async with get_repository_context(MessageRepository) as repo:
            # Get message
            message = await repo.get_by_id(message_id)
            if not message:
                return None
            
            # Check authorization
            if str(message.user_id) != str(user_id):
                return None
            
            # Update status
            updated = await repo.update_message_status(
                message_id=message_id,
                status=status,
                event_type="manual_update",
                reason=reason,
                data={"updated_by": user_id}
            )
            
            return updated.dict() if updated else None
    
    async def delete_message(self, message_id: str, user_id: str) -> bool:
        """
        Delete a message.
        
        Args:
            message_id: Message ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if deleted, False otherwise
        """
        async with get_repository_context(MessageRepository) as repo:
            # Get message
            message = await repo.get_by_id(message_id)
            if not message:
                return False
            
            # Check authorization
            if str(message.user_id) != str(user_id):
                return False
            
            # Delete message
            return await repo.delete(id=message_id)
    
    async def get_task_status(self, task_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """
        Get status of a background task (batch).
        
        Args:
            task_id: Task/batch ID
            user_id: User ID for authorization
            
        Returns:
            Dict: Task status or None if not found
        """
        async with get_repository_context(MessageRepository) as repo:
            # Get batch
            batch = await repo.get_by_id(task_id)
            if not batch:
                return None
            
            # Check authorization
            if str(batch.user_id) != str(user_id):
                return None
            
            # Get message stats
            messages, total = await repo.get_messages_for_batch(
                batch_id=task_id,
                limit=5  # Just get the first few for preview
            )
            
            # Convert to dict
            message_previews = [message.dict() for message in messages]
            
            return {
                "id": batch.id,
                "status": batch.status,
                "total": batch.total,
                "processed": batch.processed,
                "successful": batch.successful,
                "failed": batch.failed,
                "created_at": batch.created_at,
                "completed_at": batch.completed_at,
                "message_previews": message_previews
            }

    async def _send_to_gateway(
        self,
        *,
        phone_number: str,
        message_text: str,
        custom_id: str,
        priority: int = 0,
        ttl: Optional[int] = None,
        sim_number: Optional[int] = None,
        is_encrypted: bool = False
    ) -> Dict[str, Any]:
        """
        Send message to SMS gateway with high-volume optimizations.
        """
        # High-volume systems should avoid synchronized rate limiting
        # Instead, we'll use semaphores for concurrency control
        
        # Check if gateway client is available or mock mode enabled
        if not SMS_GATEWAY_AVAILABLE or getattr(settings, "SMS_GATEWAY_MOCK", False):
            # Simulate sending for development/testing
            logger.info(f"[MOCK] Sending SMS to {phone_number}: {message_text[:30]}...")
            # Fast simulation for high volume
            await asyncio.sleep(0.05)
            
            # Return simulated response
            return {
                "status": MessageStatus.SENT,
                "gateway_message_id": f"mock_{uuid.uuid4()}",
                "phone_number": phone_number,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        
        # Use semaphore to limit concurrent requests to gateway
        async with self._semaphore:
            try:
                # Create client with optimized connection settings
                connect_timeout = 5.0  # shorter timeout for high volume
                timeout = 10.0
                
                async with client.AsyncAPIClient(
                    login=settings.SMS_GATEWAY_LOGIN,
                    password=settings.SMS_GATEWAY_PASSWORD,
                    base_url=settings.SMS_GATEWAY_URL,
                    timeout=timeout,
                    connect_timeout=connect_timeout
                ) as sms_client:
                    # Build message
                    message_params = {
                        "id": custom_id,
                        "message": message_text,
                        "phone_numbers": [phone_number],
                        "with_delivery_report": True,
                    }
                    
                    # Add optional parameters if provided
                    if ttl is not None:
                        message_params["ttl"] = ttl
                    if sim_number is not None:
                        message_params["sim_number"] = sim_number
                    if is_encrypted:
                        message_params["is_encrypted"] = True
                    
                    # Create message object
                    message = domain.Message(**message_params)
                    
                    # Send with timing metrics for performance monitoring
                    start_time = time.time()
                    response = await sms_client.send(message)
                    elapsed = time.time() - start_time
                    
                    # Log timing for performance monitoring
                    if elapsed > 1.0:
                        logger.warning(f"Slow gateway response: {elapsed:.2f}s for message to {phone_number}")
                    
                    # Check for errors in recipients
                    recipient_state = response.recipients[0] if response.recipients else None
                    if recipient_state and recipient_state.error:
                        raise SMSGatewayError(message=recipient_state.error)
                    
                    # Extract status
                    status = str(response.state.value).lower() if hasattr(response, 'state') else MessageStatus.PENDING
                    gateway_id = getattr(response, 'id', None)
                    
                    return {
                        "status": status,
                        "gateway_message_id": gateway_id,
                        "phone_number": phone_number,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
            except HTTPStatusError as e:
                if e.response.status_code == 401:
                    logger.error("❌ Invalid SMS gateway credentials (401 Unauthorized)")
                    raise SMSAuthError()
                
                # High volume optimization: determine if error is retryable
                retryable_status_codes = [429, 503, 504]
                if e.response.status_code in retryable_status_codes:
                    retry_after = int(e.response.headers.get('Retry-After', "30"))
                    raise RetryableError(
                        message=f"Gateway rate limiting or temporary unavailability (status={e.response.status_code})",
                        retry_after=retry_after
                    )
                    
                raise SMSGatewayError(message=f"SMS gateway error: {str(e)}")

            except Exception as e:
                logger.error(f"Unexpected gateway exception: {type(e).__name__}: {str(e)}")

                # Improved retryable error detection for high-volume systems
                retryable_exceptions = ["ConnectionError", "Timeout", "CancelledError", "ServiceUnavailable"]
                if any(ex_type in str(type(e)) for ex_type in retryable_exceptions):
                    raise RetryableError(
                        message=f"Temporary SMS gateway issue: {str(e)}",
                        retry_after=30
                    )

                raise SMSGatewayError(message=f"SMS gateway error: {str(e)}")

    async def _enforce_rate_limit(self) -> None:
        """
        Enforce rate limiting for SMS sending.
        
        Adds dynamic delay based on settings.DELAY_BETWEEN_SMS.
        """
        current_time = asyncio.get_event_loop().time()
        elapsed = current_time - self._last_send_time
        remaining_delay = max(0, settings.DELAY_BETWEEN_SMS - elapsed)
        
        if remaining_delay > 0:
            await asyncio.sleep(remaining_delay)
        
        self._last_send_time = asyncio.get_event_loop().time()

    async def send_with_template(
        self,
        *,
        template_id: str,
        phone_number: str,
        variables: Dict[str, str],
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        custom_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a message using a template.
        
        Args:
            template_id: Template ID
            phone_number: Recipient phone number
            variables: Dictionary of variable values
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            custom_id: Optional custom ID for tracking
            
        Returns:
            Dict: Message details with status
            
        Raises:
            ValidationError: If phone number is invalid or template is not found
            SMSGatewayError: If there's an error sending the message
        """
        # Use context manager for template repository
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            if not template:
                raise ValidationError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != user_id:
                raise ValidationError(message="Not authorized to use this template")
            
            # Apply template
            message_text = await template_repo.apply_template(
                template_id=template_id,
                variables=variables
            )
            
            # Check for missing variables
            import re
            missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", message_text)
            if missing_vars:
                raise ValidationError(
                    message="Missing template variables", 
                    details={"missing_variables": missing_vars}
                )
        
        # Send message
        metadata = {
            "template_id": template_id,
            "template_variables": variables
        }
        
        return await self.send_message(
            phone_number=phone_number,
            message_text=message_text,
            user_id=user_id,
            scheduled_at=scheduled_at,
            custom_id=custom_id,
            metadata=metadata
        )

    async def send_batch_with_template(
        self,
        *,
        template_id: str,
        recipients: List[Dict[str, Any]],
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        options: Optional[BatchOptions] = None
    ) -> Dict[str, Any]:
        """
        Send a batch of messages using a template.
        
        Args:
            template_id: Template ID
            recipients: List of recipients with their variables
                    Each recipient should have 'phone_number' and 'variables' keys
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            options: Optional batch processing options
            
        Returns:
            Dict: Batch details with status
            
        Raises:
            ValidationError: If template is not found or recipients format is invalid
            SMSGatewayError: If there's an error sending the messages
        """
        # Use context manager for template repository
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            if not template:
                raise ValidationError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != user_id:
                raise ValidationError(message="Not authorized to use this template")
            
            # Validate recipients format
            for idx, recipient in enumerate(recipients):
                if "phone_number" not in recipient:
                    raise ValidationError(message=f"Recipient at index {idx} is missing 'phone_number'")
                if "variables" not in recipient:
                    raise ValidationError(message=f"Recipient at index {idx} is missing 'variables'")
            
            # Create messages for each recipient
            messages = []
            for recipient in recipients:
                # Apply template for each recipient
                message_text = await template_repo.apply_template(
                    template_id=template_id,
                    variables=recipient["variables"]
                )
                
                # Check for missing variables
                import re
                missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", message_text)
                if missing_vars:
                    # Skip this recipient but continue with others
                    continue
                
                # Create message
                messages.append(
                    MessageCreate(
                        phone_number=recipient["phone_number"],
                        message=message_text,
                        scheduled_at=scheduled_at,
                        custom_id=recipient.get("custom_id")
                    )
                )
            
            if not messages:
                raise ValidationError(message="No valid recipients found after applying templates")
        
        # Create batch metadata
        batch_metadata = {
            "template_id": template_id,
            "recipients_count": len(recipients),
            "messages_count": len(messages)
        }
        
        # Use standard batch sending
        batch_result = await self.send_batch(
            messages=messages,
            user_id=user_id,
            options=options
        )
        
        # Add template info to result
        batch_result["template_id"] = template_id
        batch_result["template_name"] = template.name
        
        return batch_result

# Dependency injection function
async def get_sms_sender() -> SMSSender:
    """
    Dependency injection provider for SMSSender.
    
    Returns:
        SMSSender: An instance of the SMS sending service with event bus.
    """ 
    from app.services.event_bus.bus import get_event_bus
    event_bus = get_event_bus()    
    return SMSSender(event_bus)
</file>

<file path="app/db/repositories/messages.py">
"""
Message repository for database operations related to SMS messages.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any, Tuple
from uuid import uuid4

from sqlalchemy import select, update, delete, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.exc import IntegrityError

import logging
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.models.campaign import Campaign
from app.db.repositories.base import BaseRepository
from app.models.message import Message, MessageEvent, MessageBatch, MessageTemplate
from app.schemas.message import MessageCreate, MessageStatus

logger = logging.getLogger("inboxerr.db")

class MessageRepository(BaseRepository[Message, MessageCreate, Dict[str, Any]]):
    """Message repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and Message model."""
        super().__init__(session=session, model=Message)

    async def create_message(
        self,
        *,
        phone_number: str,
        message_text: str,
        user_id: str,
        custom_id: Optional[str] = None,
        scheduled_at: Optional[datetime] = None,
        metadata: Optional[Dict[str, Any]] = None,
        batch_id: Optional[str] = None,
        campaign_id: Optional[str] = None
    ) -> Message:
        """
        Create a new message with related objects in a single transaction.
        """
        # Set initial status based on scheduling
        initial_status = MessageStatus.SCHEDULED if scheduled_at else MessageStatus.PENDING
        
        # Calculate SMS parts
        parts_count = (len(message_text) + 159) // 160  # 160 chars per SMS part
        
        # Generate IDs upfront
        message_id = generate_prefixed_id(IDPrefix.MESSAGE)
        event_id = generate_prefixed_id(IDPrefix.EVENT)
        
        # Create message instance
        message = Message(
            id=message_id,  # Pre-assign ID
            custom_id=custom_id or str(uuid4()),
            phone_number=phone_number,
            message=message_text,
            status=initial_status,
            scheduled_at=scheduled_at,
            user_id=user_id,
            meta_data=metadata or {},
            parts_count=parts_count,
            batch_id=batch_id,
            campaign_id=campaign_id
        )
        
        # Create event instance with pre-assigned message_id
        event = MessageEvent(
            id=event_id,
            message_id=message_id,
            event_type="created",
            status=initial_status,
            data={
                "phone_number": phone_number,
                "scheduled_at": scheduled_at.isoformat() if scheduled_at else None,
                "campaign_id": campaign_id
            }
        )
        
        # Begin transaction using savepoint if a transaction is already active
        async with self.session.begin_nested():
            # Add both objects to session
            self.session.add(message)
            self.session.add(event)
            
            # Handle campaign update if needed
            if campaign_id:
                try:
                    # Execute campaign update with direct SQL for better performance in high volume
                    await self.session.execute(
                        update(Campaign)
                        .where(Campaign.id == campaign_id)
                        .values(total_messages=Campaign.total_messages + 1)
                    )
                except Exception as e:
                    # Log but don't fail the message creation if campaign update fails
                    logger.error(f"Error updating campaign {campaign_id} message count: {e}")
        
        # Commit the outer transaction
        await self.session.commit()
        
        # Refresh message
        await self.session.refresh(message)
        
        return message

    async def update_message_status(
        self,
        *,
        message_id: str,
        status: str,
        event_type: str,
        reason: Optional[str] = None,
        gateway_message_id: Optional[str] = None,
        data: Optional[Dict[str, Any]] = None
    ) -> Optional[Message]:
        """
        Update message status with improved concurrency handling.
        
        Args:
            message_id: Message ID
            status: New status
            event_type: Event type triggering this update
            reason: Optional reason for status change
            gateway_message_id: Optional gateway message ID
            data: Optional additional data
            
        Returns:
            Message: Updated message or None
        """
        # Get the message first to check if it exists
        message = await self.get_by_id(message_id)
        if not message:
            return None
        
        # Update status-specific timestamp
        now = datetime.now(timezone.utc)
        update_data = {
            "status": status,
            "updated_at": now,
        }
        
        if status == MessageStatus.SENT:
            update_data["sent_at"] = now
        elif status == MessageStatus.DELIVERED:
            update_data["delivered_at"] = now
        elif status == MessageStatus.FAILED:
            update_data["failed_at"] = now
            update_data["reason"] = reason
        
        # Set gateway message ID if provided
        if gateway_message_id:
            update_data["gateway_message_id"] = gateway_message_id
        
        # Set a specific savepoint for this operation
        async with self.session.begin_nested() as nested:
            try:
                # Update the message
                await self.session.execute(
                    update(Message)
                    .where(Message.id == message_id)
                    .values(**update_data)
                )
                
                # Create event for status change with unique ID
                event_id = generate_prefixed_id(IDPrefix.EVENT)  # Generate new ID for each event
                event = MessageEvent(
                    id=event_id,
                    message_id=message_id,
                    event_type=event_type,
                    status=status,
                    data=data or {}
                )
                
                # Add event to session
                self.session.add(event)
                
                # Commit the nested transaction
                await nested.commit()
            except Exception as e:
                # Transaction will automatically roll back on exception
                logger.error(f"Error updating message status: {e}")
                return None
        
        # Complete the outer transaction
        await self.session.commit()
        
        # Refresh the message
        await self.session.refresh(message)
        
        return message

    async def create_batch(
        self,
        *,
        user_id: str,
        name: str,
        total: int
    ) -> MessageBatch:
        """
        Create a new message batch.
        
        Args:
            user_id: User ID
            name: Batch name
            total: Total number of messages
            
        Returns:
            MessageBatch: Created batch
        """
        batch_id = generate_prefixed_id(IDPrefix.BATCH)
        batch = MessageBatch(
            id=batch_id,
            name=name,
            total=total,
            processed=0,
            successful=0,
            failed=0,
            status=MessageStatus.PENDING,
            user_id=user_id
        )
        
        async with self.session.begin():
            self.session.add(batch)
        
        await self.session.refresh(batch)
        
        return batch

    async def update_batch_progress(
        self,
        *,
        batch_id: str,
        increment_processed: int = 0,
        increment_successful: int = 0,
        increment_failed: int = 0,
        status: Optional[str] = None
    ) -> Optional[MessageBatch]:
        """
        Update batch progress with proper transaction handling.
        
        Args:
            batch_id: Batch ID
            increment_processed: Increment processed count
            increment_successful: Increment successful count
            increment_failed: Increment failed count
            status: Optional new status
            
        Returns:
            MessageBatch: Updated batch or None
        """
        # Use a proper transaction
        async with self.session.begin():
            # Get batch with FOR UPDATE to prevent race conditions
            query = select(MessageBatch).where(MessageBatch.id == batch_id)
            result = await self.session.execute(query.with_for_update())
            batch = result.scalar_one_or_none()
            
            if not batch:
                return None
            
            # Update counts
            batch.processed += increment_processed
            batch.successful += increment_successful
            batch.failed += increment_failed
            
            # Update status if provided
            if status:
                batch.status = status
                
            # If all messages processed, update status and completion time
            if batch.processed >= batch.total:
                batch.status = MessageStatus.PROCESSED if batch.failed == 0 else "partial"
                batch.completed_at = datetime.now(timezone.utc)
            
            # Add batch to session
            self.session.add(batch)
        
        # Get updated batch
        query = select(MessageBatch).where(MessageBatch.id == batch_id)
        result = await self.session.execute(query)
        updated_batch = result.scalar_one_or_none()
        
        return updated_batch

    async def update_batch_progress_safe(
        self,
        *,
        batch_id: str,
        increment_processed: int = 0,
        increment_successful: int = 0,
        increment_failed: int = 0,
        status: Optional[str] = None
    ) -> Optional[MessageBatch]:
        """
        Update batch progress with proper transaction handling.
        
        Args:
            batch_id: Batch ID
            increment_processed: Increment processed count
            increment_successful: Increment successful count
            increment_failed: Increment failed count
            status: Optional new status
            
        Returns:
            MessageBatch: Updated batch or None
        """
        # Get a completely fresh session for this operation
        from app.db.session import async_session_factory
        
        async with async_session_factory() as fresh_session:
            try:
                async with fresh_session.begin():
                    # Get batch with SELECT FOR UPDATE to prevent race conditions
                    query = select(MessageBatch).where(MessageBatch.id == batch_id)
                    result = await fresh_session.execute(query.with_for_update())
                    batch = result.scalar_one_or_none()
                    
                    if not batch:
                        return None
                    
                    # Update counts
                    batch.processed += increment_processed
                    batch.successful += increment_successful
                    batch.failed += increment_failed
                    
                    # Update status if provided
                    if status:
                        batch.status = status
                        
                    # If all messages processed, update status and completion time
                    if batch.processed >= batch.total:
                        batch.status = MessageStatus.PROCESSED if batch.failed == 0 else "partial"
                        batch.completed_at = datetime.now(timezone.utc)
                    
                    # Add the updated batch to the session
                    fresh_session.add(batch)
                    
                    # Get updated batch after updates are applied
                    # This is automatically refreshed at transaction commit
                    
                # Now outside the transaction, we can safely refresh
                query = select(MessageBatch).where(MessageBatch.id == batch_id)
                result = await fresh_session.execute(query)
                updated_batch = result.scalar_one_or_none()
                    
                return updated_batch
            
            except Exception as e:
                logger.error(f"Error in update_batch_progress_safe: {e}")
                return None
            finally:
                # Explicitly close session to prevent connection leaks
                await fresh_session.close()

    async def get_by_custom_id(self, custom_id: str) -> Optional[Message]:
        """
        Get message by custom ID.
        
        Args:
            custom_id: Custom ID
            
        Returns:
            Message: Found message or None
        """
        return await self.get_by_attribute("custom_id", custom_id)

    async def get_by_gateway_id(self, gateway_id: str) -> Optional[Message]:
        """
        Get message by gateway ID.
        
        Args:
            gateway_id: Gateway message ID
            
        Returns:
            Message: Found message or None
        """
        return await self.get_by_attribute("gateway_message_id", gateway_id)

    async def list_messages_for_user(
        self,
        *,
        user_id: str,
        status: Optional[str] = None,
        phone_number: Optional[str] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        campaign_id: Optional[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Message], int]:
        """
        List messages for user with filtering.
        
        Args:
            user_id: User ID
            status: Optional status filter
            phone_number: Optional phone number filter
            from_date: Optional from date filter
            to_date: Optional to date filter
            campaign_id: Optional campaign ID filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Message], int]: List of messages and total count
        """
        # Base query
        query = (
            select(Message)
            .options(
                joinedload(Message.campaign),       # Eager load campaign
                selectinload(Message.events)        # Eager load events if needed
            )
            .where(Message.user_id == user_id)
        )
        count_query = select(func.count()).select_from(Message).where(Message.user_id == user_id)
        
        # Apply filters
        if status:
            query = query.where(Message.status == status)
            count_query = count_query.where(Message.status == status)
        
        if phone_number:
            query = query.where(Message.phone_number == phone_number)
            count_query = count_query.where(Message.phone_number == phone_number)

        if campaign_id:
            query = query.where(Message.campaign_id == campaign_id)
            count_query = count_query.where(Message.campaign_id == campaign_id)
        
        if from_date:
            try:
                from_date_obj = datetime.fromisoformat(from_date.replace('Z', '+00:00'))
                query = query.where(Message.created_at >= from_date_obj)
                count_query = count_query.where(Message.created_at >= from_date_obj)
            except ValueError:
                pass
        
        if to_date:
            try:
                to_date_obj = datetime.fromisoformat(to_date.replace('Z', '+00:00'))
                query = query.where(Message.created_at <= to_date_obj)
                count_query = count_query.where(Message.created_at <= to_date_obj)
            except ValueError:
                pass
        
        # Order by created_at desc
        query = query.order_by(desc(Message.created_at))
        
        # Pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        messages = result.scalars().all()
        total = count_result.scalar_one()
        
        return messages, total

    async def get_messages_for_batch(
        self,
        *,
        batch_id: str,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Message], int]:
        """
        Get messages for a batch.
        
        Args:
            batch_id: Batch ID
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Message], int]: List of messages and total count
        """
        # Base query
        query = (
            select(Message)
            .options(joinedload(Message.campaign))
            .where(Message.batch_id == batch_id)
        )
        count_query = select(func.count()).select_from(Message).where(Message.batch_id == batch_id)
        
        # Order by created_at desc
        query = query.order_by(desc(Message.created_at))
        
        # Pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        messages = result.scalars().all()
        total = count_result.scalar_one()
        
        return messages, total
    
    async def get_messages_for_campaign(
        self,
        *,
        campaign_id: str,
        status: Optional[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Message], int]:
        """
        Get messages for a campaign.
        
        Args:
            campaign_id: Campaign ID
            status: Optional status filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Message], int]: List of messages and total count
        """
        # Base query
        query = (
            select(Message)
            .options(joinedload(Message.campaign))  # This fixes the DetachedInstanceError
            .where(Message.campaign_id == campaign_id)
        )
        count_query = select(func.count()).select_from(Message).where(Message.campaign_id == campaign_id)
        
        # Apply status filter
        if status:
            query = query.where(Message.status == status)
            count_query = count_query.where(Message.status == status)
        
        # Order by created_at desc
        query = query.order_by(desc(Message.created_at))
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        messages = result.scalars().all()
        total = count_result.scalar_one()
        
        return messages, total
    
    async def get_by_id(self, id: str) -> Optional[Message]:
        """
        Get a message by ID with eager loading of relationships.
        
        This overrides the BaseRepository method to add eager loading
        for campaign and events relationships.
        
        Args:
            id: Message ID
            
        Returns:
            Message: Found message with relationships loaded, or None
        """
        query = (
            select(Message)
            .options(
                joinedload(Message.campaign),    # Eager load campaign
                selectinload(Message.events)     # Eager load events collection
            )
            .where(Message.id == id)
        )
        
        result = await self.session.execute(query)
        return result.scalars().first()

    async def get_retryable_messages(
        self,
        *,
        max_retries: int = 3,
        limit: int = 50
    ) -> List[Message]:
        """
        Get messages that can be retried.
        
        Args:
            max_retries: Maximum number of retry attempts
            limit: Maximum number of messages to return
            
        Returns:
            List[Message]: List of retryable messages
        """
        # Query for failed messages that can be retried
        query = select(Message).where(
            and_(
                Message.status == MessageStatus.FAILED,
                or_(
                    Message.meta_data.is_(None),  # No metadata at all
                    ~Message.meta_data.contains({"retry_count": max_retries})  # retry_count less than max
                )
            )
        ).order_by(Message.failed_at).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def check_events_for_messages(
        self,
        *,
        message_ids: List[str],
        user_id: str
    ) -> Tuple[int, List[str]]:
        """
        Check how many delivery events exist for given messages.
        
        This method performs a security-conscious check to count events only for messages
        that belong to the specified user, preventing information leakage about other
        users' messages.
        
        Args:
            message_ids: List of message IDs to check for events
            user_id: User ID for security validation
            
        Returns:
            Tuple[int, List[str]]: (total_events_count, message_ids_with_events)
            - total_events_count: Total number of events that would be deleted
            - message_ids_with_events: List of message IDs that have associated events
            
        Security:
            - Only counts events for messages owned by the specified user
            - Prevents cross-user information disclosure
            
        Performance:
            - Uses efficient subquery for user validation
            - Single database round-trip for both counts and message list
        """
        from app.models.message import MessageEvent
        
        # Count total events for user's messages
        events_count_query = select(func.count(MessageEvent.id)).where(
            and_(
                MessageEvent.message_id.in_(message_ids),
                # Security: ensure messages belong to user
                MessageEvent.message_id.in_(
                    select(Message.id).where(
                        and_(
                            Message.id.in_(message_ids),
                            Message.user_id == user_id
                        )
                    )
                )
            )
        )
        
        # Get distinct message IDs that have events (for user's messages only)
        messages_with_events_query = select(MessageEvent.message_id.distinct()).where(
            and_(
                MessageEvent.message_id.in_(message_ids),
                # Security: ensure messages belong to user
                MessageEvent.message_id.in_(
                    select(Message.id).where(
                        and_(
                            Message.id.in_(message_ids),
                            Message.user_id == user_id
                        )
                    )
                )
            )
        )
        
        # Execute both queries
        events_result = await self.session.execute(events_count_query)
        messages_result = await self.session.execute(messages_with_events_query)
        
        total_events = events_result.scalar_one()
        messages_with_events = [row[0] for row in messages_result.fetchall()]
        
        logger.debug(
            f"Event check for user {user_id}: {total_events} events found "
            f"across {len(messages_with_events)} messages"
        )
        
        return total_events, messages_with_events
    

    async def bulk_delete_with_batching(
        self,
        *,
        message_ids: List[str],
        user_id: str,
        force_delete: bool = False,
        batch_size: int = 1000
    ) -> Tuple[int, List[str], Dict[str, Any]]:
        """
        Bulk delete messages with batching for server stability and event safety.
        
        This method processes large deletion operations in smaller batches to prevent
        server overload, connection timeouts, and database lock contention. It handles
        both safe deletion (messages without events) and force deletion (with events).
        
        Args:
            message_ids: List of message IDs to delete
            user_id: User ID for authorization
            force_delete: Whether to delete messages that have delivery events
            batch_size: Number of messages to process per batch (max 5000)
            
        Returns:
            Tuple[int, List[str], Dict[str, Any]]: (total_deleted, failed_ids, batch_info)
            - total_deleted: Total number of messages successfully deleted
            - failed_ids: List of message IDs that failed to delete
            - batch_info: Dictionary with batch processing statistics
            
        Server Stability Features:
            - Processes deletions in configurable batch sizes
            - Includes inter-batch delays to prevent DB overload
            - Each batch is atomic (all succeed or all fail per batch)
            - Graceful handling of partial failures
            
        Event Safety:
            - When force_delete=False: Only deletes messages without events
            - When force_delete=True: Deletes events first, then messages
            - Two-phase deletion prevents foreign key violations
        """
        import asyncio
        
        total_deleted = 0
        all_failed_ids = []
        batches_processed = 0
        events_deleted_total = 0
        
        # Validate batch size
        if batch_size > 5000:
            batch_size = 5000
            logger.warning(f"Batch size capped at 5000 for stability")
        
        logger.info(
            f"Starting batched deletion: {len(message_ids)} messages, "
            f"batch_size={batch_size}, force_delete={force_delete}, user={user_id}"
        )
        
        # Process in batches
        for i in range(0, len(message_ids), batch_size):
            batch_ids = message_ids[i:i + batch_size]
            batches_processed += 1
            
            try:
                # Process this batch
                if force_delete:
                    deleted, failed, events_deleted = await self._delete_batch_with_events(
                        batch_ids, user_id
                    )
                    events_deleted_total += events_deleted
                else:
                    deleted, failed = await self._delete_batch_safe(batch_ids, user_id)
                
                total_deleted += deleted
                all_failed_ids.extend(failed)
                
                logger.debug(
                    f"Batch {batches_processed}: deleted {deleted}, failed {len(failed)}"
                )
                
                # Small delay between batches to prevent overwhelming DB
                if i + batch_size < len(message_ids):  # Don't delay after last batch
                    await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Batch {batches_processed} failed completely: {e}")
                # Add all IDs from failed batch to failed list
                all_failed_ids.extend(batch_ids)
        
        batch_info = {
            "batches_processed": batches_processed,
            "batch_size": batch_size,
            "total_messages": len(message_ids),
            "events_deleted": events_deleted_total
        }
        
        logger.info(
            f"Batched deletion completed: {total_deleted} deleted, "
            f"{len(all_failed_ids)} failed, {batches_processed} batches, "
            f"{events_deleted_total} events deleted"
        )
        
        return total_deleted, all_failed_ids, batch_info
    

    async def _delete_batch_safe(
        self,
        message_ids: List[str],
        user_id: str
    ) -> Tuple[int, List[str]]:
        """
        Delete a batch of messages safely (only messages without delivery events).
        
        This method performs safe deletion by only removing messages that have no
        associated delivery events. Messages with events are skipped and returned
        in the failed list, allowing the caller to handle them appropriately.
        
        Args:
            message_ids: List of message IDs to delete in this batch
            user_id: User ID for authorization
            
        Returns:
            Tuple[int, List[str]]: (deleted_count, failed_message_ids)
            - deleted_count: Number of messages successfully deleted
            - failed_message_ids: List of message IDs that have events (skipped)
            
        Safety Features:
            - Only deletes messages without delivery events
            - Preserves delivery tracking data by default
            - Atomic transaction per batch
            - User authorization on every message
            
        Performance:
            - Single transaction per batch
            - Efficient subquery to identify safe messages
            - Minimal database round-trips
        """
        from app.models.message import MessageEvent
        
        try:
            # Work within existing transaction - don't start a new one
            # Find messages that have NO events (safe to delete)
            messages_without_events_query = select(Message.id).where(
                and_(
                    Message.id.in_(message_ids),
                    Message.user_id == user_id,
                    # Only messages with NO events
                    ~Message.id.in_(
                        select(MessageEvent.message_id.distinct()).where(
                            MessageEvent.message_id.in_(message_ids)
                        )
                    )
                )
            )
            
            result = await self.session.execute(messages_without_events_query)
            safe_message_ids = [row[0] for row in result.fetchall()]
            
            if not safe_message_ids:
                # All messages have events - none can be safely deleted
                return 0, message_ids
            
            # Delete only the safe messages
            delete_query = delete(Message).where(
                and_(
                    Message.id.in_(safe_message_ids),
                    Message.user_id == user_id
                )
            )
            
            delete_result = await self.session.execute(delete_query)
            deleted_count = delete_result.rowcount
            
            # Messages that couldn't be deleted (have events)
            failed_ids = [mid for mid in message_ids if mid not in safe_message_ids]
            
            if failed_ids:
                logger.debug(
                    f"Safe deletion: {deleted_count} deleted, {len(failed_ids)} skipped (have events)"
                )
            else:
                logger.debug(f"Safe deletion: {deleted_count} deleted, no events found")
            
            return deleted_count, failed_ids
                
        except Exception as e:
            logger.error(f"Safe batch deletion failed: {e}")
            return 0, message_ids  # All IDs failed
    

    async def _delete_batch_with_events(
        self,
        message_ids: List[str],
        user_id: str
    ) -> Tuple[int, List[str], int]:
        """
        Delete a batch of messages WITH their delivery events (force deletion).
        
        This method performs force deletion by removing both messages and their
        associated delivery events in a two-phase process. Events are deleted first
        to avoid foreign key constraint violations, then messages are deleted.
        
        Args:
            message_ids: List of message IDs to delete in this batch
            user_id: User ID for authorization
            
        Returns:
            Tuple[int, List[str], int]: (deleted_count, failed_message_ids, events_deleted)
            - deleted_count: Number of messages successfully deleted
            - failed_message_ids: List of message IDs that failed to delete
            - events_deleted: Number of delivery events deleted
            
        Force Deletion Process:
            1. Delete all delivery events for the messages (prevents FK violations)
            2. Delete the messages themselves
            3. Both operations in single atomic transaction
            
        Data Loss Warning:
            - This permanently destroys delivery tracking data
            - Should only be used when explicitly confirmed by user
            - May impact delivery analytics and compliance records
            
        Performance:
            - Two-phase deletion in single transaction
            - Efficient bulk operations with IN clauses
            - User authorization on every operation
        """
        from app.models.message import MessageEvent
        
        try:
            # Work within existing transaction - don't start a new one
            # Phase 1: Delete events first (to avoid FK constraint violations)
            events_delete_query = delete(MessageEvent).where(
                and_(
                    MessageEvent.message_id.in_(message_ids),
                    # Security: only delete events for user's messages
                    MessageEvent.message_id.in_(
                        select(Message.id).where(
                            and_(
                                Message.id.in_(message_ids),
                                Message.user_id == user_id
                            )
                        )
                    )
                )
            )
            
            events_result = await self.session.execute(events_delete_query)
            events_deleted = events_result.rowcount
            
            # Phase 2: Delete messages
            messages_delete_query = delete(Message).where(
                and_(
                    Message.id.in_(message_ids),
                    Message.user_id == user_id
                )
            )
            
            messages_result = await self.session.execute(messages_delete_query)
            messages_deleted = messages_result.rowcount
            
            # Determine which messages failed to delete
            if messages_deleted < len(message_ids):
                # Query to find which messages still exist (failed to delete)
                remaining_query = select(Message.id).where(
                    and_(
                        Message.id.in_(message_ids),
                        Message.user_id == user_id
                    )
                )
                
                remaining_result = await self.session.execute(remaining_query)
                remaining_ids = [row[0] for row in remaining_result.fetchall()]
                failed_ids = remaining_ids
            else:
                failed_ids = []
            
            logger.info(
                f"Force deletion batch: {messages_deleted} messages deleted, "
                f"{events_deleted} events deleted, {len(failed_ids)} failed"
            )
            
            return messages_deleted, failed_ids, events_deleted
                
        except Exception as e:
            logger.error(f"Force deletion batch failed: {e}")
            return 0, message_ids, 0  # All IDs failed, no events deleted
    
    
    async def bulk_delete_campaign_messages(
        self,
        *,
        campaign_id: str,
        user_id: str,
        status: Optional[str] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit: int = 10000,
        force_delete: bool = False,
        batch_size: int = 1000
    ) -> Tuple[int, List[str], Dict[str, Any]]:
        """
        Bulk delete messages for a campaign with event safety and server stability.
        
        This method efficiently deletes multiple messages belonging to a specific campaign
        with optional filtering by status and date range. Enhanced with event safety
        checking and batched processing for server stability under high load.
        
        Args:
            campaign_id: Campaign ID - messages must belong to this campaign
            user_id: User ID - for authorization (messages must belong to this user)
            status: Optional status filter (e.g., 'failed', 'sent')
            from_date: Optional from date filter (ISO format string)
            to_date: Optional to date filter (ISO format string)
            limit: Maximum number of messages to delete (default 10K, max 10K for safety)
            force_delete: Whether to delete messages that have delivery events
            batch_size: Number of messages to process per batch for stability
            
        Returns:
            Tuple[int, List[str], Dict[str, Any]]: (deleted_count, failed_ids, metadata)
            - deleted_count: Number of messages successfully deleted
            - failed_ids: List of message IDs that failed to delete
            - metadata: Dictionary with operation details including:
                - requires_confirmation: Whether force delete is needed
                - events_count: Number of events that would be deleted
                - events_deleted: Number of events actually deleted
                - batch_info: Batch processing statistics
                - safety_warnings: List of safety warnings
            
        Event Safety:
            - When force_delete=False: Returns safety warning if events exist
            - When force_delete=True: Deletes both messages and events
            - Two-phase deletion prevents foreign key violations
            
        Server Stability:
            - Processes large operations in configurable batches
            - Includes delays between batches to prevent DB overload
            - Graceful handling of partial failures
            
        Performance:
            - Handles 30K deletions efficiently with batching
            - Uses optimized queries with proper indexes
            - Single transaction per batch for consistency
        """
        # Validate limit for safety
        if limit > 10000:
            limit = 10000
        
        # Build query to get message IDs that match criteria
        id_subquery = (
            select(Message.id)
            .where(
                and_(
                    Message.campaign_id == campaign_id,
                    Message.user_id == user_id
                )
            )
            .limit(limit)
        )
        
        # Apply optional filters
        if status:
            id_subquery = id_subquery.where(Message.status == status)
        
        if from_date:
            try:
                from_date_obj = datetime.fromisoformat(from_date.replace('Z', '+00:00'))
                id_subquery = id_subquery.where(Message.created_at >= from_date_obj)
            except ValueError:
                pass
        
        if to_date:
            try:
                to_date_obj = datetime.fromisoformat(to_date.replace('Z', '+00:00'))
                id_subquery = id_subquery.where(Message.created_at <= to_date_obj)
            except ValueError:
                pass
        
        try:
            # Get the message IDs that match criteria
            id_result = await self.session.execute(id_subquery)
            message_ids = [row[0] for row in id_result.fetchall()]
            
            if not message_ids:
                return 0, [], {
                    "requires_confirmation": False,
                    "events_count": 0,
                    "events_deleted": 0,
                    "batch_info": {"batches_processed": 0, "batch_size": batch_size},
                    "safety_warnings": []
                }
            
            # Check for events if not force deleting
            if not force_delete:
                events_count, messages_with_events = await self.check_events_for_messages(
                    message_ids=message_ids,
                    user_id=user_id
                )
                
                if events_count > 0:
                    # Return safety warning instead of proceeding
                    return 0, [], {
                        "requires_confirmation": True,
                        "events_count": events_count,
                        "events_deleted": 0,
                        "batch_info": {"batches_processed": 0, "batch_size": batch_size},
                        "safety_warnings": [
                            f"Cannot delete {len(message_ids)} message(s) because {events_count} delivery/status events exist.",
                            "Please confirm deletion to remove both message(s) and all associated events."
                        ]
                    }
            
            # Proceed with deletion using batching
            deleted_count, failed_ids, batch_info = await self.bulk_delete_with_batching(
                message_ids=message_ids,
                user_id=user_id,
                force_delete=force_delete,
                batch_size=batch_size
            )
            
            logger.info(
                f"Campaign bulk delete completed: campaign={campaign_id}, "
                f"deleted={deleted_count}, failed={len(failed_ids)}, "
                f"force_delete={force_delete}, batches={batch_info.get('batches_processed', 0)}"
            )
            
            return deleted_count, failed_ids, {
                "requires_confirmation": False,
                "events_count": 0,
                "events_deleted": batch_info.get("events_deleted", 0),
                "batch_info": batch_info,
                "safety_warnings": []
            }
            
        except Exception as e:
            logger.error(f"Error in bulk_delete_campaign_messages: {e}")
            raise e

    
    async def bulk_delete_messages(
        self,
        *,
        message_ids: List[str],
        user_id: str,
        campaign_id: Optional[str] = None,
        force_delete: bool = False
    ) -> Tuple[int, List[str], Dict[str, Any]]:
        """
        Global bulk delete messages by message IDs with event safety.
        
        This method efficiently deletes multiple messages by their specific IDs with
        user authorization and event safety checking. Designed for edge cases like
        cross-campaign cleanup, orphaned message removal, and power user operations.
        
        Args:
            message_ids: List of message IDs to delete (max 1000 for safety)
            user_id: User ID - for authorization (messages must belong to this user)
            campaign_id: Optional campaign context for additional validation
            force_delete: Whether to delete messages that have delivery events
            
        Returns:
            Tuple[int, List[str], Dict[str, Any]]: (deleted_count, failed_ids, metadata)
            - deleted_count: Number of messages successfully deleted
            - failed_ids: List of message IDs that failed to delete
            - metadata: Dictionary with operation details including:
                - requires_confirmation: Whether force delete is needed
                - events_count: Number of events that would be deleted
                - events_deleted: Number of events actually deleted
                - safety_warnings: List of safety warnings
            
        Event Safety:
            - When force_delete=False: Returns safety warning if events exist
            - When force_delete=True: Deletes both messages and events
            - Two-phase deletion prevents foreign key violations
            
        Use Cases:
            - Cross-campaign message cleanup by power users
            - Orphaned message removal during system maintenance
            - Selective message deletion from UI multi-select
            - Compliance-driven deletion by specific message IDs
            
        Performance:
            - Handles up to 1K deletions efficiently
            - Uses IN clause with message ID list
            - Smaller batches for safety vs campaign-scoped operations
        """

        # Validate input
        if not message_ids:
            return 0, [], {
                "requires_confirmation": False,
                "events_count": 0,
                "events_deleted": 0,
                "safety_warnings": []
            }
        
        # Safety limit - smaller than campaign-scoped for global operations
        if len(message_ids) > 1000:
            logger.warning(f"Global bulk delete limited to 1000 messages, received {len(message_ids)}")
            message_ids = message_ids[:1000]
        
        # Remove duplicates while preserving order
        unique_message_ids = list(dict.fromkeys(message_ids))
        
        try:
            # Check for events if not force deleting
            if not force_delete:
                events_count, messages_with_events = await self.check_events_for_messages(
                    message_ids=unique_message_ids,
                    user_id=user_id
                )
                
                if events_count > 0:
                    # Return safety warning instead of proceeding
                    return 0, [], {
                        "requires_confirmation": True,
                        "events_count": events_count,
                        "events_deleted": 0,
                        "safety_warnings": [
                            f"Cannot delete {len(unique_message_ids)} message(s) because {events_count} delivery/status events exist.",
                            "Please confirm deletion to remove both message(s) and all associated events."
                        ]
                    }
            
            # Proceed with deletion (using smaller batches for global operations)
            deleted_count, failed_ids, batch_info = await self.bulk_delete_with_batching(
                message_ids=unique_message_ids,
                user_id=user_id,
                force_delete=force_delete,
                batch_size=500  # Smaller batches for global operations
            )
            
            # Additional campaign context validation for failed messages
            if campaign_id and failed_ids:
                # Filter failed IDs to only those that actually belong to the campaign
                campaign_failed_query = select(Message.id).where(
                    and_(
                        Message.id.in_(failed_ids),
                        Message.user_id == user_id,
                        Message.campaign_id == campaign_id
                    )
                )
                result = await self.session.execute(campaign_failed_query)
                campaign_failed_ids = [row[0] for row in result.fetchall()]
                
                # Update failed list to only include campaign-context failures
                failed_ids = campaign_failed_ids
            
            logger.info(
                f"Global bulk delete completed: deleted={deleted_count}, "
                f"failed={len(failed_ids)}, force_delete={force_delete}, "
                f"campaign_context={campaign_id}, events_deleted={batch_info.get('events_deleted', 0)}"
            )
            
            return deleted_count, failed_ids, {
                "requires_confirmation": False,
                "events_count": 0,
                "events_deleted": batch_info.get("events_deleted", 0),
                "safety_warnings": []
            }
            
        except Exception as e:
            logger.error(f"Error in global bulk_delete_messages: {e}")
            raise e
</file>

</files>
</file>

<file path="database-schema.md">
# Inboxerr Database Schema Overview

This document outlines the structure of the PostgreSQL database used by the Inboxerr backend service. It includes all tables, relationships, and index/foreign key mappings, aligned with the SQLAlchemy models defined in the codebase.

---

## 🔢 Tables & Relationships

| **Table**           | **Primary Keys** | **Foreign Keys**                             | **Relationships**                             |
|---------------------|------------------|-----------------------------------------------|------------------------------------------------|
| `user`              | `id`             | –                                             | Referenced by many tables                      |
| `apikey`            | `id`             | `user_id` → `user(id)`                     | Each API key belongs to a user                 |
| `campaign`          | `id`             | `user_id` → `user(id)`                     | One user owns many campaigns                   |
| `message`           | `id`             | `user_id` → `user(id)`<br>`campaign_id` → `campaign(id)`<br>`batch_id` → `messagebatch(id)` | Messages belong to a campaign and batch        |
| `messagebatch`      | `id`             | `user_id` → `user(id)`                     | Groups messages sent together                  |
| `messageevent`      | `id`             | `message_id` → `message(id)`              | Tracks status updates for a message            |
| `messagetemplate`   | `id`             | `user_id` → `user(id)`                     | Message content templates                      |
| `webhook`           | `id`             | `user_id` → `user(id)`                     | Defines external callbacks                     |
| `webhookdelivery`   | `id`             | `webhook_id` → `webhook(id)`<br>`message_id` → `message(id)` | Stores actual webhook attempts                 |
| `webhookevent`      | `id`             | –                                             | Events that can trigger webhooks               |
| `alembic_version`   | –                | –                                             | Managed by Alembic for schema migrations       |

---

## 📊 Indexes & Performance

Each table includes relevant indexes, such as:
- `id` (primary key, indexed by default)
- Frequently queried fields like `user_id`, `campaign_id`, `status`, and `scheduled_at`

---

## 🔒 Data Types Overview

| **Field**            | **Type**                       |
|----------------------|---------------------------------|
| `id`                 | `character varying` (UUIDs)     |
| `user_id`            | `character varying` (FK)        |
| `campaign_id`        | `character varying` (FK)        |
| `message`            | `text`                          |
| `status`             | `character varying`             |
| `scheduled_at`       | `timestamp without time zone`   |
| `settings`, `data`   | `json`                          |

---

## 📄 How to Inspect the Schema

From inside `psql`:
```bash
\c inboxerr        -- Connect to DB
\dt                 -- List tables
\d tablename       -- Describe table structure
SELECT * FROM tablename LIMIT 5;  -- Preview data
```

To see all foreign keys:
```sql
SELECT conname AS constraint_name, conrelid::regclass AS table,
       a.attname AS column, confrelid::regclass AS referenced_table
FROM pg_constraint
JOIN pg_class ON conrelid = pg_class.oid
JOIN pg_attribute a ON a.attrelid = conrelid AND a.attnum = ANY(conkey)
WHERE contype = 'f';
```
</file>

<file path="Dockerfile">
# Use Python 3.10 slim as base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONPATH=/app

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        build-essential \
        libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN adduser --disabled-password --gecos "" appuser
RUN chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Start application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path="docs/FRONTEND_API_SETUP.md">
# Inboxerr Backend - Frontend Developer Setup

This guide will help frontend developers set up and interact with the Inboxerr backend API.

## Quick Start

1. Clone the repository
```bash
git clone https://github.com/your-org/inboxerr-backend.git
cd inboxerr-backend
```

2. Set up a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

4. Create a local config file
```bash
cp .env.example .env
```

5. Set up the database
```bash
# Make sure PostgreSQL is running on your system
# Create database
psql -U postgres -c "CREATE DATABASE inboxerr;"

# Run migrations
alembic upgrade head
```

6. Seed sample data for frontend development
```bash
python scripts/seed_frontend_data.py
```

7. Start the server
```bash
uvicorn app.main:app --reload
```

8. Access the API at http://localhost:8000/api/docs

## Sample Account

After running the seed script, you can use these credentials:
- Email: `test@example.com`
- Password: `Test1234!`

## Key Features Ready for Frontend Integration

- ✅ User authentication (JWT)
- ✅ Send individual and batch SMS messages
- ✅ Message templates with variable substitution
- ✅ Campaign management
- ✅ Message status tracking
- ✅ Webhook handling for status updates

## API Documentation

See the [Inboxerr API Frontend Developer Guide](FRONTEND_API_GUIDE.md) for complete documentation of all available endpoints.

## Mock SMS Gateway

For frontend development, the backend can operate without real SMS Gateway credentials. Messages will be processed normally but not actually sent:

1. In development mode, the backend will simulate sending messages
2. All webhook events can be manually triggered for testing
3. All message statuses can be updated through the API

## Using with Docker (Alternative)

If you prefer using Docker:

```bash
# Start all services
docker-compose up -d

# Seed sample data
docker-compose exec api python scripts/seed_frontend_data.py
```

## Testing Webhooks

To test webhook events for message status updates:

```bash
# Replace EVENT_TYPE with: sms:sent, sms:delivered, or sms:failed
curl -X POST http://localhost:8000/api/v1/webhooks/test/{EVENT_TYPE} \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message_id": "YOUR_MESSAGE_ID"}'
```

## Troubleshooting

- **Database connection issues**: Ensure PostgreSQL is running and credentials are correct in `.env`
- **Authentication errors**: Check that you're using the correct bearer token format
- **CORS errors**: Add your frontend URL to `BACKEND_CORS_ORIGINS` in `.env`

## Need Help?

Contact the backend team via:
- Slack: #inboxerr-backend
- Email: backend@inboxerr.com
</file>

<file path="docs/FRONTEND_DEVELOPER_GUIDE.md">
# Inboxerr API - Frontend Developer Guide

This document provides frontend developers with essential information for integrating with the Inboxerr backend API.

## Base URL

```
http://localhost:8000/api/v1
```

For production, this will be replaced with the actual deployment URL.

## Authentication

### Getting a Token

```
POST /auth/token
```

**Request Body:**
```json
{
  "username": "your-email@example.com",
  "password": "your-password"
}
```

**Response:**
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_at": "2025-04-25T20:04:04.790Z"
}
```

### Using Authentication

Include the token in all subsequent requests:

```
Authorization: Bearer {access_token}
```

### Test User Credentials

For development, use the following credentials:

- Email: `test@example.com`
- Password: `Test1234!`

## Key Endpoints

### 1. Send a Single SMS

```
POST /messages/send
```

**Request Body:**
```json
{
  "phone_number": "+1234567890",
  "message": "Your message content",
  "scheduled_at": null,
  "custom_id": "optional-tracking-id"
}
```

### 2. Send Batch Messages

```
POST /messages/batch
```

**Request Body:**
```json
{
  "messages": [
    {
      "phone_number": "+1234567890",
      "message": "Message for recipient 1",
      "scheduled_at": null
    },
    {
      "phone_number": "+9876543210",
      "message": "Message for recipient 2",
      "scheduled_at": null
    }
  ],
  "options": {
    "delay_between_messages": 0.3,
    "fail_on_first_error": false
  }
}
```

### 3. List Messages

```
GET /messages?skip=0&limit=20
```

Optional query parameters:
- `status` - Filter by message status (pending, sent, delivered, failed)
- `phone_number` - Filter by phone number
- `from_date` - Filter by date (ISO format)
- `to_date` - Filter by date (ISO format)

### 4. Message Templates

#### Create Template

```
POST /templates
```

**Request Body:**
```json
{
  "name": "Welcome Template",
  "content": "Hello {{name}}, welcome to our service!",
  "description": "Welcome message for new users"
}
```

#### Send Using Template

```
POST /templates/send
```

**Request Body:**
```json
{
  "template_id": "template-uuid",
  "phone_number": "+1234567890",
  "variables": {
    "name": "John"
  }
}
```

### 5. Campaigns

#### Create Campaign

```
POST /campaigns
```

**Request Body:**
```json
{
  "name": "Marketing Campaign",
  "description": "Product launch campaign",
  "scheduled_start_at": "2025-05-01T09:00:00Z",
  "scheduled_end_at": "2025-05-01T18:00:00Z"
}
```

#### Start Campaign

```
POST /campaigns/{campaign_id}/start
```

## Status Codes

- `200` - Success
- `201` - Created
- `202` - Accepted (for async processing)
- `400` - Bad request
- `401` - Unauthorized
- `403` - Forbidden
- `404` - Not found
- `422` - Validation error
- `429` - Rate limit exceeded
- `500` - Server error

## Error Format

All API errors follow this format:

```json
{
  "status": "error",
  "code": "ERROR_CODE",
  "message": "Human-readable error message",
  "details": {}
}
```

## Pagination

Endpoints that return lists support pagination:

```
GET /messages?page=1&limit=20
```

Response includes pagination info:

```json
{
  "items": [...],
  "page_info": {
    "current_page": 1,
    "total_pages": 5,
    "page_size": 20,
    "total_items": 100,
    "has_previous": false,
    "has_next": true
  }
}
```

## Message Status Flow

Messages follow this status flow:

1. `pending` - Initial state when created
2. `scheduled` - For future delivery
3. `processed` - Submitted to SMS gateway
4. `sent` - Accepted by the carrier
5. `delivered` - Confirmed delivery to recipient
6. `failed` - Failed to deliver

## Webhooks

For development, you can test webhook events using:

```
GET /webhooks/test/{event_type}
```

Where `event_type` can be:
- `sms:sent`
- `sms:delivered`
- `sms:failed`

## Rate Limits

- Message sending: 60 requests per minute
- Batch operations: 10 requests per minute
- Template operations: 100 requests per minute

## Development Notes

- Phone numbers should be in E.164 format (e.g., +1234567890)
- Messages longer than 160 characters will be sent as multi-part SMS
- SMS templates support variable substitution using `{{variable_name}}` syntax
</file>

<file path="docs/Message Template System - User Guide.md">
# Message Template System - User Guide

## Overview

The message template system allows you to create reusable templates for your SMS messages. This is particularly useful when you need to send similar messages to multiple recipients with personalized content.

## Key Features

- Create and manage reusable message templates
- Support for variables using the `{{variable_name}}` syntax
- Preview how templates will look with specific variable values
- Send messages using templates with just a phone number and variable values
- Send batch messages using the same template with different variables for each recipient

## Creating Templates

### Via API

```http
POST /api/v1/templates
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "name": "OTP Notification",
  "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
  "description": "Template for sending OTP codes",
  "is_active": true
}
```

The system will automatically detect variables in the format `{{variable_name}}` from your template content.

### Variable Format

Variables should be enclosed in double curly braces like `{{variable_name}}`. Variable names can contain letters, numbers, and underscores.

Examples:
- `{{code}}`
- `{{user_name}}`
- `{{order_123}}`

## Using Templates

### Previewing a Template

Before sending, you can preview how your template will look with specific variables:

```http
POST /api/v1/templates/apply?template_id=TEMPLATE_ID
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "variables": {
    "code": "123456",
    "minutes": "15"
  }
}
```

Response:
```json
{
  "result": "Your verification code is 123456. It will expire in 15 minutes.",
  "missing_variables": []
}
```

### Sending a Message with a Template

```http
POST /api/v1/templates/send
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "template_id": "TEMPLATE_ID",
  "phone_number": "+1234567890",
  "variables": {
    "code": "123456",
    "minutes": "15"
  },
  "scheduled_at": null,
  "custom_id": "otp-1234"
}
```

This will apply the variables to your template and send the resulting message.

### Batch Sending with Templates

For sending to multiple recipients with different variables:

```http
POST /api/v1/messages/batch
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "messages": [
    {
      "phone_number": "+1234567890",
      "message": "Your custom message using {{variable}} syntax",
      "custom_id": "batch-1"
    },
    {
      "phone_number": "+0987654321",
      "message": "Another message with {{different}} variable",
      "custom_id": "batch-2"
    }
  ],
  "options": {
    "delay_between_messages": 0.3,
    "fail_on_first_error": false
  }
}
```

## Managing Templates

### Listing Templates

```http
GET /api/v1/templates?active_only=true
Authorization: Bearer YOUR_TOKEN
```

### Getting a Specific Template

```http
GET /api/v1/templates/{template_id}
Authorization: Bearer YOUR_TOKEN
```

### Updating a Template

```http
PUT /api/v1/templates/{template_id}
Content-Type: application/json
Authorization: Bearer YOUR_TOKEN

{
  "name": "Updated OTP Template",
  "content": "Your code is {{code}}. Valid for {{minutes}} minutes.",
  "is_active": true
}
```

### Deleting a Template

```http
DELETE /api/v1/templates/{template_id}
Authorization: Bearer YOUR_TOKEN
```

## Best Practices

1. **Descriptive Variable Names**: Use clear, descriptive variable names that indicate what data should be inserted.

2. **Test Before Sending**: Always use the `/templates/apply` endpoint to test how your template will look with real data before sending messages.

3. **Handle Missing Variables**: Check the `missing_variables` field in responses to ensure all required variables are provided.

4. **Version Your Templates**: If you need to make significant changes to a template that's in use, consider creating a new version instead of updating the existing one.

5. **Keep Templates Simple**: Avoid complex formatting that might not render well on all mobile devices.

6. **Include Message Signature**: Consider including your company name or service identifier at the end of templates to help recipients identify the sender.

## Examples

### Appointment Reminder

```
Template Content:
"Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule."

Applied with:
{
  "name": "John",
  "date": "May 5, 2025",
  "time": "2:30 PM",
  "phone": "555-123-4567"
}

Result:
"Hi John, this is a reminder for your appointment on May 5, 2025 at 2:30 PM. Reply YES to confirm or call 555-123-4567 to reschedule."
```

### Order Confirmation

```
Template Content:
"Your order #{{order_id}} has been confirmed! Estimated delivery: {{delivery_date}}. Track your package at {{tracking_url}}. Thanks for shopping with {{company_name}}!"

Applied with:
{
  "order_id": "1234567",
  "delivery_date": "Apr 28-30, 2025",
  "tracking_url": "https://track.example.com/1234567",
  "company_name": "Example Shop"
}

Result:
"Your order #1234567 has been confirmed! Estimated delivery: Apr 28-30, 2025. Track your package at https://track.example.com/1234567. Thanks for shopping with Example Shop!"
```
</file>

<file path="docs/sprint.md">
Based on my comprehensive analysis of the Inboxerr backend codebase, I'll now provide the deep diagnostic analysis.
Phase 1 — System Comprehension
Executive Summary

CSV Import Memory Exhaustion: await file.read() in /messages/import and /campaigns/from-csv loads entire files into RAM, causing OOM on >50MB files (app/api/v1/endpoints/messages.py:242, app/api/v1/endpoints/campaigns.py:71)
Blocking Message Endpoints: All message send endpoints (send_message, send_batch, import_messages) accept BackgroundTasks but immediately await SMS operations, violating 202 "accepted" contract and tying up event loop
Missing Campaign Personalization Schema: Campaign and Message models lack required message_content, template_id, and variables JSON columns specified in personalization requirements
Incomplete CSV→Contacts→Queued Messages Pipeline: Missing import_jobs, contacts tables and streaming parser service described in architecture specification
Synchronous SMS Gateway Calls: Direct awaits in sms_sender.send_message() block request threads instead of queuing for background processing
Task Status Tracking Gaps: /tasks/{id} endpoint returns 404 because no task records are created for background operations
Missing Database Migrations: Required schema changes for personalization (variables JSON, message_content, template_id) have no corresponding Alembic migrations
Event Loop Bottlenecks: CSV parsing and SMS sending operations perform synchronous I/O that should be moved to background workers
Unguarded Memory Usage: No streaming mechanisms or memory limits for file processing operations
Missing Queue Infrastructure: Architecture calls for dispatcher loop with FOR UPDATE SKIP LOCKED but implementation missing

Detailed Gap Analysis
ComponentIssueSeverityFile(s)Line #CSV Importawait file.read() loads entire file in RAMCriticalapp/api/v1/endpoints/messages.py242CSV Importawait file.read() in campaign CSV uploadCriticalapp/api/v1/endpoints/campaigns.py71Message Endpointssend_message awaits SMS instead of backgroundingHighapp/api/v1/endpoints/messages.py45-52Message Endpointssend_batch awaits SMS instead of backgroundingHighapp/api/v1/endpoints/messages.py82-89Message Endpointsimport_messages awaits processing instead of backgroundingHighapp/api/v1/endpoints/messages.py242-270Campaign ModelMissing message_content columnHighapp/models/campaign.pyN/ACampaign ModelMissing template_id columnHighapp/models/campaign.pyN/AMessage ModelMissing variables JSON columnHighapp/models/message.pyN/AMessage ModelMissing import_id FK columnMediumapp/models/message.pyN/ADatabase SchemaMissing import_jobs tableHighN/AN/ADatabase SchemaMissing contacts tableHighN/AN/ACSV PipelineMissing streaming parser serviceHighapp/services/imports/parser.pyMissing fileCSV PipelineMissing upload endpoint with streamingHighapp/api/v1/endpoints/imports.pyMissing fileSMS QueueMissing dispatcher loop implementationMediumapp/services/sms/dispatcher.pyMissing filePydantic SchemasCampaign schemas missing message_content/template_id validationMediumapp/schemas/campaign.pyN/ABackground TasksBackgroundTasks parameters never usedMediumMultiple endpoint filesMultipleTask TrackingNo task record creation for /tasks/{id} endpointMediumMessage endpointsN/A
Call Graph & Data Flow Analysis
Current CSV Import Flow:

POST /messages/import → import_messages()
await file.read() → Blocks event loop, loads full file
csv.reader() → Synchronous parsing
sms_sender.schedule_batch_from_numbers() → Direct await
sms_sender.send_batch() → Blocks until SMS gateway responds

Current Message Send Flow:

POST /messages/send → send_message()
sms_sender.send_message() → Direct await
_send_to_gateway() → HTTP call blocks request
Returns 202 but client already waited for SMS completion

Required vs Actual Database Schema:

Missing: import_jobs table with processing status tracking
Missing: contacts table for normalized contact storage
Missing: variables JSON column on message table
Missing: message_content, template_id columns on campaign table
Missing: import_id FK on message table for lineage

Memory Usage Hotspots
Line-by-Line Critical Sections:

app/api/v1/endpoints/messages.py:242 - contents = await file.read()
app/api/v1/endpoints/campaigns.py:71 - contents = await file.read()
No streaming mechanisms anywhere in CSV processing pipeline
No memory limits or file size validation beyond basic checks

Step-by-Step Remediation Roadmap
Phase 1A: Critical Memory & Blocking Fixes (Week 1)
Step 1.1: Add Streaming CSV Upload

Files: Create app/api/v1/endpoints/imports.py
Changes: Stream file to /tmp/{uuid}.csv, compute SHA-256, return 202
Tests: Upload 100MB file, verify constant memory usage
Outcome: Memory usage ≤ 50MB for any file size

Step 1.2: Create Background Task Infrastructure

Files: Modify app/api/v1/endpoints/messages.py lines 45-52, 82-89, 242-270
Changes: Use background_tasks.add_task() instead of direct awaits
Tests: Verify 202 returns immediately, work completes in background
Outcome: p95 endpoint latency ≤ 250ms

Phase 1B: Database Schema Updates (Week 1-2)
Step 1.3: Add Required Migrations

Files: Create alembic/versions/{timestamp}_add_personalization_columns.py
Changes: Add message_content, template_id to campaigns; variables JSON to messages
Tests: Run migration, verify schema matches spec
Outcome: All personalization spec columns available

Step 1.4: Create Import Tables

Files: Create alembic/versions/{timestamp}_add_import_tables.py
Changes: Add import_jobs, contacts tables per architecture spec
Tests: Create import job, verify foreign keys work
Outcome: CSV import pipeline data models ready

Phase 2A: Streaming Parser Service (Week 2-3)
Step 2.1: Implement CSV Parser

Files: Create app/services/imports/parser.py
Changes: Use csv.DictReader with 1000-row bulk commits
Tests: Parse 100k-row file in <3 minutes
Outcome: Constant memory parsing of large CSVs

Step 2.2: Connect Import Pipeline

Files: Modify app/api/v1/endpoints/campaigns.py, create import endpoints
Changes: Accept import_id in campaign creation, link to contacts
Tests: CSV → contacts → campaign → messages flow
Outcome: End-to-end personalized campaign creation

Phase 2B: Queue Infrastructure (Week 3-4)
Step 2.3: Background Message Dispatcher

Files: Create app/services/sms/dispatcher.py
Changes: Async loop with FOR UPDATE SKIP LOCKED, asyncio.Semaphore
Tests: Process 10k queued messages without deadlocks
Outcome: Scalable message processing queue

Step 2.4: Update Pydantic Schemas

Files: Modify app/schemas/campaign.py
Changes: Add message_content/template_id validation, require one
Tests: Verify validation errors when both missing
Outcome: API contracts match personalization spec

Migration Dependencies

Schema Migrations: Must run before code deployment
Import Tables: Required before streaming parser
Background Tasks: Can be deployed incrementally per endpoint
Queue Infrastructure: Requires message status updates to work

Measurable Success Criteria

Memory: CSV import RAM usage ≤ 300MB for 100k-row files
Latency: Message endpoints p95 ≤ 250ms (currently seconds)
Throughput: Support 1000+ concurrent CSV uploads
Reliability: Zero OOM errors on production file sizes
Functionality: Campaign personalization with variables working end-to-end



patterns:

ImportError: Inherits from InboxerrException
Migration naming: {hash}_add_campaign_personalization_columns.py and {hash}_add_import_tables.py
Background tasks: Async-only interface compatible with existing async patterns
File validation: Both extension (.csv) AND MIME type (text/csv, text/plain)
Row count validation: During streaming (abort at 1M+ rows)
Error storage: JSONB column with {row, column, message} objects
Status enum: PROCESSING, SUCCESS, FAILED, CANCELLED
Template validation: Check ownership + active status at creation
Concurrent limits: Enforce at upload (return 429 if 5+ active jobs)
Cleanup: Delete temp files immediately, 4-day retention for job records
</file>

<file path="scripts/generate_migration.py">
#!/usr/bin/env python
"""
Generate Alembic migrations.

This script parses database credentials from settings and runs alembic to generate migrations.

Usage:
    python scripts/generate_migration.py "Add message templates"
"""
import sys
import subprocess
import re
from pathlib import Path

# Add parent directory to path to allow importing from app
sys.path.append(str(Path(__file__).parent.parent))

# Import after adding to path
from app.core.config import settings

def generate_migration(message):
    """Generate an Alembic migration with the given message."""
    try:
        # Set the Alembic database URL from settings
        # Replace asyncpg with standard psycopg2 for Alembic
        db_url = settings.DATABASE_URL.replace("postgresql+asyncpg", "postgresql")
        
        # Set environment variable for Alembic
        import os
        os.environ["ALEMBIC_DB_URL"] = db_url
        
        # Run alembic command
        print(f"Generating migration: {message}")
        result = subprocess.run(
            ["alembic", "revision", "--autogenerate", "-m", message], 
            check=True,
            capture_output=True,
            text=True
        )
        
        # Parse output to find migration file path
        output = result.stdout
        file_pattern = r"Generating .*\\(.*?\.py)"
        match = re.search(file_pattern, output)
        
        if match:
            migration_file = match.group(1)
            print(f"✅ Successfully generated migration: {migration_file}")
        else:
            print(f"✅ Successfully generated migration")
            
        print("⚠️ Please review the generated migration file to ensure it's correct.")
        print("💡 To apply the migration, run: alembic upgrade head")
        
    except subprocess.CalledProcessError as e:
        print(f"❌ Error generating migration: {e}")
        print("Error output:")
        print(e.stderr)
        print("💡 Make sure alembic is installed and your database is running.")
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("❌ Error: Please provide a migration message.")
        print("💡 Example: python scripts/generate_migration.py 'Add user table'")
        sys.exit(1)
    
    message = sys.argv[1]
    generate_migration(message)
</file>

<file path="scripts/mvp.sh">
#!/bin/bash
# MVP Setup and Run Script
# This script helps set up and run all components needed for the MVP

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}=== Inboxerr MVP Setup and Run ===${NC}"
echo "This script will help you set up and run all MVP components"

# Extract database connection details from settings
get_db_info() {
  echo "Extracting database connection details..."
  
  # Run a Python script to get DB connection info from settings
  python -c "
from app.core.config import settings
import re

# Parse the connection URL
url = settings.DATABASE_URL
pattern = r'postgresql(?:\+asyncpg)?://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)'
match = re.match(pattern, url)

if match:
    print(f'DB_USER={match.group(1)}')
    print(f'DB_PASS={match.group(2)}')
    print(f'DB_HOST={match.group(3)}')
    print(f'DB_PORT={match.group(4)}')
    print(f'DB_NAME={match.group(5)}')
else:
    print('Could not parse database URL')
  "
}

# Source the DB info (if Python script outputs variables)
eval "$(get_db_info)"

# Check if PostgreSQL is running using the extracted credentials
check_postgres() {
  if [ -z "$DB_HOST" ] || [ -z "$DB_PORT" ] || [ -z "$DB_USER" ]; then
    echo -e "${RED}Could not extract database connection details from settings.${NC}"
    return 1
  fi
  
  export PGPASSWORD=$DB_PASS
  pg_status=$(psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d postgres -c "SELECT 1;" 2>/dev/null)
  
  if [ $? -ne 0 ]; then
    echo -e "${RED}PostgreSQL is not running or connection failed.${NC}"
    echo "Please make sure PostgreSQL is running and connection details are correct:"
    echo "Host: $DB_HOST"
    echo "Port: $DB_PORT"
    echo "User: $DB_USER"
    echo "Database: $DB_NAME"
    return 1
  else
    echo -e "${GREEN}PostgreSQL connection successful.${NC}"
    return 0
  fi
}

# Check PostgreSQL connection
if ! check_postgres; then
  echo -e "${YELLOW}Would you like to continue anyway? (y/N)${NC}"
  read continue_anyway
  if [[ ! "$continue_anyway" =~ ^[Yy]$ ]]; then
    echo "Exiting."
    exit 1
  fi
fi

# Create directories if needed
if [ ! -d "tests" ]; then
  echo -e "${YELLOW}Creating test directories...${NC}"
  mkdir -p tests/unit
  mkdir -p tests/integration
  mkdir -p tests/unit/services
  mkdir -p tests/unit/repositories
  mkdir -p tests/unit/api
  mkdir -p tests/integration/api
  
  # Create empty __init__.py files
  touch tests/__init__.py
  touch tests/unit/__init__.py
  touch tests/integration/__init__.py
  touch tests/unit/services/__init__.py
  touch tests/unit/repositories/__init__.py
  touch tests/unit/api/__init__.py
  touch tests/integration/api/__init__.py
  
  echo -e "${GREEN}Test directory structure created!${NC}"
fi

# Function to show menu
show_menu() {
  echo ""
  echo -e "${BLUE}Available actions:${NC}"
  echo "1) Setup development database"
  echo "2) Generate database migrations"
  echo "3) Run migrations"
  echo "4) Run tests"
  echo "5) Start API server"
  echo "6) View API documentation"
  echo "q) Quit"
  echo ""
  echo -n "Enter your choice: "
}

# Main menu loop
while true; do
  show_menu
  read choice

  case $choice in
    1)
      echo -e "${YELLOW}Setting up development database...${NC}"
      python scripts/setup_test_db.py
      ;;
    2)
      echo -e "${YELLOW}Generating database migrations...${NC}"
      read -p "Enter migration description: " desc
      python scripts/generate_migration.py "$desc"
      ;;
    3)
      echo -e "${YELLOW}Running database migrations...${NC}"
      alembic upgrade head
      ;;
    4)
      echo -e "${YELLOW}Running tests...${NC}"
      python scripts/run_tests.py
      ;;
    5)
      echo -e "${YELLOW}Starting API server...${NC}"
      echo -e "${GREEN}API will be available at: http://localhost:8000${NC}"
      echo -e "${GREEN}API Docs URL: http://localhost:8000/api/docs${NC}"
      echo -e "${YELLOW}Press CTRL+C to stop the server${NC}"
      uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
      ;;
    6)
      echo -e "${YELLOW}Opening API documentation...${NC}"
      if command -v xdg-open &> /dev/null; then
        xdg-open http://localhost:8000/api/docs
      elif command -v open &> /dev/null; then
        open http://localhost:8000/api/docs
      elif command -v start &> /dev/null; then
        start http://localhost:8000/api/docs
      else
        echo -e "${RED}Cannot open browser automatically.${NC}"
        echo -e "${GREEN}Please visit: http://localhost:8000/api/docs${NC}"
      fi
      ;;
    q|Q)
      echo -e "${GREEN}Goodbye!${NC}"
      exit 0
      ;;
    *)
      echo -e "${RED}Invalid choice. Please try again.${NC}"
      ;;
  esac
done
</file>

<file path="scripts/reset_db.py">
"""
Reset the development database, run Alembic migrations, and seed initial data.
Works with PostgreSQL database.
"""
import sys
import os
import subprocess
from pathlib import Path



# Resolve project root dynamically
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def reset_database() -> None:
    """
    Reset the development database, run Alembic migrations, and seed initial data.
    This should only be used in development environments.
    """
    # Reset PostgreSQL database (drop and recreate)
    print("🗄️ Resetting PostgreSQL database...")
    try:
        # Connect to default postgres database to drop/create our database
        subprocess.run(
            ["psql", "-U", "postgres", "-c", "DROP DATABASE IF EXISTS inboxerr;"],
            check=True
        )
        subprocess.run(
            ["psql", "-U", "postgres", "-c", "CREATE DATABASE inboxerr;"],
            check=True
        )
        print("✅ Database reset successfully")
    except subprocess.CalledProcessError as e:
        print(f"❌ Error resetting database: {e}")
        print("💡 Make sure PostgreSQL is running and you have permissions")
        sys.exit(1)

    print("🚀 Running Alembic migrations...")
    subprocess.run(["alembic", "upgrade", "head"], check=True, cwd=PROJECT_ROOT)

    print("🌱 Seeding initial data...")
    subprocess.run(
        [sys.executable, "scripts/seed_db.py"], 
        check=True, 
        cwd=PROJECT_ROOT,
        env={**os.environ, "PYTHONPATH": str(PROJECT_ROOT)}
    )

    print("✅ Database reset and seeded successfully!")

if __name__ == "__main__":
    reset_database()
</file>

<file path="scripts/run_tests.py">
#!/usr/bin/env python
"""
Run application tests with pytest.

This script:
1. Creates a test database if it doesn't exist
2. Runs pytest with specified options
3. Generates a coverage report

Usage:
    python scripts/run_tests.py [pytest_args]
    
Examples:
    python scripts/run_tests.py                             # Run all tests
    python scripts/run_tests.py tests/integration           # Run integration tests
    python scripts/run_tests.py -v tests/unit/test_users.py # Run specific test with verbose output
"""
import sys
import os
import subprocess
import re
from pathlib import Path

# Add parent directory to sys.path
sys.path.append(str(Path(__file__).parent.parent))

# Import settings after adding to path
from app.core.config import settings

# Extract database connection info from the URL
def parse_db_url(url):
    """Parse database URL to extract connection information."""
    # PostgreSQL URL format: postgresql+asyncpg://user:password@host:port/dbname
    pattern = r"postgresql(?:\+asyncpg)?://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)"
    match = re.match(pattern, url)
    
    if match:
        return {
            "user": match.group(1),
            "password": match.group(2),
            "host": match.group(3),
            "port": match.group(4),
            "dbname": match.group(5)
        }
    return None

def run_tests():
    """Run tests with pytest."""
    # Get pytest arguments from command line
    pytest_args = sys.argv[1:] if len(sys.argv) > 1 else []
    
    # Create test database name
    db_info = parse_db_url(settings.DATABASE_URL)
    if not db_info:
        print("❌ Could not parse database URL. Please check the format.")
        return False
    
    test_db_name = f"{db_info['dbname']}_test"
    
    # Set up test database environment variable
    os.environ["DATABASE_URL"] = f"postgresql+asyncpg://{db_info['user']}:{db_info['password']}@{db_info['host']}:{db_info['port']}/{test_db_name}"
    os.environ["TESTING"] = "1"
    
    # Print the test database URL
    print(f"Using test database: {test_db_name}")
    
    # Get environment variables for PGPASSWORD to avoid password prompt
    env = os.environ.copy()
    env["PGPASSWORD"] = db_info["password"]
    
    # Check if test database exists
    try:
        check_db = subprocess.run(
            ["psql", 
             "-h", db_info["host"], 
             "-p", db_info["port"], 
             "-U", db_info["user"], 
             "-d", "postgres", 
             "-c", f"SELECT 1 FROM pg_database WHERE datname = '{test_db_name}';"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env
        )
        
        if "1 row" not in check_db.stdout.decode():
            print(f"Creating test database {test_db_name}...")
            create_db = subprocess.run(
                ["psql", 
                 "-h", db_info["host"], 
                 "-p", db_info["port"], 
                 "-U", db_info["user"], 
                 "-d", "postgres", 
                 "-c", f"CREATE DATABASE {test_db_name};"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env
            )
            
            if create_db.returncode != 0:
                print(f"❌ Failed to create test database: {create_db.stderr.decode()}")
                return False
            
            print(f"✅ Test database {test_db_name} created")
    except Exception as e:
        print(f"⚠️ Could not check/create test database: {e}")
        print("Continuing with tests...")
    
    # Default pytest arguments if none provided
    if not pytest_args:
        # Run all tests with coverage
        pytest_args = [
            "--cov=app",
            "--cov-report=term-missing",
            "--cov-report=html",
            "-v",
            "tests/"
        ]
    
    # Run pytest
    print(f"Running tests with args: {' '.join(pytest_args)}")
    result = subprocess.run(["pytest"] + pytest_args)
    
    # Print results
    if result.returncode == 0:
        print("✅ All tests passed!")
    else:
        print(f"❌ Tests failed with exit code: {result.returncode}")
    
    return result.returncode == 0


if __name__ == "__main__":
    sys.exit(0 if run_tests() else 1)
</file>

<file path="scripts/seed_frontend_data.py">
#!/usr/bin/env python
"""
Seed database with sample data for frontend development.

This script creates sample users, templates, messages and campaigns
to make frontend development easier.

Usage:
    python scripts/seed_frontend_data.py
"""
import asyncio
import sys
import os
from pathlib import Path
from datetime import datetime, timedelta, timezone
import random
import uuid

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

# Import app modules
from app.db.session import initialize_database, async_session_factory
from app.db.repositories.users import UserRepository
from app.db.repositories.templates import TemplateRepository
from app.db.repositories.messages import MessageRepository
from app.db.repositories.campaigns import CampaignRepository
from app.core.security import get_password_hash
from app.schemas.message import MessageStatus

# Sample phone numbers
PHONE_NUMBERS = [
    "+12025550108", "+12025550112", "+12025550118", "+12025550121",
    "+12025550125", "+12025550132", "+12025550139", "+12025550144",
    "+12025550152", "+12025550158", "+12025550165", "+12025550171"
]

# Sample message templates
TEMPLATES = [
    {
        "name": "Welcome Message",
        "content": "Hi {{name}}, welcome to our service! We're glad you've joined us.",
        "description": "Template for welcoming new users"
    },
    {
        "name": "OTP Verification",
        "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
        "description": "Template for sending OTP codes"
    },
    {
        "name": "Appointment Reminder",
        "content": "Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule.",
        "description": "Template for appointment reminders"
    },
    {
        "name": "Order Confirmation",
        "content": "Your order #{{order_id}} has been confirmed! Estimated delivery: {{delivery_date}}. Track your package at {{tracking_url}}",
        "description": "Order confirmation message"
    },
    {
        "name": "Payment Reminder",
        "content": "Reminder: Your payment of ${{amount}} is due on {{due_date}}. Please ensure your account has sufficient funds.",
        "description": "Payment reminder notification"
    }
]

# Sample messages
MESSAGES = [
    "Your verification code is 123456",
    "Your appointment is confirmed for tomorrow at 10:00 AM",
    "Your order #12345 has been shipped and will arrive on Friday",
    "Thank you for your payment of $99.99",
    "Your subscription will renew on May 15, 2025",
    "Your account password has been reset successfully",
    "Your flight PO491 has been delayed by 30 minutes",
    "Your table reservation at Milano Restaurant is confirmed",
    "Your prescription is ready for pickup at Central Pharmacy",
    "Reminder: You have a meeting scheduled in 1 hour"
]

# Sample campaign names
CAMPAIGN_NAMES = [
    "Spring Sale Promotion",
    "Customer Feedback Survey",
    "Product Launch Announcement",
    "Abandoned Cart Reminder",
    "Loyalty Program Update"
]

async def create_test_user():
    """Create a test user if it doesn't exist."""
    async with async_session_factory() as session:
        # Create user repository
        user_repo = UserRepository(session)
        
        # Check if user exists
        existing_user = await user_repo.get_by_email("test@example.com")
        if existing_user:
            print(f"✅ Test user test@example.com already exists")
            return existing_user
        
        # Create user
        hashed_password = get_password_hash("Test1234!")
        user = await user_repo.create(
            email="test@example.com",
            hashed_password=hashed_password,
            full_name="Test User",
            role="user"
        )
        
        print(f"✅ Created test user: {user.email}")
        return user

async def create_test_templates(user_id):
    """Create test message templates."""
    async with async_session_factory() as session:
        # Create template repository
        template_repo = TemplateRepository(session)
        
        created_templates = []
        # Create templates
        for template_data in TEMPLATES:
            template = await template_repo.create_template(
                name=template_data["name"],
                content=template_data["content"],
                description=template_data["description"],
                user_id=user_id
            )
            created_templates.append(template)
            print(f"✅ Created template: {template_data['name']}")
        
        return created_templates

async def create_test_messages(user_id, template_id=None):
    """Create test messages with different statuses."""
    async with async_session_factory() as session:
        # Create message repository
        message_repo = MessageRepository(session)
        
        # Generate different message statuses
        statuses = [
            MessageStatus.PENDING,
            MessageStatus.SENT,
            MessageStatus.DELIVERED,
            MessageStatus.FAILED,
            MessageStatus.SCHEDULED
        ]
        
        created_messages = []
        # Create messages with different statuses
        for i, message_text in enumerate(MESSAGES):
            phone = random.choice(PHONE_NUMBERS)
            status = statuses[i % len(statuses)]
            
            # For scheduled messages, set a future time
            scheduled_at = None
            if status == MessageStatus.SCHEDULED:
                scheduled_at = datetime.now(timezone.utc) + timedelta(days=1)
            
            # Create message
            message = await message_repo.create_message(
                phone_number=phone,
                message_text=message_text,
                user_id=user_id,
                custom_id=f"sample-{uuid.uuid4().hex[:8]}",
                scheduled_at=scheduled_at,
                metadata={"sample": True, "template_id": template_id}
            )
            
            # If not scheduled, update to the appropriate status
            if status != MessageStatus.SCHEDULED and status != MessageStatus.PENDING:
                # Update message status
                await message_repo.update_message_status(
                    message_id=message.id,
                    status=status,
                    event_type="seeded_data",
                    reason="Sample data" if status == MessageStatus.FAILED else None,
                    gateway_message_id=f"gw-{uuid.uuid4()}" if status != MessageStatus.PENDING else None
                )
            
            created_messages.append(message)
            print(f"✅ Created message with status {status}: {message_text[:30]}...")
        
        return created_messages

async def create_test_campaigns(user_id):
    """Create test campaigns."""
    async with async_session_factory() as session:
        # Create campaign repository
        campaign_repo = CampaignRepository(session)
        message_repo = MessageRepository(session)
        
        created_campaigns = []
        # Create campaigns with different statuses
        statuses = ["draft", "active", "paused", "completed", "cancelled"]
        
        for i, name in enumerate(CAMPAIGN_NAMES):
            status = statuses[i % len(statuses)]
            
            # Create campaign
            campaign = await campaign_repo.create_campaign(
                name=name,
                description=f"Sample campaign: {name}",
                user_id=user_id,
                scheduled_start_at=datetime.now(timezone.utc) + timedelta(days=1),
                scheduled_end_at=datetime.now(timezone.utc) + timedelta(days=2),
                settings={"sample": True}
            )
            
            # Add 3-5 messages to each campaign
            msg_count = random.randint(3, 5)
            for j in range(msg_count):
                phone = random.choice(PHONE_NUMBERS)
                message_text = f"Campaign {name}: {random.choice(MESSAGES)}"
                
                await message_repo.create_message(
                    phone_number=phone,
                    message_text=message_text,
                    user_id=user_id,
                    campaign_id=campaign.id,
                    metadata={"campaign": name}
                )
            
            # Update campaign stats
            campaign.total_messages = msg_count
            
            # Update campaign status
            if status != "draft":
                await campaign_repo.update_campaign_status(
                    campaign_id=campaign.id,
                    status=status,
                    started_at=datetime.now(timezone.utc) - timedelta(days=1) if status != "draft" else None,
                    completed_at=datetime.now(timezone.utc) if status in ["completed", "cancelled"] else None
                )
                
                # Update stats for non-draft campaigns
                if status in ["active", "paused", "completed"]:
                    sent = msg_count if status in ["completed"] else random.randint(1, msg_count)
                    delivered = random.randint(0, sent) if status in ["completed"] else 0
                    failed = random.randint(0, msg_count - sent) if status in ["completed"] else 0
                    
                    await campaign_repo.update_campaign_stats(
                        campaign_id=campaign.id,
                        increment_sent=sent,
                        increment_delivered=delivered,
                        increment_failed=failed
                    )
            
            created_campaigns.append(campaign)
            print(f"✅ Created campaign with status {status}: {name}")
        
        return created_campaigns

async def seed_database():
    """Seed database with sample data."""
    print("🌱 Seeding database with frontend development data...")
    
    # Initialize database
    await initialize_database()
    
    # Create test user
    user = await create_test_user()
    
    # Create templates
    templates = await create_test_templates(user.id)
    
    # Create messages
    if templates:
        await create_test_messages(user.id, templates[0].id)
    else:
        await create_test_messages(user.id)
    
    # Create campaigns
    await create_test_campaigns(user.id)
    
    print("\n✅ Database seeded successfully with frontend development data!")
    print(f"📱 Test user: test@example.com")
    print(f"🔑 Password: Test1234!")

if __name__ == "__main__":
    asyncio.run(seed_database())
</file>

<file path="scripts/setup_test_db.py">
#!/usr/bin/env python
"""
Setup a test database for development.

This script:
1. Checks if the test database exists
2. Creates it if it doesn't
3. Runs all migrations
4. Seeds it with test data

Usage:
    python scripts/setup_test_db.py
"""
import sys
import os
import asyncio
import subprocess
from pathlib import Path
import re

# Add parent directory to path to allow importing app
sys.path.append(str(Path(__file__).parent.parent))

# Import app modules
from app.core.config import settings
from app.db.session import initialize_database
from app.db.repositories.users import UserRepository
from app.db.repositories.templates import TemplateRepository
from app.core.security import get_password_hash

# Test data to seed
TEST_USER = {
    "email": "test@example.com",
    "password": "Test1234!",
    "full_name": "Test User",
    "role": "user"
}

TEST_TEMPLATES = [
    {
        "name": "Welcome Message",
        "content": "Hi {{name}}, welcome to our service! We're glad you've joined us.",
        "description": "Template for welcoming new users"
    },
    {
        "name": "OTP Verification",
        "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
        "description": "Template for sending OTP codes"
    },
    {
        "name": "Appointment Reminder",
        "content": "Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule.",
        "description": "Template for appointment reminders"
    }
]

# Extract database connection info from the URL
def parse_db_url(url):
    """Parse database URL to extract connection information."""
    # PostgreSQL URL format: postgresql+asyncpg://user:password@host:port/dbname
    pattern = r"postgresql(?:\+asyncpg)?://([^:]+):([^@]+)@([^:]+):(\d+)/([^?]+)"
    match = re.match(pattern, url)
    
    if match:
        return {
            "user": match.group(1),
            "password": match.group(2),
            "host": match.group(3),
            "port": match.group(4),
            "dbname": match.group(5)
        }
    return None

async def create_test_user():
    """Create a test user if it doesn't exist."""
    from app.db.session import async_session_factory
    
    async with async_session_factory() as session:
        # Create user repository
        user_repo = UserRepository(session)
        
        # Check if user exists
        existing_user = await user_repo.get_by_email(TEST_USER["email"])
        if existing_user:
            print(f"✅ Test user {TEST_USER['email']} already exists")
            return existing_user
        
        # Create user
        hashed_password = get_password_hash(TEST_USER["password"])
        user = await user_repo.create(
            email=TEST_USER["email"],
            hashed_password=hashed_password,
            full_name=TEST_USER["full_name"],
            role=TEST_USER["role"]
        )
        
        print(f"✅ Created test user: {user.email}")
        return user

async def create_test_templates(user_id):
    """Create test message templates."""
    from app.db.session import async_session_factory
    
    async with async_session_factory() as session:
        # Create template repository
        template_repo = TemplateRepository(session)
        
        # Create templates
        for template_data in TEST_TEMPLATES:
            await template_repo.create_template(
                name=template_data["name"],
                content=template_data["content"],
                description=template_data["description"],
                user_id=user_id
            )
            print(f"✅ Created template: {template_data['name']}")

async def setup_database():
    """Setup the test database."""
    try:
        # Get database configuration from settings
        db_info = parse_db_url(settings.DATABASE_URL)
        if not db_info:
            print("❌ Could not parse database URL. Please check the format.")
            return False
        
        # Extract database name and create connection to postgres database
        db_name = db_info["dbname"]
        postgres_url = f"postgresql://{db_info['user']}:{db_info['password']}@{db_info['host']}:{db_info['port']}/postgres"
        
        # Get environment variables for PGPASSWORD to avoid password prompt
        env = os.environ.copy()
        env["PGPASSWORD"] = db_info["password"]
        
        # Test connection using psql
        connection_test = subprocess.run(
            ["psql", 
             "-h", db_info["host"], 
             "-p", db_info["port"], 
             "-U", db_info["user"], 
             "-d", "postgres", 
             "-c", "SELECT 1;"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env
        )
        
        if connection_test.returncode != 0:
            print("❌ Could not connect to PostgreSQL server.")
            print(connection_test.stderr.decode())
            print("Please make sure PostgreSQL is running and the credentials are correct.")
            return False
        
        # Check if database exists
        check_db = subprocess.run(
            ["psql", 
             "-h", db_info["host"], 
             "-p", db_info["port"], 
             "-U", db_info["user"], 
             "-d", "postgres", 
             "-c", f"SELECT 1 FROM pg_database WHERE datname = '{db_name}';"],
            stdout=subprocess.PIPE,
            env=env
        )
        
        if "1 row" not in check_db.stdout.decode():
            print(f"Creating database {db_name}...")
            create_db = subprocess.run(
                ["psql", 
                 "-h", db_info["host"], 
                 "-p", db_info["port"], 
                 "-U", db_info["user"], 
                 "-d", "postgres", 
                 "-c", f"CREATE DATABASE {db_name};"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env
            )
            
            if create_db.returncode != 0:
                print(f"❌ Failed to create database {db_name}")
                print(create_db.stderr.decode())
                return False
            
            print(f"✅ Database {db_name} created successfully")
        else:
            print(f"✅ Database {db_name} already exists")
        
        # Run migrations
        print("Running Alembic migrations...")
        alembic = subprocess.run(
            ["alembic", "upgrade", "head"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        if alembic.returncode != 0:
            print("❌ Failed to run migrations")
            print(alembic.stderr.decode())
            return False
        
        print("✅ Migrations applied successfully")
        
        # Initialize database
        await initialize_database()
        
        # Create test user
        user = await create_test_user()
        
        # Create test templates
        await create_test_templates(user.id)
        
        print("\n🎉 Test database setup complete!")
        print(f"📝 Test user: {TEST_USER['email']}")
        print(f"🔑 Password: {TEST_USER['password']}")
        
        return True
        
    except Exception as e:
        print(f"❌ Error setting up test database: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(setup_database())
</file>

<file path="tests/core_functionality_test.py">
import pytest
import asyncio
from datetime import datetime, timezone, timedelta
from uuid import uuid4
from unittest.mock import AsyncMock, patch, MagicMock

from app.schemas.message import MessageStatus
from app.db.repositories.messages import MessageRepository
from app.db.repositories.templates import TemplateRepository
from app.db.repositories.campaigns import CampaignRepository
from app.services.event_bus.bus import get_event_bus
from app.services.sms.sender import SMSSender
from app.utils.phone import validate_phone


@pytest.fixture
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


import pytest_asyncio

@pytest_asyncio.fixture
async def event_bus():
    bus = get_event_bus()
    await bus.initialize()
    yield bus
    await bus.shutdown()


@pytest.fixture
def mock_session():
    session = AsyncMock()
    session.commit = AsyncMock()
    session.rollback = AsyncMock()
    session.close = AsyncMock()
    session.execute = AsyncMock()
    session.refresh = AsyncMock()
    session.add = MagicMock()
    session.delete = AsyncMock()
    # Mock session.begin for async context
    session.begin = MagicMock(return_value=AsyncMock(__aenter__=AsyncMock(return_value=None), __aexit__=AsyncMock(return_value=None)))
    return session


@pytest.fixture
def message_repository(mock_session):
    return MessageRepository(mock_session)


@pytest.fixture
def template_repository(mock_session):
    return TemplateRepository(mock_session)


@pytest.fixture
def campaign_repository(mock_session):
    return CampaignRepository(mock_session)


@pytest.fixture
def sms_sender(message_repository, event_bus):
    return SMSSender(message_repository, event_bus)


def test_phone_validation_valid_numbers():
    valid_numbers = ["+12025550108", "+447911123456", "+61412345678", "+33612345678"]
    for number in valid_numbers:
        is_valid, formatted, error, _ = validate_phone(number)
        assert is_valid
        assert formatted.startswith("+")


def test_phone_validation_invalid_numbers():
    invalid_numbers = ["not a number", "123", "+1234567890123456789", "+123abcd5678"]
    for number in invalid_numbers:
        is_valid, *_ = validate_phone(number)
        assert not is_valid


@pytest.mark.asyncio
async def test_create_message(message_repository):
    message_repository.session.refresh.side_effect = lambda x: x
    mock_campaign = MagicMock()
    mock_campaign.total_messages = 0
    with patch("app.db.repositories.campaigns.CampaignRepository.get_by_id", new_callable=AsyncMock, return_value=mock_campaign):
        message = await message_repository.create_message(
            phone_number="+12025550108",
            message_text="Test message",
            user_id="user-123",
            custom_id="custom-123",
            campaign_id="campaign-123"
        )
        assert message.phone_number == "+12025550108"
        assert message.status == MessageStatus.PENDING
        assert message_repository.session.add.call_count >= 1


@pytest.mark.asyncio
async def test_update_message_status(message_repository):
    message = MagicMock()
    message.id = "msg-123"
    message_repository.get_by_id = AsyncMock(return_value=message)
    message_repository.session.refresh.side_effect = lambda x: x
    updated_message = await message_repository.update_message_status(
        message_id="msg-123",
        status=MessageStatus.SENT,
        event_type="test",
        gateway_message_id="gw-123"
    )
    assert updated_message is not None
    message_repository.get_by_id.assert_awaited_with("msg-123")
    assert message_repository.session.add.call_count >= 1


@pytest.mark.asyncio
async def test_apply_template(template_repository):
    template = MagicMock()
    template.content = "Hello {{name}}, your code is {{code}}"
    template_repository.get_by_id = AsyncMock(return_value=template)
    result = await template_repository.apply_template(
        template_id="template-123",
        variables={"name": "John", "code": "123456"}
    )
    assert result == "Hello John, your code is 123456"
    template_repository.get_by_id.assert_awaited_with("template-123")


@pytest.mark.asyncio
async def test_create_template(template_repository):
    template_repository.session.refresh.side_effect = lambda x: x
    template = await template_repository.create_template(
        name="Test Template",
        content="Hello {{name}}",
        description="Test description",
        user_id="user-123"
    )
    assert template.name == "Test Template"
    template_repository.session.add.assert_called_with(template)
    template_repository.session.commit.assert_called()


@pytest.mark.asyncio
async def test_create_campaign(campaign_repository):
    campaign_repository.session.refresh.side_effect = lambda x: x
    campaign = await campaign_repository.create_campaign(
        name="Test Campaign",
        description="Test description",
        user_id="user-123",
        scheduled_start_at=datetime.now(timezone.utc) + timedelta(days=1)
    )
    assert campaign.name == "Test Campaign"
    campaign_repository.session.add.assert_called_with(campaign)
    campaign_repository.session.commit.assert_called()


@pytest.mark.asyncio
async def test_update_campaign_status(campaign_repository):
    campaign = MagicMock()
    campaign.id = "campaign-123"
    campaign.status = "draft"
    campaign_repository.get_by_id = AsyncMock(return_value=campaign)
    with patch("app.services.event_bus.bus.get_event_bus") as mock_get_bus:
        mock_bus = AsyncMock()
        mock_get_bus.return_value = mock_bus
        updated_campaign = await campaign_repository.update_campaign_status(
            campaign_id="campaign-123",
            status="active"
        )
        assert updated_campaign is not None
        assert campaign.status == "active"
        campaign_repository.session.add.assert_called_with(campaign)
        assert mock_bus.publish.called


@pytest.mark.asyncio
async def test_event_bus_subscribe_publish(event_bus):
    callback = AsyncMock()
    subscriber_id = await event_bus.subscribe("test_event", callback)
    assert event_bus.get_subscriber_count("test_event") == 1
    test_data = {"test": "data"}
    success = await event_bus.publish("test_event", test_data)
    assert success
    callback.assert_called_once()
    call_args = callback.call_args[0][0]
    assert call_args["test"] == "data"
    assert call_args["event_type"] == "test_event"
    assert "timestamp" in call_args
    assert "event_id" in call_args
    await event_bus.unsubscribe("test_event", subscriber_id)
    assert event_bus.get_subscriber_count("test_event") == 0


@pytest.mark.asyncio
async def test_sms_sender_send_message(sms_sender):
    sms_sender._send_to_gateway = AsyncMock(return_value={
        "status": MessageStatus.SENT,
        "gateway_message_id": "gw-123"
    })

    mock_message = MagicMock()
    mock_message.id = "msg-123"
    mock_message.dict = MagicMock(return_value={"id": "msg-123"})

    sms_sender.message_repository.create_message = AsyncMock(return_value=mock_message)
    sms_sender.message_repository.update_message_status = AsyncMock(return_value=mock_message)
    sms_sender.message_repository.get_by_id = AsyncMock(return_value=mock_message)  # ✅ add this line

    result = await sms_sender.send_message(
        phone_number="+12025550108",
        message_text="Test message",
        user_id="user-123"
    )

    assert result == {"id": "msg-123"}
    assert sms_sender.message_repository.create_message.called
    assert sms_sender._send_to_gateway.called
    assert sms_sender.message_repository.update_message_status.called
</file>

<file path="tests/unit/api/messages/test_messages_endpoints.py">
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_send_valid_batch(async_client: AsyncClient, override_auth):
    payload = {
        "messages": [
            {"phone_number": "+1234567890", "message": "Batch Msg 1"},
            {"phone_number": "+1987654321", "message": "Batch Msg 2"}
        ],
        "options": {}
    }
    response = await async_client.post("/api/v1/messages/batch", json=payload)
    assert response.status_code == 202
    assert "results" in response.json()

@pytest.mark.asyncio
async def test_send_empty_batch(async_client: AsyncClient, override_auth):
    payload = {
        "messages": [],
        "options": {}
    }
    response = await async_client.post("/api/v1/messages/batch", json=payload)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_batch_with_invalid_phone(async_client: AsyncClient, override_auth):
    payload = {
        "messages": [
            {"phone_number": "invalid", "message": "Failing message"}
        ],
        "options": {}
    }
    response = await async_client.post("/api/v1/messages/batch", json=payload)
    assert response.status_code in [422, 502]


@pytest.mark.asyncio
async def test_delete_existing_message(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    response = await async_client.delete(f"/api/v1/messages/{message_id}")
    assert response.status_code == 204

@pytest.mark.asyncio
async def test_delete_nonexistent_message(async_client: AsyncClient, override_auth):
    message_id = "nonexistent-msg-id"
    response = await async_client.delete(f"/api/v1/messages/{message_id}")
    assert response.status_code == 404

@pytest.mark.asyncio
async def test_get_existing_message(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    response = await async_client.get(f"/api/v1/messages/{message_id}")
    assert response.status_code == 200
    assert response.json()["id"] == message_id

@pytest.mark.asyncio
async def test_get_nonexistent_message(async_client: AsyncClient, override_auth):
    message_id = "nonexistent-msg-id"
    response = await async_client.get(f"/api/v1/messages/{message_id}")
    assert response.status_code == 404


@pytest.mark.asyncio
async def test_import_valid_csv(async_client: AsyncClient, override_auth):
    csv_content = "phone\n+1234567890\n+1987654321"
    file = {"file": ("contacts.csv", csv_content, "text/csv")}
    params = {
        "message_template": "Test message",
        "delimiter": ",",
        "has_header": "true",
        "phone_column": "phone"
    }
    response = await async_client.post("/api/v1/messages/import", params=params, files=file)
    assert response.status_code == 202
    assert "task_id" in response.json()

@pytest.mark.asyncio
async def test_import_missing_column(async_client: AsyncClient, override_auth):
    csv_content = "name\nAlice\nBob"
    file = {"file": ("contacts.csv", csv_content, "text/csv")}
    params = {
        "message_template": "Hi",
        "delimiter": ",",
        "has_header": "true",
        "phone_column": "phone"
    }
    response = await async_client.post("/api/v1/messages/import", params=params, files=file)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_import_invalid_format(async_client: AsyncClient, override_auth):
    csv_content = "random|data|columns"
    file = {"file": ("contacts.csv", csv_content, "text/csv")}
    params = {
        "message_template": "Hi again",
        "delimiter": ",",
        "has_header": "true",
        "phone_column": "phone"
    }
    response = await async_client.post("/api/v1/messages/import", params=params, files=file)
    assert response.status_code == 422


@pytest.mark.asyncio
async def test_list_all_messages(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert isinstance(data["items"], list)
    assert "page_info" in data
    assert isinstance(data["page_info"], dict)

@pytest.mark.asyncio
async def test_filter_messages_by_status(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/?status=sent")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert isinstance(data["items"], list)

@pytest.mark.asyncio
async def test_filter_messages_by_phone(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/?phone_number=+1234567890")
    assert response.status_code == 200
    data = response.json()
    assert "items" in data
    assert isinstance(data["items"], list)

@pytest.mark.asyncio
async def test_filter_messages_invalid_date(async_client: AsyncClient, override_auth):
    response = await async_client.get("/api/v1/messages/?from_date=invalid-date")
    assert response.status_code in [422, 500]

@pytest.mark.asyncio
async def test_send_valid_message(async_client: AsyncClient, override_auth):
    payload = {
        "phone_number": "+1234567890",
        "message": "Hello there!",
        "scheduled_at": None,
        "custom_id": "test-msg-123"
    }
    response = await async_client.post("/api/v1/messages/send", json=payload)
    assert response.status_code == 202
    assert "id" in response.json()

@pytest.mark.asyncio
async def test_send_invalid_phone(async_client: AsyncClient, override_auth):
    payload = {
        "phone_number": "invalid-phone",
        "message": "Hello there!",
        "scheduled_at": None,
        "custom_id": "test-msg-456"
    }
    response = await async_client.post("/api/v1/messages/send", json=payload)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_send_empty_message(async_client: AsyncClient, override_auth):
    payload = {
        "phone_number": "+1234567890",
        "message": "",
        "scheduled_at": None,
        "custom_id": "test-msg-789"
    }
    response = await async_client.post("/api/v1/messages/send", json=payload)
    assert response.status_code == 422

@pytest.mark.asyncio
async def test_get_existing_task_status(async_client: AsyncClient, override_auth):
    task_id = "existing-task-id"
    response = await async_client.get(f"/api/v1/messages/tasks/{task_id}")
    assert response.status_code == 200
    assert "status" in response.json()

@pytest.mark.asyncio
async def test_get_nonexistent_task_status(async_client: AsyncClient, override_auth):
    task_id = "nonexistent-task-id"
    response = await async_client.get(f"/api/v1/messages/tasks/{task_id}")
    assert response.status_code == 404

@pytest.mark.asyncio
async def test_update_valid_message_status(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    payload = {
        "status": "delivered",
        "reason": "confirmed by carrier"
    }
    response = await async_client.put(f"/api/v1/messages/{message_id}/status", json=payload)
    assert response.status_code == 200
    assert response.json()["status"] == "delivered"

@pytest.mark.asyncio
async def test_update_status_message_not_found(async_client: AsyncClient, override_auth):
    message_id = "nonexistent-msg-id"
    payload = {
        "status": "failed",
        "reason": "user unreachable"
    }
    response = await async_client.put(f"/api/v1/messages/{message_id}/status", json=payload)
    assert response.status_code == 404

@pytest.mark.asyncio
async def test_update_status_invalid_value(async_client: AsyncClient, override_auth):
    message_id = "existing-msg-id"
    payload = {
        "status": "not_a_valid_status",
        "reason": "some reason"
    }
    response = await async_client.put(f"/api/v1/messages/{message_id}/status", json=payload)
    assert response.status_code == 422
</file>

<file path=".gitignore">
# Python specific
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.env
*.venv
env/
venv/
ENV/
.venv/
env.bak/
venv.bak/
*.egg
*.egg-info/
dist/
build/
*.log

# FastAPI specific
instance/
*.db
*.sqlite3

# IDE specific
.vscode/
.idea/
*.swp
*.swo

# OS generated files
.DS_Store
Thumbs.db

# Test and coverage reports
htmlcov/
.tox/
.nox/
.coverage
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
</file>

<file path="alembic/README">
Generic single-database configuration.
</file>

<file path="alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="app/api/v1/endpoints/auth.py">
"""
API endpoints for authentication.
"""
from datetime import datetime, timedelta, timezone
from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException, status, Body
from fastapi.security import OAuth2PasswordRequestForm

from app.core.config import settings
from app.core.exceptions import AuthenticationError, AuthorizationError, NotFoundError
from app.api.v1.dependencies import (
    get_current_user,
    get_user_repository,
    validate_permissions
)
from app.schemas.user import (
    User,
    UserCreate,
    Token,
    APIKey,
    APIKeyCreate
)
from app.core.security import (
    create_access_token,
    verify_password,
    get_password_hash
)

router = APIRouter()


@router.post("/token", response_model=Token)
async def login_for_access_token(
    form_data: OAuth2PasswordRequestForm = Depends(),
    user_repository = Depends(get_user_repository)
):
    """
    OAuth2 compatible token login, get an access token for future requests.
    """
    try:
        # Authenticate user
        user = await user_repository.get_by_email(form_data.username)
        if not user:
            raise AuthenticationError("Incorrect email or password")
        
        # Verify password
        if not verify_password(form_data.password, user.hashed_password):
            raise AuthenticationError("Incorrect email or password")
        
        # Check if user is active
        if not user.is_active:
            raise AuthenticationError("Inactive user")
        
        # Create access token
        access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
        expires_at = datetime.now(timezone.utc) + access_token_expires
        
        access_token = create_access_token(
            data={
                "sub": str(user.id),
                "role": user.role,
                "exp": expires_at
            },
            expires_delta=access_token_expires
        )
        
        return {
            "access_token": access_token,
            "token_type": "bearer",
            "expires_at": expires_at
        }
        
    except AuthenticationError as e:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=str(e),
            headers={"WWW-Authenticate": "Bearer"},
        )


@router.post("/register", response_model=User, status_code=status.HTTP_201_CREATED)
async def register_user(
    user_data: UserCreate,
    user_repository = Depends(get_user_repository)
):
    """
    Register a new user.
    """
    # Check if user already exists
    existing_user = await user_repository.get_by_email(user_data.email)
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="Email already registered"
        )
    
    # Create new user
    hashed_password = get_password_hash(user_data.password)
    
    user = await user_repository.create(
        email=user_data.email,
        hashed_password=hashed_password,
        full_name=user_data.full_name,
        is_active=user_data.is_active,
        role=user_data.role
    )
    
    return user


@router.get("/me", response_model=User)
async def read_users_me(
    current_user: User = Depends(get_current_user)
):
    """
    Get current user information.
    """
    return current_user


@router.post("/keys", response_model=APIKey)
async def create_api_key(
    api_key_data: APIKeyCreate,
    current_user: User = Depends(get_current_user),
    user_repository = Depends(get_user_repository)
):
    """
    Create a new API key.
    
    This is the only time the full API key will be returned.
    """
    # Create API key
    api_key = await user_repository.create_api_key(
        user_id=current_user.id,
        name=api_key_data.name,
        expires_at=api_key_data.expires_at,
        permissions=api_key_data.permissions
    )
    
    return api_key


@router.get("/keys", response_model=List[APIKey])
async def list_api_keys(
    current_user: User = Depends(get_current_user),
    user_repository = Depends(get_user_repository)
):
    """
    List all API keys for the current user.
    
    Note: The full API key value is not returned, only the ID and metadata.
    """
    # List API keys
    api_keys = await user_repository.list_api_keys(user_id=current_user.id)
    
    # Remove sensitive information
    for key in api_keys:
        key.key = f"{key.key[:8]}..." if key.key else None
    
    return api_keys


@router.delete("/keys/{key_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_api_key(
    key_id: str,
    current_user: User = Depends(get_current_user),
    user_repository = Depends(get_user_repository)
):
    """
    Delete an API key.
    """
    # Get API key
    api_key = await user_repository.get_api_key_by_id(key_id)
    if not api_key:
        raise NotFoundError(message="API key not found")
    
    # Check ownership
    if api_key.user_id != str(current_user.id) and current_user.role != "admin":
        raise AuthorizationError(message="Not authorized to delete this API key")
    
    # Delete API key
    success = await user_repository.delete_api_key(key_id)
    if not success:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete API key"
        )
    
    return None
</file>

<file path="app/api/v1/endpoints/templates.py">
# app/api/v1/endpoints/templates.py
"""
API endpoints for message templates.
"""
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Path, Query, status, Body
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from app.api.v1.dependencies import get_current_user
from app.core.exceptions import ValidationError, NotFoundError
from app.schemas.template import (
    MessageTemplateCreate,
    MessageTemplateUpdate,
    MessageTemplateResponse,
    MessageWithTemplate
)
from app.schemas.user import User
from app.utils.pagination import PaginationParams, paginate_response
from app.services.sms.sender import get_sms_sender
from app.db.session import get_repository_context

router = APIRouter()


@router.post("/", response_model=MessageTemplateResponse, status_code=status.HTTP_201_CREATED)
async def create_template(
    template: MessageTemplateCreate,
    current_user: User = Depends(get_current_user)
):
    """
    Create a new message template.
    
    Templates can include variables in the format {{variable_name}} which will
    be replaced when sending messages.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Create template
            result = await template_repo.create_template(
                name=template.name,
                content=template.content,
                description=template.description,
                variables=template.variables,
                is_active=template.is_active,
                user_id=current_user.id
            )
            
            return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating template: {str(e)}")


@router.get("/", response_model=Dict[str, Any])
async def list_templates(
    pagination: PaginationParams = Depends(),
    active_only: bool = Query(False, description="Return only active templates"),
    current_user: User = Depends(get_current_user)
):
    """
    List message templates for the current user.
    
    Returns a paginated list of templates.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get templates
            templates, total = await template_repo.get_templates_for_user(
                user_id=current_user.id,
                active_only=active_only,
                skip=pagination.skip,
                limit=pagination.limit
            )
            
            # Return paginated response
            return paginate_response(
                items=[template.dict() for template in templates],
                total=total,
                pagination=pagination
            )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing templates: {str(e)}")


@router.get("/{template_id}", response_model=MessageTemplateResponse)
async def get_template(
    template_id: str = Path(..., description="Template ID"),
    current_user: User = Depends(get_current_user)
):
    """
    Get a specific message template.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to access this template")
            
            return template
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting template: {str(e)}")


@router.put("/{template_id}", response_model=MessageTemplateResponse)
async def update_template(
    template_update: MessageTemplateUpdate,
    template_id: str = Path(..., description="Template ID"),
    current_user: User = Depends(get_current_user)
):
    """
    Update a message template.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to update this template")
            
            # Extract variables from content if content was updated
            update_data = template_update.dict(exclude_unset=True)
            if "content" in update_data:
                import re
                pattern = r"{{([a-zA-Z0-9_]+)}}"
                update_data["variables"] = list(set(re.findall(pattern, update_data["content"])))
            
            # Update template
            updated_template = await template_repo.update(id=template_id, obj_in=update_data)
            
            return updated_template
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating template: {str(e)}")


@router.delete("/{template_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_template(
    template_id: str = Path(..., description="Template ID"),
    current_user: User = Depends(get_current_user)
):
    """
    Delete a message template.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to delete this template")
            
            # Delete template
            success = await template_repo.delete(id=template_id)
            
            if not success:
                raise HTTPException(status_code=500, detail="Failed to delete template")
            
            # Return no content
            return None
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting template: {str(e)}")


# Create a Pydantic model for the apply request
class TemplateApplyRequest(BaseModel):
    """Request model for applying a template."""
    template_id: str
    variables: Dict[str, str]


@router.post("/apply")
async def apply_template(
    request: TemplateApplyRequest = Body(...),
    current_user: User = Depends(get_current_user)
):
    """
    Apply variables to a template and return the result.
    
    This endpoint is useful for previewing how a template will look with specific variables.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(request.template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {request.template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to access this template")
            
            # Apply template
            result = await template_repo.apply_template(
                template_id=request.template_id,
                variables=request.variables
            )
            
            # Check for missing variables
            import re
            missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", result)
            
            return {
                "result": result,
                "missing_variables": missing_vars
            }
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error applying template: {str(e)}")


@router.post("/send", status_code=status.HTTP_202_ACCEPTED)
async def send_with_template(
    message: MessageWithTemplate,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender)
):
    """
    Send a message using a template.
    
    Applies the provided variables to the template and sends the resulting message.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.templates import TemplateRepository
        
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(message.template_id)
            
            # Check if template exists
            if not template:
                raise NotFoundError(message=f"Template {message.template_id} not found")
            
            # Check authorization
            if template.user_id != current_user.id:
                raise HTTPException(status_code=403, detail="Not authorized to use this template")
            
            # Apply template
            message_text = await template_repo.apply_template(
                template_id=message.template_id,
                variables=message.variables
            )
            
            # Check for missing variables
            import re
            missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", message_text)
            if missing_vars:
                raise ValidationError(
                    message="Missing template variables", 
                    details={"missing_variables": missing_vars}
                )
        
        # Send message using sms_sender which already uses context managers internally
        result = await sms_sender.send_message(
            phone_number=message.phone_number,
            message_text=message_text,
            user_id=current_user.id,
            scheduled_at=message.scheduled_at,
            custom_id=message.custom_id,
            metadata={"template_id": message.template_id, "template_variables": message.variables}
        )
        
        return result
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error sending message: {str(e)}")
</file>

<file path="app/core/exceptions.py">
"""
Custom exception classes for Inboxerr Backend.
"""
from typing import Any, Dict, Optional


class InboxerrException(Exception):
    """Base exception class for Inboxerr application."""
    
    def __init__(
        self,
        message: str,
        code: str = "INTERNAL_ERROR",
        status_code: int = 500,
        details: Optional[Dict[str, Any]] = None,
    ):
        self.message = message
        self.code = code
        self.status_code = status_code
        self.details = details or {}
        super().__init__(message)


class AuthenticationError(InboxerrException):
    """Raised when authentication fails."""
    
    def __init__(
        self,
        message: str = "Authentication failed",
        code: str = "AUTHENTICATION_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=401, details=details)


class AuthorizationError(InboxerrException):
    """Raised when a user doesn't have permission."""
    
    def __init__(
        self,
        message: str = "Not authorized",
        code: str = "AUTHORIZATION_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=403, details=details)


class ValidationError(InboxerrException):
    """Raised for validation errors."""
    
    def __init__(
        self,
        message: str = "Validation error",
        code: str = "VALIDATION_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=422, details=details)


class NotFoundError(InboxerrException):
    """Raised when a resource is not found."""
    
    def __init__(
        self,
        message: str = "Resource not found",
        code: str = "NOT_FOUND",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=404, details=details)


class SMSGatewayError(InboxerrException):
    """Raised when there's an error with the SMS gateway."""
    
    def __init__(
        self,
        message: str = "SMS Gateway error",
        code: str = "SMS_GATEWAY_ERROR",
        details: Optional[Dict[str, Any]] = None,
        status_code: int = 502,
    ):
        super().__init__(message=message, code=code, status_code=status_code, details=details)


class RetryableError(InboxerrException):
    """Error that can be retried."""
    
    def __init__(
        self,
        message: str = "Retryable error",
        code: str = "RETRYABLE_ERROR",
        details: Optional[Dict[str, Any]] = None,
        retry_after: int = 60,
    ):
        details = details or {}
        details["retry_after"] = retry_after
        super().__init__(message=message, code=code, status_code=503, details=details)


class WebhookError(InboxerrException):
    """Raised when there's an issue with webhook processing."""
    
    def __init__(
        self,
        message: str = "Webhook error",
        code: str = "WEBHOOK_ERROR",
        details: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(message=message, code=code, status_code=400, details=details)


class SMSAuthError(SMSGatewayError):
    """Raised when SMS gateway credentials are invalid."""
    def __init__(
        self,
        message: str = "Invalid SMS gateway credentials",
        details: Optional[Dict[str, Any]] = None
    ):
        super().__init__(
            message=message,
            code="SMS_AUTH_ERROR",
            status_code=401,
            details=details
        )
</file>

<file path="app/core/security.py">
"""
Security utilities for authentication and authorization.
"""
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional, Union
import jwt
from passlib.context import CryptContext
import secrets
import string

from app.core.config import settings

# Password hashing context
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """
    Verify a password against a hash.
    
    Args:
        plain_password: Plain-text password
        hashed_password: Hashed password
        
    Returns:
        bool: True if password matches hash
    """
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    """
    Hash a password.
    
    Args:
        password: Plain-text password
        
    Returns:
        str: Hashed password
    """
    return pwd_context.hash(password)


def create_access_token(
    data: Dict[str, Any],
    expires_delta: Optional[timedelta] = None
) -> str:
    """
    Create a JWT access token.
    
    Args:
        data: Data to encode in the token
        expires_delta: Token expiration time
        
    Returns:
        str: JWT token
    """
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(minutes=15)
    
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(
        to_encode,
        settings.SECRET_KEY,
        algorithm="HS256"
    )
    
    return encoded_jwt


def generate_api_key() -> str:
    """
    Generate a secure API key.
    
    Returns:
        str: API key
    """
    # Characters to use in API key
    alphabet = string.ascii_letters + string.digits
    
    # Generate a random string for the API key
    api_key = ''.join(secrets.choice(alphabet) for _ in range(32))
    
    # Add prefix for identification
    return f"ibx_{''.join(secrets.choice(alphabet) for _ in range(8))}_{api_key}"


def validate_api_key(api_key: str) -> bool:
    """
    Validate API key format.
    
    Args:
        api_key: API key to validate
        
    Returns:
        bool: True if format is valid
    """
    # Check format (prefix_random_key)
    parts = api_key.split('_')
    if len(parts) != 3:
        return False
    
    prefix, random_part, key = parts
    
    # Validate prefix
    if prefix != "ibx":
        return False
    
    # Validate random part length
    if len(random_part) != 8:
        return False
    
    # Validate key length
    if len(key) != 32:
        return False
    
    # Validate characters
    valid_chars = set(string.ascii_letters + string.digits)
    return all(c in valid_chars for c in random_part + key)


def generate_webhook_signing_key() -> str:
    """
    Generate a secure webhook signing key.
    
    Returns:
        str: Webhook signing key
    """
    # Generate a random string for the signing key
    return secrets.token_hex(32)  # 64 character hex string


def create_hmac_signature(payload: str, secret_key: str, timestamp: str) -> str:
    """
    Create HMAC signature for webhook payload validation.
    
    Args:
        payload: JSON payload as string
        secret_key: Secret key for signing
        timestamp: Timestamp string
        
    Returns:
        str: HMAC signature
    """
    import hmac
    import hashlib
    
    message = (payload + timestamp).encode()
    signature = hmac.new(
        secret_key.encode(),
        message,
        hashlib.sha256
    ).hexdigest()
    
    return signature


def verify_webhook_signature(
    payload: str,
    signature: str,
    secret_key: str,
    timestamp: str,
    tolerance: int = 300
) -> bool:
    """
    Verify webhook signature.
    
    Args:
        payload: JSON payload as string
        signature: Signature to verify
        secret_key: Secret key for signing
        timestamp: Timestamp used in signature
        tolerance: Timestamp tolerance in seconds
        
    Returns:
        bool: True if signature is valid
    """
    import hmac
    import time
    
    # Verify timestamp is within tolerance
    try:
        ts = int(timestamp)
        current_time = int(time.time())
        if abs(current_time - ts) > tolerance:
            return False
    except (ValueError, TypeError):
        return False
    
    # Calculate expected signature
    expected = create_hmac_signature(payload, secret_key, timestamp)
    
    # Compare signatures (constant-time comparison)
    return hmac.compare_digest(expected, signature)
</file>

<file path="app/db/repositories/base.py">
"""
Base repository with common database operations.
"""
from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union
from uuid import uuid4

from fastapi.encoders import jsonable_encoder
from pydantic import BaseModel
from sqlalchemy import select, update, delete
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.base import Base

# Define generic types for models
ModelType = TypeVar("ModelType", bound=Base)
CreateSchemaType = TypeVar("CreateSchemaType", bound=BaseModel)
UpdateSchemaType = TypeVar("UpdateSchemaType", bound=BaseModel)


class BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):
    """
    Base repository with common CRUD operations.
    
    Generic repository pattern implementation for database access.
    """
    
    def __init__(self, session: AsyncSession, model: Type[ModelType]):
        """
        Initialize repository with session and model.
        
        Args:
            session: Database session
            model: SQLAlchemy model class
        """
        self.session = session
        self.model = model
        self.session_is_owned = False
    
    async def __aenter__(self):
        """Support async context manager protocol."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Clean up resources when exiting context."""
        if self._session_is_owned:
            await self.close()
    
    async def close(self):
        """Close the session if we own it."""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def get_by_id(self, id: str) -> Optional[ModelType]:
        """
        Get a record by ID.
        
        Args:
            id: Record ID
            
        Returns:
            ModelType: Found record or None
        """
        query = select(self.model).where(self.model.id == id)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def get_by_attribute(self, attr_name: str, attr_value: Any) -> Optional[ModelType]:
        """
        Get a record by a specific attribute.
        
        Args:
            attr_name: Attribute name
            attr_value: Attribute value
            
        Returns:
            ModelType: Found record or None
        """
        query = select(self.model).where(getattr(self.model, attr_name) == attr_value)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def list(
        self, 
        *,
        filters: Optional[Dict[str, Any]] = None,
        skip: int = 0, 
        limit: int = 100
    ) -> List[ModelType]:
        """
        Get a list of records with optional filtering.
        
        Args:
            filters: Optional filters as dict
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            List[ModelType]: List of records
        """
        query = select(self.model)
        
        # Apply filters if provided
        if filters:
            for attr_name, attr_value in filters.items():
                if hasattr(self.model, attr_name) and attr_value is not None:
                    query = query.where(getattr(self.model, attr_name) == attr_value)
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def create(self, *, obj_in: Union[CreateSchemaType, Dict[str, Any]]) -> ModelType:
        """
        Create a new record.
        
        Args:
            obj_in: Data to create record with
            
        Returns:
            ModelType: Created record
        """
        # Convert to dict if it's a Pydantic model
        obj_in_data = obj_in if isinstance(obj_in, dict) else obj_in.dict(exclude_unset=True)
        
        # Create model instance
        db_obj = self.model(**obj_in_data)
        
        # Generate ID if not provided
        if not db_obj.id:
            db_obj.id = str(uuid4())
        
        # Add to session
        self.session.add(db_obj)
        
        return db_obj
    
    async def update(
        self, 
        *,
        id: str,
        obj_in: Union[UpdateSchemaType, Dict[str, Any]]
    ) -> Optional[ModelType]:
        """
        Update a record.
        
        Args:
            id: Record ID
            obj_in: Data to update record with
            
        Returns:
            ModelType: Updated record or None
        """
        # Get current record
        db_obj = await self.get_by_id(id)
        if not db_obj:
            return None
        
        # Convert to dict if it's a Pydantic model
        update_data = obj_in if isinstance(obj_in, dict) else obj_in.dict(exclude_unset=True)
        
        # Remove None values
        update_data = {k: v for k, v in update_data.items() if v is not None}
        
        # Update record
        for field, value in update_data.items():
            if hasattr(db_obj, field):
                setattr(db_obj, field, value)
        
        # Save changes
        self.session.add(db_obj)
        
        return db_obj
    
    async def delete(self, *, id: str) -> bool:
        """
        Delete a record.
        
        Args:
            id: Record ID
            
        Returns:
            bool: True if deleted, False if not found
        """
        # Check if record exists
        db_obj = await self.get_by_id(id)
        if not db_obj:
            return False
        
        # Delete record
        await self.session.delete(db_obj)        
        return True
    
    async def count(self, *, filters: Optional[Dict[str, Any]] = None) -> int:
        """
        Count records with optional filtering.
        
        Args:
            filters: Optional filters as dict
            
        Returns:
            int: Number of records
        """
        from sqlalchemy import func
        
        query = select(func.count()).select_from(self.model)
        
        # Apply filters if provided
        if filters:
            for attr_name, attr_value in filters.items():
                if hasattr(self.model, attr_name) and attr_value is not None:
                    query = query.where(getattr(self.model, attr_name) == attr_value)
        
        result = await self.session.execute(query)
        return result.scalar_one()
    

    async def execute_in_transaction(self, func, *args, **kwargs):
        """
        Execute a function within a transaction.
        
        Args:
            func: Async function to execute
            args: Function positional arguments
            kwargs: Function keyword arguments
            
        Returns:
            The result of the function
        """
        async with self.session.begin():
            return await func(*args, **kwargs)
</file>

<file path="app/db/repositories/webhooks.py">
"""
Webhook repository for database operations related to webhooks.
"""
from datetime import datetime, timedelta, timezone
from typing import List, Optional, Dict, Any, Tuple
from uuid import uuid4

from sqlalchemy import select, update, delete, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.repositories.base import BaseRepository
from app.models.webhook import Webhook, WebhookDelivery, WebhookEvent
from app.core.security import generate_webhook_signing_key


class WebhookRepository(BaseRepository[Webhook, Dict[str, Any], Dict[str, Any]]):
    """Webhook repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and Webhook model."""
        super().__init__(session=session, model=Webhook)
    
    async def create_webhook(
        self,
        *,
        name: str,
        url: str,
        event_types: List[str],
        user_id: str,
        secret_key: Optional[str] = None,
        gateway_webhook_id: Optional[str] = None
    ) -> Webhook:
        """
        Create a new webhook.
        
        Args:
            name: Webhook name
            url: Webhook URL
            event_types: List of event types to receive
            user_id: User ID
            secret_key: Optional secret key for signature validation
            gateway_webhook_id: Optional gateway webhook ID
            
        Returns:
            Webhook: Created webhook
        """
        # Generate secret key if not provided
        if not secret_key:
            secret_key = generate_webhook_signing_key()
        
        webhook = Webhook(
            id=str(uuid4()),
            name=name,
            url=url,
            event_types=event_types,
            user_id=user_id,
            secret_key=secret_key,
            gateway_webhook_id=gateway_webhook_id,
            is_active=True
        )
        
        self.session.add(webhook)
        
        return webhook
    
    async def get_webhooks_for_event(
        self,
        *,
        event_type: str,
        user_id: Optional[str] = None
    ) -> List[Webhook]:
        """
        Get webhooks for a specific event type.
        
        Args:
            event_type: Event type
            user_id: Optional user ID to filter webhooks
            
        Returns:
            List[Webhook]: List of matching webhooks
        """
        query = select(Webhook).where(
            and_(
                Webhook.is_active == True,
                Webhook.event_types.contains([event_type])
            )
        )
        
        if user_id:
            query = query.where(Webhook.user_id == user_id)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def create_webhook_delivery(
        self,
        *,
        webhook_id: str,
        event_type: str,
        message_id: Optional[str],
        payload: Dict[str, Any],
        status_code: Optional[int] = None,
        is_success: bool = False,
        error_message: Optional[str] = None,
        retry_count: int = 0
    ) -> WebhookDelivery:
        """
        Record a webhook delivery attempt.
        
        Args:
            webhook_id: Webhook ID
            event_type: Event type
            message_id: Message ID (if applicable)
            payload: Webhook payload
            status_code: HTTP status code
            is_success: Whether delivery was successful
            error_message: Error message if failed
            retry_count: Number of retry attempts
            
        Returns:
            WebhookDelivery: Created webhook delivery record
        """
        delivery = WebhookDelivery(
            id=str(uuid4()),
            webhook_id=webhook_id,
            event_type=event_type,
            message_id=message_id,
            payload=payload,
            status_code=status_code,
            is_success=is_success,
            error_message=error_message,
            retry_count=retry_count
        )
        
        if not is_success and retry_count < 3:  # Configure max retries
            # Schedule next retry with exponential backoff
            backoff = 5 * (2 ** retry_count)  # 5, 10, 20 minutes
            delivery.next_retry_at = datetime.now(timezone.utc) + timedelta(minutes=backoff)
        
        self.session.add(delivery)
        
        # Update webhook stats
        await self._update_webhook_stats(
            webhook_id=webhook_id,
            is_success=is_success,
            last_triggered=datetime.now(timezone.utc)
        )
        
        return delivery
    
    async def _update_webhook_stats(
        self,
        *,
        webhook_id: str,
        is_success: bool,
        last_triggered: datetime
    ) -> None:
        """
        Update webhook statistics.
        
        Args:
            webhook_id: Webhook ID
            is_success: Whether delivery was successful
            last_triggered: Timestamp of delivery attempt
        """
        webhook = await self.get_by_id(webhook_id)
        if not webhook:
            return
        
        webhook.last_triggered_at = last_triggered
        
        if is_success:
            webhook.success_count += 1
        else:
            webhook.failure_count += 1
        
        self.session.add(webhook)
    
    async def get_pending_retries(
        self,
        *,
        limit: int = 10
    ) -> List[WebhookDelivery]:
        """
        Get webhook deliveries pending retry.
        
        Args:
            limit: Maximum number of deliveries to return
            
        Returns:
            List[WebhookDelivery]: List of deliveries pending retry
        """
        now = datetime.now(timezone.utc)
        
        query = select(WebhookDelivery).where(
            and_(
                WebhookDelivery.is_success == False,
                WebhookDelivery.next_retry_at <= now,
                WebhookDelivery.next_retry_at.is_not(None),
                WebhookDelivery.retry_count < 3  # Configure max retries
            )
        ).order_by(WebhookDelivery.next_retry_at).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update_delivery_status(
        self,
        *,
        delivery_id: str,
        status_code: int,
        is_success: bool,
        error_message: Optional[str] = None,
        increment_retry: bool = False
    ) -> Optional[WebhookDelivery]:
        """
        Update webhook delivery status.
        
        Args:
            delivery_id: Delivery ID
            status_code: HTTP status code
            is_success: Whether delivery was successful
            error_message: Error message if failed
            increment_retry: Whether to increment retry count
            
        Returns:
            WebhookDelivery: Updated delivery record or None
        """
        delivery = await self.session.get(WebhookDelivery, delivery_id)
        if not delivery:
            return None
        
        delivery.status_code = status_code
        delivery.is_success = is_success
        delivery.error_message = error_message
        
        if increment_retry:
            delivery.retry_count += 1
        
        if not is_success and delivery.retry_count < 3:  # Configure max retries
            # Schedule next retry with exponential backoff
            backoff = 5 * (2 ** delivery.retry_count)  # 5, 10, 20 minutes
            delivery.next_retry_at = datetime.now(timezone.utc) + timedelta(minutes=backoff)
        else:
            delivery.next_retry_at = None
        
        self.session.add(delivery)
        
        # Update webhook stats
        await self._update_webhook_stats(
            webhook_id=delivery.webhook_id,
            is_success=is_success,
            last_triggered=datetime.now(timezone.utc)
        )
        
        return delivery
    
    async def create_webhook_event(
        self,
        *,
        event_type: str,
        payload: Dict[str, Any],
        phone_number: Optional[str] = None,
        message_id: Optional[str] = None,
        gateway_message_id: Optional[str] = None
    ) -> WebhookEvent:
        """
        Record a webhook event received from SMS gateway.
        
        Args:
            event_type: Event type
            payload: Event payload
            phone_number: Phone number
            message_id: Message ID
            gateway_message_id: Gateway message ID
            
        Returns:
            WebhookEvent: Created webhook event
        """
        event = WebhookEvent(
            id=str(uuid4()),
            event_type=event_type,
            phone_number=phone_number,
            message_id=message_id,
            gateway_message_id=gateway_message_id,
            payload=payload,
            processed=False
        )
        
        self.session.add(event)
        
        return event
    
    async def mark_event_processed(
        self,
        *,
        event_id: str,
        error_message: Optional[str] = None
    ) -> Optional[WebhookEvent]:
        """
        Mark a webhook event as processed.
        
        Args:
            event_id: Event ID
            error_message: Optional error message
            
        Returns:
            WebhookEvent: Updated event or None
        """
        event = await self.session.get(WebhookEvent, event_id)
        if not event:
            return None
        
        event.processed = True
        event.error_message = error_message
        
        self.session.add(event)
        
        return event
    
    async def get_unprocessed_events(
        self,
        *,
        limit: int = 10
    ) -> List[WebhookEvent]:
        """
        Get unprocessed webhook events.
        
        Args:
            limit: Maximum number of events to return
            
        Returns:
            List[WebhookEvent]: List of unprocessed events
        """
        query = select(WebhookEvent).where(
            WebhookEvent.processed == False
        ).order_by(WebhookEvent.created_at).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
</file>

<file path="app/models/metrics.py">
"""
Database models for metrics.
"""
from datetime import datetime, timezone, date
from typing import Optional, Dict, Any

from sqlalchemy import Column, String, Integer, Float, Date, DateTime, JSON, ForeignKey, UniqueConstraint
from sqlalchemy.orm import relationship

from app.models.base import Base


class UserMetrics(Base):
    """Model for storing user-level metrics."""
    
    # Identification and relationships
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    date = Column(Date, nullable=False, index=True)
    
    # Message metrics
    messages_sent = Column(Integer, default=0, nullable=False)
    messages_delivered = Column(Integer, default=0, nullable=False)
    messages_failed = Column(Integer, default=0, nullable=False)
    messages_scheduled = Column(Integer, default=0, nullable=False)
    
    # Campaign metrics
    campaigns_created = Column(Integer, default=0, nullable=False)
    campaigns_completed = Column(Integer, default=0, nullable=False)
    campaigns_active = Column(Integer, default=0, nullable=False)
    
    # Template metrics
    templates_created = Column(Integer, default=0, nullable=False)
    templates_used = Column(Integer, default=0, nullable=False)
    
    # Usage metrics
    quota_total = Column(Integer, default=1000, nullable=False)
    quota_used = Column(Integer, default=0, nullable=False)
    
    # Additional stats
    meta_data = Column(JSON, nullable=True)
    
    # Relationships
    user = relationship("User", back_populates="metrics")
    
    # Constraints
    __table_args__ = (
        UniqueConstraint('user_id', 'date', name='uix_user_date'),
    )
</file>

<file path="app/schemas/metrics.py">
"""
Pydantic schemas for metrics-related API operations.
"""
from typing import List, Dict, Any
from datetime import date
from pydantic import BaseModel, Field


class PeriodInfo(BaseModel):
    """Schema for period information."""
    start_date: str = Field(..., description="Start date in ISO format")
    end_date: str = Field(..., description="End date in ISO format")


class MessageMetrics(BaseModel):
    """Schema for message metrics."""
    sent: int = Field(..., description="Number of sent messages")
    delivered: int = Field(..., description="Number of delivered messages")
    failed: int = Field(..., description="Number of failed messages")
    delivery_rate: float = Field(..., description="Delivery rate percentage")


class CampaignMetrics(BaseModel):
    """Schema for campaign metrics."""
    created: int = Field(..., description="Number of created campaigns")
    completed: int = Field(..., description="Number of completed campaigns")
    active: int = Field(..., description="Number of active campaigns")


class TemplateMetrics(BaseModel):
    """Schema for template metrics."""
    created: int = Field(..., description="Number of created templates")
    used: int = Field(..., description="Number of times templates were used")


class QuotaMetrics(BaseModel):
    """Schema for quota metrics."""
    used: int = Field(..., description="Number of messages used from quota")
    total: int = Field(..., description="Total quota limit")
    percent: float = Field(..., description="Percentage of quota used")


class DailyData(BaseModel):
    """Schema for daily metrics data."""
    date: str = Field(..., description="Date in ISO format")
    sent: int = Field(..., description="Messages sent on this date")
    delivered: int = Field(..., description="Messages delivered on this date")
    failed: int = Field(..., description="Messages failed on this date")


class MetricsSummary(BaseModel):
    """Schema for metrics summary."""
    period: PeriodInfo = Field(..., description="Period information")
    messages: MessageMetrics = Field(..., description="Message metrics")
    campaigns: CampaignMetrics = Field(..., description="Campaign metrics")
    templates: TemplateMetrics = Field(..., description="Template metrics")
    quota: QuotaMetrics = Field(..., description="Quota metrics")


class DashboardMetricsResponse(BaseModel):
    """Schema for dashboard metrics response."""
    summary: MetricsSummary = Field(..., description="Summary metrics")
    daily_data: List[DailyData] = Field(..., description="Daily metrics data for charts")
    period: str = Field(..., description="Requested period")

    class Config:
        """Pydantic config."""
        from_attributes = True


class UsageMetricsResponse(BaseModel):
    """Schema for usage metrics response."""
    message_count: int = Field(..., description="Total message count")
    delivery_rate: float = Field(..., description="Overall delivery rate")
    quota: QuotaMetrics = Field(..., description="Quota information")

    class Config:
        """Pydantic config."""
        from_attributes = True


class SystemMessageMetrics(BaseModel):
    """Schema for system-wide message metrics."""
    total: int = Field(..., description="Total messages in system")
    sent: int = Field(..., description="Total sent messages")
    delivered: int = Field(..., description="Total delivered messages")
    failed: int = Field(..., description="Total failed messages")
    last_24h: int = Field(..., description="Messages in last 24 hours")


class SystemUserMetrics(BaseModel):
    """Schema for system-wide user metrics."""
    total: int = Field(..., description="Total users in system")
    active: int = Field(..., description="Active users")
    new_today: int = Field(..., description="New users today")


class SystemCampaignMetrics(BaseModel):
    """Schema for system-wide campaign metrics."""
    total: int = Field(..., description="Total campaigns in system")
    active: int = Field(..., description="Active campaigns")
    completed_today: int = Field(..., description="Campaigns completed today")


class SystemMetricsResponse(BaseModel):
    """Schema for system metrics response (admin only)."""
    messages: SystemMessageMetrics = Field(..., description="System message metrics")
    users: SystemUserMetrics = Field(..., description="System user metrics")
    campaigns: SystemCampaignMetrics = Field(..., description="System campaign metrics")

    class Config:
        """Pydantic config."""
        from_attributes = True
</file>

<file path="app/services/event_bus/bus.py">
"""
Enhanced event bus implementation for asynchronous messaging between components.
Improvements:
- Better lock handling
- Enhanced error handling and reporting
- Subscriber management
- Event batching support
- Proper subscriber cleanup
"""
import asyncio
import logging
import time
import uuid
from typing import Dict, List, Callable, Any, Set, Optional, Tuple
from datetime import datetime, timezone
from contextlib import asynccontextmanager

from app.services.event_bus.events import EventType, Event

logger = logging.getLogger("inboxerr.eventbus")


class EventBus:
    """
    Enhanced event bus for asynchronous messaging between components.
    
    Supports subscription to events, publishing events, and now includes:
    - Better lock handling for thread safety
    - Error propagation for subscribers
    - Event batching
    - Subscriber cleanup
    """
    
    def __init__(self):
        """Initialize the event bus."""
        self._subscribers: Dict[str, List[Tuple[str, Callable]]] = {}
        self._subscriber_ids: Dict[str, Set[str]] = {}
        self._lock = asyncio.Lock()
        self._initialized = False
        self._event_history: List[Dict[str, Any]] = []  # For debugging
        self._max_history = 100  # Maximum events to keep in history
        self._failed_deliveries: Dict[str, List[Dict[str, Any]]] = {}  # Failed event deliveries
    
    async def initialize(self) -> None:
        """Initialize the event bus."""
        if self._initialized:
            return
        
        logger.info("Initializing event bus")
        self._initialized = True
    
    async def shutdown(self) -> None:
        """Shutdown the event bus and clean up resources."""
        logger.info("Shutting down event bus")
        self._initialized = False
        
        # Clear subscribers
        async with self._lock:
            self._subscribers.clear()
            self._subscriber_ids.clear()
    
    @asynccontextmanager
    async def batch(self):
        """
        Context manager for batching multiple events.
        
        This allows multiple events to be published atomically.
        """
        # Create a batch container
        batch = []
        
        # Define the add_event function that will be used within the context
        async def add_event(event_type: str, data: Dict[str, Any]) -> None:
            batch.append((event_type, data))
        
        try:
            # Yield the add_event function for use within the context
            yield add_event
            
            # Process the batch after the context exits
            for event_type, data in batch:
                await self.publish(event_type, data)
                
        except Exception as e:
            logger.error(f"Error in event batch: {e}", exc_info=True)
            # Re-raise the exception after logging
            raise
    
    async def publish(self, event_type: str, data: Dict[str, Any]) -> bool:
        """
        Publish an event to subscribers.
        
        Args:
            event_type: Type of event
            data: Event data
            
        Returns:
            bool: True if event was successfully published
        """
        if not self._initialized:
            await self.initialize()
        
        subscribers = []
        subscriber_ids = []
        
        # Get subscribers with lock
        async with self._lock:
            if event_type in self._subscribers:
                subscribers = self._subscribers[event_type].copy()
                subscriber_ids = list(self._subscriber_ids[event_type])
        
        if not subscribers:
            logger.debug(f"No subscribers for event: {event_type}")
            return True
        
        # Add timestamp if not present
        if "timestamp" not in data:
            data["timestamp"] = datetime.now(timezone.utc).isoformat()
        
        # Add event type for reference
        data["event_type"] = event_type
        # Add unique event ID
        data["event_id"] = str(uuid.uuid4())
        
        # Keep history for debugging
        if len(self._event_history) >= self._max_history:
            self._event_history.pop(0)
        self._event_history.append({
            "event_type": event_type,
            "data": data,
            "subscribers": subscriber_ids,
            "timestamp": data["timestamp"]
        })
        
        # Execute callbacks outside of the lock
        logger.debug(f"Publishing event {event_type} to {len(subscribers)} subscribers")
        
        all_successful = True
        
        for subscriber_id, callback in subscribers:
            try:
                await callback(data)
            except asyncio.CancelledError:
                # Re-raise cancellation to allow proper task cleanup
                logger.warning(f"Subscriber {subscriber_id} was cancelled during event {event_type}")
                raise
            except Exception as e:
                logger.error(f"Error in subscriber {subscriber_id} for {event_type}: {e}", exc_info=True)
                
                # Record failed delivery
                if subscriber_id not in self._failed_deliveries:
                    self._failed_deliveries[subscriber_id] = []
                    
                self._failed_deliveries[subscriber_id].append({
                    "event_type": event_type,
                    "event_id": data["event_id"],
                    "error": str(e),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                })
                
                # Limit failed deliveries history
                if len(self._failed_deliveries[subscriber_id]) > self._max_history:
                    self._failed_deliveries[subscriber_id].pop(0)
                
                all_successful = False
        
        return all_successful
    
    async def subscribe(
        self,
        event_type: str,
        callback: Callable,
        subscriber_id: Optional[str] = None
    ) -> str:
        """
        Subscribe to an event type.
        
        Args:
            event_type: Event type to subscribe to
            callback: Function to call when event occurs
            subscriber_id: Optional subscriber ID
            
        Returns:
            str: Subscriber ID
        """
        if not self._initialized:
            await self.initialize()
        
        # Generate subscriber ID if not provided
        if subscriber_id is None:
            subscriber_id = f"{callback.__module__}.{callback.__name__}_{str(uuid.uuid4())[:8]}"
        
        async with self._lock:
            # Initialize event type if not exists
            if event_type not in self._subscribers:
                self._subscribers[event_type] = []
                self._subscriber_ids[event_type] = set()
            
            # Add subscriber if not already subscribed
            if subscriber_id not in self._subscriber_ids[event_type]:
                self._subscribers[event_type].append((subscriber_id, callback))
                self._subscriber_ids[event_type].add(subscriber_id)
                logger.info(f"Subscribed to {event_type}: {subscriber_id}")
            else:
                # Update callback for existing subscriber ID
                for i, (sid, _) in enumerate(self._subscribers[event_type]):
                    if sid == subscriber_id:
                        self._subscribers[event_type][i] = (subscriber_id, callback)
                        logger.debug(f"Updated subscriber callback for {event_type}: {subscriber_id}")
                        break
        
        return subscriber_id
    
    async def unsubscribe(self, event_type: str, subscriber_id: str) -> bool:
        """
        Unsubscribe from an event type.
        
        Args:
            event_type: Event type to unsubscribe from
            subscriber_id: Subscriber ID
            
        Returns:
            bool: True if unsubscribed, False if not found
        """
        if not self._initialized:
            await self.initialize()
        
        async with self._lock:
            if event_type not in self._subscribers:
                return False
            
            if subscriber_id not in self._subscriber_ids[event_type]:
                return False
            
            # Find and remove the subscriber
            self._subscribers[event_type] = [
                (sid, callback) for sid, callback in self._subscribers[event_type]
                if sid != subscriber_id
            ]
            self._subscriber_ids[event_type].remove(subscriber_id)
            
            logger.info(f"Unsubscribed from {event_type}: {subscriber_id}")
            
            # Clean up failed deliveries for this subscriber
            if subscriber_id in self._failed_deliveries:
                del self._failed_deliveries[subscriber_id]
            
            return True
    
    async def unsubscribe_all(self, subscriber_id: str) -> int:
        """
        Unsubscribe from all event types.
        
        Args:
            subscriber_id: Subscriber ID
            
        Returns:
            int: Number of subscriptions removed
        """
        if not self._initialized:
            await self.initialize()
        
        count = 0
        
        async with self._lock:
            for event_type in list(self._subscribers.keys()):
                # Check if subscriber exists for this event type
                if subscriber_id in self._subscriber_ids[event_type]:
                    # Remove from subscribers list
                    self._subscribers[event_type] = [
                        (sid, callback) for sid, callback in self._subscribers[event_type]
                        if sid != subscriber_id
                    ]
                    # Remove from subscriber IDs set
                    self._subscriber_ids[event_type].remove(subscriber_id)
                    count += 1
            
            # Clean up failed deliveries for this subscriber
            if subscriber_id in self._failed_deliveries:
                del self._failed_deliveries[subscriber_id]
        
        if count > 0:
            logger.info(f"Unsubscribed {subscriber_id} from {count} event types")
        
        return count
    
    def get_subscriber_count(self, event_type: Optional[str] = None) -> int:
        """
        Get the number of subscribers.
        
        Args:
            event_type: Optional event type to count subscribers for
            
        Returns:
            int: Number of subscribers
        """
        if event_type:
            return len(self._subscribers.get(event_type, []))
        else:
            return sum(len(subscribers) for subscribers in self._subscribers.values())
    
    def get_event_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent event history for debugging.
        
        Args:
            limit: Maximum number of events to return
            
        Returns:
            List[Dict]: Recent events
        """
        return self._event_history[-limit:] if self._event_history else []
    
    def get_failed_deliveries(self, subscriber_id: Optional[str] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        Get failed event deliveries.
        
        Args:
            subscriber_id: Optional subscriber ID to filter by
            
        Returns:
            Dict: Failed deliveries by subscriber ID
        """
        if subscriber_id:
            return {subscriber_id: self._failed_deliveries.get(subscriber_id, [])}
        else:
            return self._failed_deliveries


# Singleton instance
_event_bus = EventBus()

def get_event_bus() -> EventBus:
    """Get the singleton event bus instance."""
    return _event_bus
</file>

<file path="app/services/rate_limiter.py">
"""
Rate limiting service for API request throttling.
"""
import asyncio
import time
from typing import Dict, Any, Optional, Tuple
import logging
from datetime import datetime, timezone

from app.core.config import settings

logger = logging.getLogger("inboxerr.rate_limiter")

class RateLimiter:
    """
    Service for enforcing rate limits on API requests.
    
    Uses a simple in-memory storage for tracking request counts.
    For production, consider using Redis or another distributed storage.
    """
    
    def __init__(self):
        """Initialize the rate limiter with default limits."""
        self._requests = {}
        self._lock = asyncio.Lock()
        
        # Default rate limits by operation type
        self._rate_limits = {
            "send_message": {"requests": 60, "period": 60},  # 60 requests per minute
            "send_batch": {"requests": 10, "period": 60},    # 10 batch requests per minute
            "import_messages": {"requests": 5, "period": 300},  # 5 imports per 5 minutes
            "default": {"requests": 100, "period": 60},      # Default: 100 requests per minute
        }
    
    async def check_rate_limit(
        self, 
        user_id: str, 
        operation: str = "default"
    ) -> bool:
        """
        Check if a request is within rate limits.
        
        Args:
            user_id: ID of the user making the request
            operation: Type of operation being performed
            
        Returns:
            bool: True if request is allowed, raises exception otherwise
            
        Raises:
            HTTPException: If rate limit is exceeded
        """
        from fastapi import HTTPException, status
        
        # Get rate limit for operation
        limit = self._rate_limits.get(operation, self._rate_limits["default"])
        
        # Create key for this user and operation
        key = f"{user_id}:{operation}"
        
        current_time = time.time()
        
        async with self._lock:
            # Initialize if not exists
            if key not in self._requests:
                self._requests[key] = {"count": 0, "reset_at": current_time + limit["period"]}
            
            # Check if we need to reset the counter
            if current_time > self._requests[key]["reset_at"]:
                self._requests[key] = {"count": 0, "reset_at": current_time + limit["period"]}
            
            # Check if we're over the limit
            if self._requests[key]["count"] >= limit["requests"]:
                reset_in = int(self._requests[key]["reset_at"] - current_time)
                logger.warning(f"Rate limit exceeded for {key}. Reset in {reset_in} seconds.")
                
                # Calculate when the rate limit will reset
                reset_at = datetime.fromtimestamp(self._requests[key]["reset_at"])
                
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=f"Rate limit exceeded. Try again in {reset_in} seconds.",
                    headers={"Retry-After": str(reset_in)}
                )
            
            # Increment counter
            self._requests[key]["count"] += 1
            
            logger.debug(f"Rate limit for {key}: {self._requests[key]['count']}/{limit['requests']}")
            
            return True
    
    def set_limit(self, operation: str, requests: int, period: int) -> None:
        """
        Set a custom rate limit for an operation.
        
        Args:
            operation: Operation type to set limit for
            requests: Maximum number of requests allowed
            period: Time period in seconds
        """
        self._rate_limits[operation] = {"requests": requests, "period": period}
    
    async def get_limit_status(self, user_id: str, operation: str = "default") -> Dict[str, Any]:
        """
        Get current rate limit status for a user and operation.
        
        Args:
            user_id: User ID
            operation: Operation type
            
        Returns:
            Dict: Rate limit status information
        """
        # Get rate limit for operation
        limit = self._rate_limits.get(operation, self._rate_limits["default"])
        
        # Create key for this user and operation
        key = f"{user_id}:{operation}"
        
        current_time = time.time()
        
        async with self._lock:
            # Handle case where user hasn't made any requests yet
            if key not in self._requests:
                return {
                    "limit": limit["requests"],
                    "remaining": limit["requests"],
                    "reset": int(current_time + limit["period"]),
                    "used": 0
                }
            
            # Reset counter if needed
            if current_time > self._requests[key]["reset_at"]:
                self._requests[key] = {"count": 0, "reset_at": current_time + limit["period"]}
            
            # Return current status
            return {
                "limit": limit["requests"],
                "remaining": max(0, limit["requests"] - self._requests[key]["count"]),
                "reset": int(self._requests[key]["reset_at"]),
                "used": self._requests[key]["count"]
            }


# Singleton instance for dependency injection
_rate_limiter = RateLimiter()

def get_rate_limiter() -> RateLimiter:
    """Get the singleton rate limiter instance."""
    return _rate_limiter
</file>

<file path="app/services/webhooks/models.py">
# app/services/webhooks/models.py
"""
Pydantic models for webhook payloads from SMS Gateway.
"""
from datetime import datetime, timezone
from typing import Dict, Any, Optional, Literal
from pydantic import BaseModel, Field

class SmsReceivedPayload(BaseModel):
    """Payload for sms:received event."""
    message_id: str = Field(..., alias="messageId")
    message: str
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    received_at: datetime = Field(..., alias="receivedAt")
    
class SmsSentPayload(BaseModel):
    """Payload for sms:sent event."""
    message_id: str = Field(..., alias="messageId")
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    sent_at: datetime = Field(..., alias="sentAt")
    
class SmsDeliveredPayload(BaseModel):
    """Payload for sms:delivered event."""
    message_id: str = Field(..., alias="messageId")
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    delivered_at: datetime = Field(..., alias="deliveredAt")
    
class SmsFailedPayload(BaseModel):
    """Payload for sms:failed event."""
    message_id: str = Field(..., alias="messageId")
    phone_number: str = Field(..., alias="phoneNumber")
    sim_number: Optional[int] = Field(None, alias="simNumber")
    failed_at: datetime = Field(..., alias="failedAt")
    reason: str
    
class SystemPingPayload(BaseModel):
    """Payload for system:ping event."""
    health: Dict[str, Any]
    
EventType = Literal["sms:received", "sms:sent", "sms:delivered", "sms:failed", "system:ping"]

class WebhookPayload(BaseModel):
    """Base webhook payload from SMS Gateway."""
    device_id: str = Field(..., alias="deviceId")
    event: EventType
    id: str
    webhook_id: str = Field(..., alias="webhookId")
    payload: Dict[str, Any]  # Will be converted to specific payload types based on event
</file>

<file path="app/utils/ids.py">
# app/utils/ids.py

from enum import Enum
from uuid import uuid4

class IDPrefix(str, Enum):
    MESSAGE = "msg"
    EVENT = "event"
    BATCH = "batch"
    USER = "user"
    CAMPAIGN = "campaign"
    TEMPLATE = "template"
    WEBHOOK = "webhook"
    IMPORT = "import"
    TASK = "task"
    CONTACT = "contact"

def generate_prefixed_id(prefix: IDPrefix) -> str:
    """
    Generate a UUID string with a prefix.
    
    Args:
        prefix (IDPrefix): The entity prefix (e.g., MESSAGE, EVENT).
        
    Returns:
        str: A prefixed UUID string like 'msg-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'
    """
    return f"{prefix.value}-{uuid4()}"
</file>

<file path="app/utils/phone.py">
"""
Phone number validation and formatting utilities.
"""
import re
from typing import Any, Tuple, Dict, Optional, List
import logging

logger = logging.getLogger("inboxerr.phone")

try:
    import phonenumbers
    from phonenumbers import NumberParseException, PhoneNumberFormat
    PHONENUMBERS_AVAILABLE = True
except ImportError:
    PHONENUMBERS_AVAILABLE = False
    logger.warning("phonenumbers library not available, using basic validation")


class PhoneValidationError(Exception):
    """Exception raised for phone validation errors."""
    
    def __init__(self, message: str, details: Optional[Dict] = None):
        self.message = message
        self.details = details or {}
        super().__init__(message)


def cleanup_phone_number(raw: str) -> str:
    """
    Clean up a raw phone number string before validation/parsing.

    - Removes common formatting characters: spaces, dashes, dots, parentheses.
    - Removes extension info (e.g., "x123", "ext 456", "#789").
    - Strips out any characters except digits and leading '+'.
    - Converts Unicode digits to ASCII.
    - Returns a cleaned number string, ready for phonenumbers.parse().

    Args:
        raw (str): The raw phone number input.

    Returns:
        str: The cleaned phone number.
    """
    if not isinstance(raw, str):
        return ""

    # Convert full-width/Unicode digits to ASCII
    raw = raw.translate(str.maketrans('０１２３４５６７８９', '0123456789'))

    # Normalize leading 001/00/etc to + preserve following digits
    raw = re.sub(r'^\s*(?:00|001)[\s\-\.]*', '+', raw)


    # Remove common extension patterns (x123, ext. 456, #789, extension 101)
    raw = re.sub(r'(ext\.?|x|extension|#)\s*\d+', '', raw, flags=re.IGNORECASE)

    # Remove spaces, dashes, dots, and parentheses
    raw = re.sub(r'[\s\-\.\(\)]', '', raw)

    # Keep only digits and leading +
    if raw.startswith('+'):
        cleaned = '+' + re.sub(r'[^\d]', '', raw[1:])
    else:
        cleaned = re.sub(r'[^\d]', '', raw)

    return cleaned


def validate_phone_basic(number: str) -> Tuple[bool, str, Optional[str]]:
    """
    Basic phone number validation without external libraries.
    
    Args:
        number: Phone number to validate
        
    Returns:
        Tuple[bool, str, str]: (is_valid, formatted_number, error_message)
    """
    # Use the unified cleanup logic
    cleaned = cleanup_phone_number(number)

    # Remove common formatting characters
    cleaned = re.sub(r'[\s\-\(\)\.]+', '', number)
    
    # Check if it's just digits and maybe a leading +
    if not re.match(r'^\+?\d+$', cleaned):
        return False, number, "Phone number contains invalid characters"
    
    # Ensure it starts with + for E.164 format
    if not cleaned.startswith('+'):
        cleaned = '+' + cleaned
    
    # Basic length check
    if len(cleaned) < 8:
        return False, cleaned, "Phone number too short"
    if len(cleaned) > 16:
        return False, cleaned, "Phone number too long"
    
    return True, cleaned, None

def validate_phone_advanced(number: str, default_country: Optional[str] = "US") -> Tuple[bool, str, Optional[str], Optional[Dict[str, Any]]]:
    """
    Advanced phone number validation using the phonenumbers library.
    
    Args:
        number: Phone number to validate
        default_country: The default region for non-E.164 numbers (e.g., "US", "CA")
        
    Returns:
        Tuple[bool, str, str, dict]: (is_valid, formatted_number, error_message, metadata)
    """
    metadata = {}
    
    try:
        # Parse the phone number
        try:
            cleaned_number = cleanup_phone_number(number)
            # If already in E.164 (+...), parse as is; otherwise, parse as US (covers both US and Canada)
            if cleaned_number.startswith('+1'):
                parsed = phonenumbers.parse(cleaned_number, None)
            else:
                parsed = phonenumbers.parse(cleaned_number, default_country or "US")

        except NumberParseException as e:
            return False, number, f"Parse error: {str(e)}", None
        
        # Check if it's a valid number
        if not phonenumbers.is_valid_number(parsed):
            return False, number, "Invalid phone number", None
        
        # Format in E.164 format
        formatted = phonenumbers.format_number(
            parsed, PhoneNumberFormat.E164
        )
        
        # Get the country and carrier
        country = phonenumbers.region_code_for_number(parsed)
        metadata["country"] = country
        
        # Check if it's a mobile number
        number_type = phonenumbers.number_type(parsed)
        is_mobile = (number_type == phonenumbers.PhoneNumberType.MOBILE)
        metadata["is_mobile"] = is_mobile
        
        # Check for other properties
        metadata["number_type"] = str(number_type)
        metadata["country_code"] = parsed.country_code
        metadata["national_number"] = parsed.national_number
        
        # Additional validations
        is_possible = phonenumbers.is_possible_number(parsed)
        if not is_possible:
            return False, formatted, "Number is not possible", metadata
        
        return True, formatted, None, metadata
    except Exception as e:
        return False, number, f"Validation error: {str(e)}", None


def validate_phone(number: str, strict: bool = False, default_country: str = "US") -> Tuple[bool, str, Optional[str], Optional[Dict]]:
    """
    Validate and format a phone number.
    
    Uses the phonenumbers library if available, otherwise falls back to basic validation.
    
    Args:
        number: Phone number to validate
        strict: Whether to apply strict validation (country code check, etc.)
        
    Returns:
        Tuple[bool, str, str, dict]: (is_valid, formatted_number, error_message, metadata)
    """
    if PHONENUMBERS_AVAILABLE:
        return validate_phone_advanced(number, default_country)
    else:
        is_valid, formatted, error = validate_phone_basic(number)
        return is_valid, formatted, error, None


def is_valid_phone(number: str, strict: bool = False) -> bool:
    """
    Check if a phone number is valid.
    
    Args:
        number: Phone number to validate
        strict: Whether to apply strict validation
        
    Returns:
        bool: True if valid, False otherwise
    """
    is_valid, _, _, _ = validate_phone(number, strict)
    return is_valid


def format_phone(number: str) -> str:
    """
    Format a phone number in E.164 format.
    
    Args:
        number: Phone number to format
        
    Returns:
        str: Formatted phone number or original if invalid
        
    Raises:
        PhoneValidationError: If the phone number is invalid
    """
    is_valid, formatted, error, _ = validate_phone(number)
    if not is_valid:
        raise PhoneValidationError(error or "Invalid phone number", {"number": number})
    return formatted


def validate_batch_phone_numbers(phone_numbers: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Validate a batch of phone numbers.
    
    Args:
        phone_numbers: List of phone numbers to validate
        
    Returns:
        Dict: Dictionary with 'valid' and 'invalid' lists
    """
    valid = []
    invalid = []
    
    for number in phone_numbers:
        is_valid, formatted, error, metadata = validate_phone(number)
        if is_valid:
            valid.append({
                "original": number,
                "formatted": formatted,
                "metadata": metadata or {}
            })
        else:
            invalid.append({
                "original": number,
                "error": error,
                "metadata": metadata or {}
            })
    
    return {
        "valid": valid,
        "invalid": invalid,
        "summary": {
            "total": len(phone_numbers),
            "valid_count": len(valid),
            "invalid_count": len(invalid)
        }
    }


def extract_phone_numbers(text: str) -> List[str]:
    """
    Extract potential phone numbers from text.
    
    Args:
        text: Text to extract phone numbers from
        
    Returns:
        List[str]: List of potential phone numbers
    """
    # Define regex patterns for phone number detection
    patterns = [
        r'\+\d{1,3}\s?\d{1,14}',  # +1 123456789
        r'\(\d{1,4}\)\s?\d{1,14}', # (123) 456789
        r'\d{1,4}[- .]\d{1,4}[- .]\d{1,10}'  # 123-456-7890
    ]
    
    results = []
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        results.extend(matches)
    
    # Deduplicate and return
    return list(set(results))
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  # API service
  api:
    build: .
    container_name: inboxerr-api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/inboxerr
      - SMS_GATEWAY_URL=${SMS_GATEWAY_URL:-https://endpointnumber1.work.gd/api/3rdparty/v1}
      - SMS_GATEWAY_LOGIN=${SMS_GATEWAY_LOGIN:-}
      - SMS_GATEWAY_PASSWORD=${SMS_GATEWAY_PASSWORD:-}
      - SECRET_KEY=${SECRET_KEY:-CHANGEME_IN_PRODUCTION}
      - WEBHOOK_HOST=0.0.0.0
      - WEBHOOK_PORT=5000
      - LOG_LEVEL=DEBUG
    depends_on:
      db:
        condition: service_healthy
      db-init:
        condition: service_completed_successfully
    networks:
      - inboxerr-network
    restart: unless-stopped

  # Database service
  db:
    image: postgres:14-alpine
    container_name: inboxerr-db
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=inboxerr
    ports:
      - "5432:5432"
    networks:
      - inboxerr-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Database initialization service
  db-init:
    build: .
    container_name: inboxerr-db-init
    command: >
      sh -c "
        echo 'Waiting for database to be ready...' &&
        sleep 5 &&
        echo 'Running database migrations...' &&
        alembic upgrade head &&
        echo 'Creating admin user...' &&
        python app/scripts/create_admin.py &&
        echo 'Database initialization completed.'
      "
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/inboxerr
      - SECRET_KEY=${SECRET_KEY:-CHANGEME_IN_PRODUCTION}
    volumes:
      - .:/app
    depends_on:
      db:
        condition: service_healthy
    networks:
      - inboxerr-network
    restart: "no"  # Run once and exit

  # Optional: Redis for caching and task queue
  redis:
    image: redis:alpine
    container_name: inboxerr-redis
    ports:
      - "6379:6379"
    networks:
      - inboxerr-network
    restart: unless-stopped

  # Optional: PgAdmin for database management
  pgadmin:
    image: dpage/pgadmin4
    container_name: inboxerr-pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@inboxerr.com
      - PGADMIN_DEFAULT_PASSWORD=admin
    ports:
      - "5050:80"
    depends_on:
      - db
    networks:
      - inboxerr-network
    restart: unless-stopped

networks:
  inboxerr-network:
    driver: bridge

volumes:
  postgres_data:
</file>

<file path="project_structure.md">
# Inboxerr Backend Project Structure

```
/inboxerr-backend/
│
├── /app/                      # Application package
│   ├── __init__.py            # Package initializer
│   ├── main.py                # FastAPI application entry point
│   │
│   ├── /api/                  # API endpoints and routing
│   │   ├── __init__.py
│   │   ├── router.py          # Main API router
│   │   ├── /v1/               # API version 1
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/     # API endpoints by resource
│   │   │   │   ├── __init__.py
│   │   │   │   ├── auth.py    # Authentication endpoints
│   │   │   │   ├── messages.py # SMS message endpoints
│   │   │   │   ├── webhooks.py # Webhook endpoints
│   │   │   │   └── metrics.py  # Metrics and reporting endpoints
│   │   │   └── dependencies.py # API-specific dependencies
│   │
│   ├── /core/                 # Core application components
│   │   ├── __init__.py
│   │   ├── config.py          # Application configuration
│   │   ├── security.py        # Security utilities (auth, encryption)
│   │   ├── events.py          # Event handlers for application lifecycle
│   │   └── exceptions.py      # Custom exception classes
│   │
│   ├── /db/                   # Database related code
│   │   ├── __init__.py
│   │   ├── base.py            # Base DB session setup
│   │   ├── session.py         # DB session management
│   │   └── repositories/      # Repository pattern implementations
│   │       ├── __init__.py
│   │       ├── base.py        # Base repository class
│   │       ├── messages.py    # Message repository
│   │       └── users.py       # User repository
│   │
│   ├── /models/               # Database models
│   │   ├── __init__.py
│   │   ├── base.py            # Base model class
│   │   ├── message.py         # SMS message model
│   │   ├── user.py            # User model
│   │   └── webhook.py         # Webhook model
│   │
│   ├── /schemas/              # Pydantic schemas for API
│   │   ├── __init__.py
│   │   ├── base.py            # Base schema
│   │   ├── message.py         # Message schemas
│   │   ├── user.py            # User schemas
│   │   ├── webhook.py         # Webhook schemas
│   │   └── metrics.py         # Metrics schemas
│   │
│   ├── /services/             # Business logic services
│   │   ├── __init__.py
│   │   ├── sms/               # SMS related services
│   │   │   ├── __init__.py
│   │   │   ├── sender.py      # SMS sender implementation
│   │   │   ├── validator.py   # Phone/message validation
│   │   │   └── gateway.py     # SMS gateway client
│   │   │
│   │   ├── event_bus/         # Event management
│   │   │   ├── __init__.py
│   │   │   ├── bus.py         # Event bus implementation
│   │   │   ├── events.py      # Event definitions
│   │   │   └── handlers/      # Event handlers
│   │   │       ├── __init__.py
│   │   │       ├── message_handlers.py
│   │   │       └── system_handlers.py
│   │   │
│   │   ├── webhooks/          # Webhook handling
│   │   │   ├── __init__.py
│   │   │   ├── handler.py     # Webhook processor
│   │   │   └── manager.py     # Webhook registration/management
│   │   │
│   │   └── metrics/           # Metrics collection
│   │       ├── __init__.py
│   │       └── collector.py   # Metrics collector
│   │
│   └── /utils/                # Utility functions and helpers
│       ├── __init__.py
│       ├── phone.py           # Phone number utilities
│       ├── logging.py         # Logging configuration
│       └── pagination.py      # Pagination utilities
│
├── /alembic/                  # Database migrations
│   ├── env.py                 # Alembic environment
│   ├── README                 # Alembic readme
│   ├── script.py.mako         # Migration script template
│   └── /versions/             # Migration scripts
│
├── /tests/                    # Test suite
│   ├── __init__.py
│   ├── conftest.py            # Test configuration and fixtures
│   ├── /unit/                 # Unit tests
│   │   ├── __init__.py
│   │   ├── /services/         # Tests for services
│   │   └── /api/              # Tests for API endpoints
│   └── /integration/          # Integration tests
│       ├── __init__.py
│       └── /api/              # API integration tests
│
├── /scripts/                  # Utility scripts
│   ├── seed_db.py             # Database seeding script
│   └── generate_keys.py       # Generate security keys
│
├── .env.example               # Example environment variables
├── .gitignore                 # Git ignore file
├── docker-compose.yml         # Docker Compose configuration
├── Dockerfile                 # Docker build configuration
├── pyproject.toml             # Python project metadata
├── requirements.txt           # Python dependencies
├── requirements-dev.txt       # Development dependencies
└── README.md                  # Project documentation
```
</file>

<file path="scripts/seed_db.py">
"""
Placeholder script for seeding the database in development.
Currently not in use — extend as needed.
Seed the database with essential data (e.g., admin user).
Currently skips message seeding — placeholder for future use.

"""

import subprocess
import os
import sys

def run_admin_script():
    """Ensure an admin user exists by running create_admin.py."""
    try:
        subprocess.run(
            [sys.executable, "app/scripts/create_admin.py"],
            check=True,
            cwd=os.getcwd(),  # Ensures correct working directory
            env={**os.environ, "PYTHONPATH": os.getcwd()}
        )
        print("👮 Admin user created.")
    except subprocess.CalledProcessError:
        print("⚠️ Failed to create admin user. Check create_admin.py.")


def seed():
    print("🌱 [SKIPPED] No seed data logic implemented yet.")

if __name__ == "__main__":
    seed()
    run_admin_script()
</file>

<file path="alembic/env.py">
from logging.config import fileConfig
import os
import sys
from pathlib import Path

# Add the parent directory to sys.path
sys.path.append(str(Path(__file__).parent.parent))

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context
from app.core.config import settings
from app.db.base import Base


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# Get database URL from environment or settings
db_url = os.environ.get("ALEMBIC_DB_URL", settings.DATABASE_URL)

# Convert asyncpg URL to standard psycopg2 URL for Alembic
if "+asyncpg" in db_url:
    db_url = db_url.replace("+asyncpg", "")


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    # Set SQL Alchemy URL from our db_url
    config.set_main_option("sqlalchemy.url", db_url)
    
    url = config.get_main_option("sqlalchemy.url")
    
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Set SQL Alchemy URL from our db_url
    config.set_main_option("sqlalchemy.url", db_url)

    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="app/api/v1/dependencies.py">
"""
Dependencies for API endpoints.
"""
from fastapi import Depends, HTTPException, status, Header
from fastapi.security import OAuth2PasswordBearer
from typing import Optional, List
import jwt
from datetime import datetime, timedelta, timezone

from app.core.config import settings
from app.core.exceptions import AuthenticationError, AuthorizationError
from app.schemas.user import User, TokenData, UserRole
from app.db.session import get_repository_factory
from app.db.repositories.users import UserRepository
from app.services.rate_limiter import RateLimiter

# OAuth2 scheme for token authentication
oauth2_scheme = OAuth2PasswordBearer(tokenUrl=f"{settings.API_PREFIX}/auth/token")

#: Dependency injection provider for UserRepository.
#: This uses FastAPI's built-in dependency system to automatically inject
get_user_repository = get_repository_factory(UserRepository)




async def get_rate_limiter():
    """Get rate limiter service."""
    return RateLimiter()


async def get_current_user(
    token: str = Depends(oauth2_scheme),
    user_repository = Depends(get_user_repository)
) -> User:
    """
    Get the current authenticated user from the JWT token.
    
    Args:
        token: JWT token from Authorization header
        user_repository: User repository for database access
        
    Returns:
        User: The authenticated user
        
    Raises:
        AuthenticationError: If the token is invalid or expired
    """
    try:
        # Decode JWT token
        payload = jwt.decode(
            token, 
            settings.SECRET_KEY, 
            algorithms=["HS256"]
        )
        token_data = TokenData(**payload)
        
        # Check if token is expired
        if token_data.exp and datetime.now(timezone.utc) > token_data.exp:
            raise AuthenticationError("Token has expired")
        
        # Get user from database
        user = await user_repository.get_by_id(token_data.sub)
        if not user:
            raise AuthenticationError("User not found")
        
        # Check if user is active
        if not user.is_active:
            raise AuthenticationError("User is inactive")
        
        return user
        
    except jwt.PyJWTError:
        raise AuthenticationError("Invalid token")


async def verify_api_key(
    api_key: str = Header(..., alias=settings.API_KEY_HEADER),
    user_repository = Depends(get_user_repository)
) -> User:
    """
    Verify API key and return the associated user.
    
    Args:
        api_key: API key from header
        user_repository: User repository for database access
        
    Returns:
        User: The authenticated user
        
    Raises:
        AuthenticationError: If the API key is invalid
    """
    try:
        # Get API key from database
        api_key_record = await user_repository.get_api_key(api_key)
        if not api_key_record:
            raise AuthenticationError("Invalid API key")
        
        # Check if API key is active
        if not api_key_record.is_active:
            raise AuthenticationError("API key is inactive")
        
        # Check if API key is expired
        if api_key_record.expires_at and datetime.now(timezone.utc) > api_key_record.expires_at:
            raise AuthenticationError("API key has expired")
        
        # Get user associated with API key
        user = await user_repository.get_by_id(api_key_record.user_id)
        if not user:
            raise AuthenticationError("User not found")
        
        # Check if user is active
        if not user.is_active:
            raise AuthenticationError("User is inactive")
        
        # Update last used timestamp
        await user_repository.update_api_key_usage(api_key)
        
        return user
        
    except Exception as e:
        if isinstance(e, AuthenticationError):
            raise
        raise AuthenticationError("API key verification failed")


async def get_current_active_user(
    current_user: User = Depends(get_current_user)
) -> User:
    """
    Get the current active user.
    
    Args:
        current_user: Current authenticated user
        
    Returns:
        User: The authenticated active user
        
    Raises:
        AuthenticationError: If the user is inactive
    """
    if not current_user.is_active:
        raise AuthenticationError("Inactive user")
    return current_user


async def validate_permissions(
    required_permissions: List[str],
    current_user: User = Depends(get_current_user)
) -> None:
    """
    Validate that the current user has the required permissions.
    
    Args:
        required_permissions: List of required permissions
        current_user: Current authenticated user
        
    Raises:
        AuthorizationError: If the user doesn't have the required permissions
    """
    # Admin role has all permissions
    if current_user.role == UserRole.ADMIN:
        return
    
    # TODO: Implement proper permission checking
    # For now, just verify role-based access
    if current_user.role != UserRole.API and "api" in required_permissions:
        raise AuthorizationError("Insufficient permissions")
</file>

<file path="app/api/v1/endpoints/webhooks.py">
# app/api/v1/endpoints/webhooks.py
"""
API endpoints for webhook management.
"""
import logging
from typing import Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Request, Body, Header, status

from app.api.v1.dependencies import get_current_user
from app.schemas.user import User
from app.services.webhooks.manager import process_gateway_webhook
from app.services.webhooks.manager import fetch_registered_webhooks_from_gateway


router = APIRouter()
logger = logging.getLogger("inboxerr.webhooks")

@router.get("/")
async def list_webhooks(
    current_user: User = Depends(get_current_user)
):
    """
    List all webhooks for the current user.
    """
    # This is a stub - implementation will be added later
    return {"message": "Webhook listing not implemented yet"}

@router.post("/gateway", status_code=status.HTTP_200_OK)
async def webhook_receiver(
    request: Request,
    x_signature: str = Header(None),
    x_timestamp: str = Header(None)
):
    """
    Receive webhooks from the SMS Gateway.
    
    This endpoint is called by the SMS Gateway when events occur.
    No authentication is required as we validate using signatures.
    """
    # Get raw body for signature validation
    body = await request.body()
    
    # Prepare headers for signature verification
    headers = {
        "X-Signature": x_signature,
        "X-Timestamp": x_timestamp
    }
    
    # Process the webhook
    success, result = await process_gateway_webhook(body, headers)
    
    if not success:
        logger.error(f"Error processing webhook: {result.get('error')}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=result.get('error', 'Error processing webhook')
        )
    
    return result

@router.get("/registered", tags=["Webhooks"])
async def get_registered_gateway_webhooks(current_user: User = Depends(get_current_user)):
    """
    Fetch registered webhooks from the external SMS Gateway.
    This helps confirm webhook registration status.
    """
    webhooks = await fetch_registered_webhooks_from_gateway()
    return {"registered_webhooks": webhooks}
</file>

<file path="app/core/events.py">
"""
Event handlers for application lifecycle events.
"""
import asyncio
import logging
from typing import Dict, List, Any

from app.core.config import settings
from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType

logger = logging.getLogger("inboxerr")

# Collection of background tasks to manage
background_tasks: List[asyncio.Task] = []


async def startup_event_handler() -> None:
    """
    Handle application startup.
    
    Initialize services, database connections, and start background processes.
    """
    logger.info("Starting Inboxerr Backend application")
    
    # Initialize database (async)
    try:
        # We'll implement this in the database module
        from app.db.session import initialize_database
        await initialize_database()
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing database: {e}")
        # Don't raise error to allow startup to continue
    
    # Initialize event bus
    try:
        event_bus = get_event_bus()
        await event_bus.initialize()
        logger.info("Event bus initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing event bus: {e}")
    
    # Start retry engine if enabled
    if settings.RETRY_ENABLED:
        try:
            from app.services.sms.retry_engine import get_retry_engine
            retry_engine = await get_retry_engine()
            retry_task = asyncio.create_task(retry_engine.start())
            background_tasks.append(retry_task)
            logger.info("Retry engine started successfully")
        except Exception as e:
            logger.error(f"Error starting retry engine: {e}")
    
    # Start webhook listener if enabled
    try:
        from app.services.webhooks.manager import initialize_webhook_manager
        await initialize_webhook_manager()
        logger.info("Webhook manager initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing webhooks: {e}")
    
    # Initialize metrics collector
    try:
        from app.services.metrics.collector import initialize_metrics
        await initialize_metrics()
        logger.info("Metrics collector initialized successfully")
        # Start metrics background task
        if settings.METRICS_ENABLED:
            try:
                from app.services.metrics.collector import schedule_metrics_update
                metrics_task = asyncio.create_task(schedule_metrics_update())
                metrics_task.set_name("metrics_updater")
                background_tasks.append(metrics_task)
                logger.info("Metrics background task started")
            except Exception as e:
                logger.error(f"Error starting metrics task: {e}")
    except Exception as e:
        logger.error(f"Error initializing metrics: {e}")
    
    # Log successful startup
    logger.info(f"✅ {settings.PROJECT_NAME} v{settings.VERSION} startup complete")


async def shutdown_event_handler() -> None:
    """
    Handle application shutdown.
    
    Clean up resources and close connections properly.
    """
    logger.info("Shutting down Inboxerr Backend application")
    
    # Publish shutdown event
    try:
        event_bus = get_event_bus()
        await event_bus.publish(EventType.SYSTEM_SHUTDOWN, {
            "reason": "Application shutdown",
            "graceful": True
        })
        logger.info("Published shutdown event")
    except Exception as e:
        logger.error(f"Error publishing shutdown event: {e}")
    
    # Cancel all background tasks
    for task in background_tasks:
        if not task.done():
            task.cancel()
            try:
                # Wait briefly for task to cancel
                await asyncio.wait_for(task, timeout=5.0)
            except (asyncio.TimeoutError, asyncio.CancelledError):
                logger.warning(f"Task {task.get_name()} was cancelled")
    
    # Close database connections
    try:
        from app.db.session import close_database_connections
        await close_database_connections()
        logger.info("Database connections closed")
    except Exception as e:
        logger.error(f"Error closing database connections: {e}")
    
    # Shutdown webhook manager
    try:
        from app.services.webhooks.manager import shutdown_webhook_manager
        await shutdown_webhook_manager()
        logger.info("Webhook manager shutdown complete")
    except Exception as e:
        logger.error(f"Error shutting down webhook manager: {e}")
    
    logger.info("✅ Application shutdown complete")
</file>

<file path="app/db/repositories/campaigns.py">
# app/db/repositories/campaigns.py
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any, Tuple
from app.utils.ids import generate_prefixed_id, IDPrefix


from sqlalchemy import select, update, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.db.repositories.base import BaseRepository
from app.models.campaign import Campaign
from app.models.message import Message


class CampaignRepository(BaseRepository[Campaign, Dict[str, Any], Dict[str, Any]]):
    """Campaign repository for campaign operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize repository with session."""
        super().__init__(session=session, model=Campaign)
    
    async def create_campaign(
        self,
        *,
        name: str,
        user_id: str,
        description: Optional[str] = None,
        scheduled_start_at: Optional[datetime] = None,
        scheduled_end_at: Optional[datetime] = None,
        settings: Optional[Dict[str, Any]] = None
    ) -> Campaign:
        """
        Create a new campaign.
        
        Args:
            name: Campaign name
            user_id: User ID
            description: Optional campaign description
            scheduled_start_at: Optional scheduled start time
            scheduled_end_at: Optional scheduled end time
            settings: Optional campaign settings
            
        Returns:
            Campaign: Created campaign
        """
        campaign_id = generate_prefixed_id(IDPrefix.CAMPAIGN)
        campaign = Campaign(
            id=campaign_id,
            name=name,
            description=description,
            status="draft",
            user_id=user_id,
            scheduled_start_at=scheduled_start_at,
            scheduled_end_at=scheduled_end_at,
            settings=settings or {}
        )
        
        self.session.add(campaign)
        
        return campaign
    
    async def update_campaign_status(
        self,
        *,
        campaign_id: str,
        status: str,
        started_at: Optional[datetime] = None,
        completed_at: Optional[datetime] = None
    ) -> Optional[Campaign]:
        """
        Update campaign status with proper transaction handling.
        
        Args:
            campaign_id: Campaign ID
            status: New status
            started_at: Optional start timestamp
            completed_at: Optional completion timestamp
            
        Returns:
            Campaign: Updated campaign or None
        """
        # Start a transaction
        async with self.session.begin():
            # Get campaign
            campaign = await self.get_by_id(campaign_id)
            if not campaign:
                return None
            
            old_status = campaign.status
            campaign.status = status
            
            if started_at:
                campaign.started_at = started_at
            
            if completed_at:
                campaign.completed_at = completed_at
            
            # If status is active and no start time, set it now
            if status == "active" and not campaign.started_at:
                campaign.started_at = datetime.now(timezone.utc)
            
            # If status is completed and no completion time, set it now
            if status in ["completed", "cancelled", "failed"] and not campaign.completed_at:
                campaign.completed_at = datetime.now(timezone.utc)
            
            # Add campaign to session
            self.session.add(campaign)
            
            # If transitioning from draft to active, also update any pending messages
            # that are associated with this campaign
            if old_status == "draft" and status == "active":
                from app.models.message import Message
                from app.schemas.message import MessageStatus
                
                # Update messages
                query = update(Message).where(
                    and_(
                        Message.campaign_id == campaign_id,
                        Message.status == MessageStatus.PENDING,
                        or_(
                            Message.scheduled_at.is_(None),
                            Message.scheduled_at <= datetime.now(timezone.utc)
                        )
                    )
                ).values(
                    status=MessageStatus.PROCESSED
                )
                
                await self.session.execute(query)
            
            # Publish event about status change
            from app.services.event_bus.bus import get_event_bus
            from app.services.event_bus.events import EventType
            
            event_bus = get_event_bus()
            event_type = None
            
            if status == "active":
                event_type = EventType.CAMPAIGN_STARTED
            elif status == "paused":
                event_type = EventType.CAMPAIGN_PAUSED
            elif status == "completed":
                event_type = EventType.CAMPAIGN_COMPLETED
            elif status == "cancelled":
                event_type = EventType.CAMPAIGN_CANCELLED
            elif status == "failed":
                event_type = EventType.CAMPAIGN_FAILED
                
            if event_type:
                await event_bus.publish(
                    event_type,
                    {
                        "campaign_id": campaign_id,
                        "previous_status": old_status,
                        "new_status": status,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                )
            
            
            
            return campaign

    async def update_campaign_stats(
        self,
        *,
        campaign_id: str,
        increment_sent: int = 0,
        increment_delivered: int = 0,
        increment_failed: int = 0
    ) -> Optional[Campaign]:
        """
        Update campaign statistics.
        
        Args:
            campaign_id: Campaign ID
            increment_sent: Increment sent count
            increment_delivered: Increment delivered count
            increment_failed: Increment failed count
            
        Returns:
            Campaign: Updated campaign or None
        """
        campaign = await self.get_by_id(campaign_id)
        if not campaign:
            return None
        
        # Update counts
        campaign.sent_count += increment_sent
        campaign.delivered_count += increment_delivered
        campaign.failed_count += increment_failed
        
        # Check if campaign is complete
        total_processed = campaign.sent_count + campaign.failed_count
        if total_processed >= campaign.total_messages and campaign.total_messages > 0:
            campaign.status = "completed"
            campaign.completed_at = datetime.now(timezone.utc)
        
        self.session.add(campaign)
        
        return campaign
    
    async def get_campaigns_for_user(
        self,
        *,
        user_id: str,
        status: Optional[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Campaign], int]:
        """
        Get campaigns for a user with optional filtering.
        
        Args:
            user_id: User ID
            status: Optional status filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Campaign], int]: List of campaigns and total count
        """
        # Base query
        query = select(Campaign).where(Campaign.user_id == user_id)
        count_query = select(func.count()).select_from(Campaign).where(Campaign.user_id == user_id)
        
        # Apply status filter
        if status:
            query = query.where(Campaign.status == status)
            count_query = count_query.where(Campaign.status == status)
        
        # Order by created_at desc
        query = query.order_by(desc(Campaign.created_at))
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        campaigns = result.scalars().all()
        total = count_result.scalar_one()
        
        return campaigns, total
    
    async def add_messages_to_campaign(
        self,
        *,
        campaign_id: str,
        phone_numbers: List[str],
        message_text: str,
        user_id: str
    ) -> int:
        """
        Add messages to a campaign.
        
        Args:
            campaign_id: Campaign ID
            phone_numbers: List of recipient phone numbers
            message_text: Message content
            user_id: User ID
            
        Returns:
            int: Number of messages added
        """
        from app.db.repositories.messages import MessageRepository
        from app.utils.phone import validate_phone
        
        # Get campaign
        campaign = await self.get_by_id(campaign_id)
        if not campaign:
            return 0
        
        # Validate campaign belongs to user
        if campaign.user_id != user_id:
            return 0
        
        # TODO: Implement bulk insertion for better performance
        message_repo = MessageRepository(self.session)
        added_count = 0
        
        for phone in phone_numbers:
            # Basic validation
            is_valid, formatted_number, error, _ = validate_phone(phone)
            if is_valid:
                # Add message to campaign
                await message_repo.create_message(
                    phone_number=formatted_number,
                    message_text=message_text,
                    user_id=user_id,
                    scheduled_at=campaign.scheduled_start_at,
                    metadata={"campaign_id": campaign_id},
                    campaign_id=campaign_id  # Direct link to campaign
                )
                added_count += 1
        
        # Update campaign message count
        if added_count > 0:
            campaign.total_messages += added_count
            self.session.add(campaign)
        
        return added_count
</file>

<file path="app/db/repositories/templates.py">
# app/db/repositories/templates.py
"""
Repository for message template operations.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any, Tuple
from app.utils.ids import generate_prefixed_id, IDPrefix
import re
import logging

from sqlalchemy import select, update, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.db.repositories.base import BaseRepository
from app.models.message import MessageTemplate
from app.schemas.template import MessageTemplateCreate, MessageTemplateUpdate

logger = logging.getLogger("inboxerr.templates")


class TemplateRepository(BaseRepository[MessageTemplate, MessageTemplateCreate, MessageTemplateUpdate]):
    """Repository for message template operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize repository with session."""
        super().__init__(session=session, model=MessageTemplate)
    
    async def create_template(
        self,
        *,
        name: str,
        content: str,
        user_id: str,
        description: Optional[str] = None,
        variables: Optional[List[str]] = None,
        is_active: bool = True
    ) -> MessageTemplate:
        """
        Create a new message template.
        
        Args:
            name: Template name
            content: Template content with placeholders
            user_id: User ID
            description: Optional template description
            variables: Optional list of variables
            is_active: Whether the template is active
            
        Returns:
            MessageTemplate: Created template
        """
        # Extract variables from content if not provided
        if variables is None:
            pattern = r"{{([a-zA-Z0-9_]+)}}"
            variables = list(set(re.findall(pattern, content)))
        
        # Create template
        template_id = generate_prefixed_id(IDPrefix.TEMPLATE)
        template = MessageTemplate(
            id=template_id,
            name=name,
            content=content,
            description=description,
            is_active=is_active,
            user_id=user_id,
            variables=variables
        )
        
        self.session.add(template)

        # Publish template created event
        try:
            event_bus = get_event_bus()
            await event_bus.publish(
                EventType.TEMPLATE_CREATED, 
                {
                    "template_id": template.id,
                    "user_id": user_id,
                    "name": name
                }
            )
        except Exception as e:
            logger.warning(f"Failed to publish template created event: {e}")
        
        return template
    
    async def get_templates_for_user(
        self,
        *,
        user_id: str,
        active_only: bool = False,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[MessageTemplate], int]:
        """
        Get message templates for a user.
        
        Args:
            user_id: User ID
            active_only: Whether to return only active templates
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[MessageTemplate], int]: List of templates and total count
        """
        # Base query
        query = select(MessageTemplate).where(MessageTemplate.user_id == user_id)
        count_query = select(func.count()).select_from(MessageTemplate).where(MessageTemplate.user_id == user_id)
        
        # Filter active templates if requested
        if active_only:
            query = query.where(MessageTemplate.is_active == True)
            count_query = count_query.where(MessageTemplate.is_active == True)
        
        # Order by name
        query = query.order_by(MessageTemplate.name)
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        templates = result.scalars().all()
        total = count_result.scalar_one()
        
        return templates, total
    
    async def apply_template(
        self,
        *,
        template_id: str,
        variables: Dict[str, str]
    ) -> Optional[str]:
        """
        Apply variables to a template.
        
        Args:
            template_id: Template ID
            variables: Dictionary of variable values
            
        Returns:
            str: Processed template content or None if template not found
        """
        # Get template
        template = await self.get_by_id(template_id)
        if not template:
            return None
        
        # Apply variables to template
        content = template.content
        
        for key, value in variables.items():
            # Replace {{key}} with value
            content = content.replace(f"{{{{{key}}}}}", value)

        # Publish template used event
        try:
            event_bus = get_event_bus()
            await event_bus.publish(
                EventType.TEMPLATE_USED, 
                {
                    "template_id": template_id,
                    "user_id": template.user_id,
                    "name": template.name
                }
            )
        except Exception as e:
            logger.warning(f"Failed to publish template used event: {e}")
        
        return content
</file>

<file path="app/db/repositories/users.py">
"""
User repository for database operations related to users.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any
from app.utils.ids import generate_prefixed_id, IDPrefix
from uuid import uuid4

from sqlalchemy import select, update
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.security import get_password_hash, generate_api_key
from app.db.repositories.base import BaseRepository
from app.models.user import User, APIKey
from app.schemas.user import UserCreate, UserUpdate


class UserRepository(BaseRepository[User, UserCreate, UserUpdate]):
    """User repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and User model."""
        super().__init__(session=session, model=User)
    
    async def get_by_email(self, email: str) -> Optional[User]:
        """
        Get a user by email.
        
        Args:
            email: User email
            
        Returns:
            User: Found user or None
        """
        return await self.get_by_attribute("email", email)
    
    async def create(
        self, 
        *,
        email: str,
        hashed_password: str,
        full_name: Optional[str] = None,
        is_active: bool = True,
        role: str = "user"
    ) -> User:
        """
        Create a new user.
        
        Args:
            email: User email
            hashed_password: Hashed password
            full_name: User's full name
            is_active: Whether the user is active
            role: User role
            
        Returns:
            User: Created user
        """
        db_obj = User(
            email=email,
            hashed_password=hashed_password,
            full_name=full_name,
            is_active=is_active,
            role=role
        )
        
        self.session.add(db_obj)

        
        return db_obj
    
    async def update_password(self, *, user_id: str, new_password: str) -> Optional[User]:
        """
        Update user password.
        
        Args:
            user_id: User ID
            new_password: New password (plain text)
            
        Returns:
            User: Updated user or None
        """
        # Hash the new password
        hashed_password = get_password_hash(new_password)
        
        # Update the user
        return await self.update(
            id=user_id,
            obj_in={"hashed_password": hashed_password}
        )
    
    async def create_api_key(
        self, 
        *,
        user_id: str,
        name: str,
        expires_at: Optional[datetime] = None,
        permissions: List[str] = None
    ) -> APIKey:
        """
        Create a new API key for a user.
        
        Args:
            user_id: User ID
            name: API key name
            expires_at: Expiration timestamp
            permissions: List of permissions
            
        Returns:
            APIKey: Created API key
        """
        # Generate API key
        key_value = generate_api_key()
        
        # Create API key
        api_key = APIKey(
            id=str(uuid4()),
            key=key_value,
            name=name,
            user_id=user_id,
            expires_at=expires_at,
            is_active=True,
            permissions=permissions or []
        )
        
        self.session.add(api_key)
        
        return api_key
    
    async def get_api_key(self, key: str) -> Optional[APIKey]:
        """
        Get an API key by its value.
        
        Args:
            key: API key value
            
        Returns:
            APIKey: Found API key or None
        """
        query = select(APIKey).where(APIKey.key == key, APIKey.is_active == True)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def get_api_key_by_id(self, key_id: str) -> Optional[APIKey]:
        """
        Get an API key by its ID.
        
        Args:
            key_id: API key ID
            
        Returns:
            APIKey: Found API key or None
        """
        query = select(APIKey).where(APIKey.id == key_id)
        result = await self.session.execute(query)
        return result.scalar_one_or_none()
    
    async def list_api_keys(self, user_id: str) -> List[APIKey]:
        """
        List all API keys for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            List[APIKey]: List of API keys
        """
        query = select(APIKey).where(APIKey.user_id == user_id)
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update_api_key_usage(self, key: str) -> bool:
        """
        Update the last_used_at timestamp for an API key.
        
        Args:
            key: API key value
            
        Returns:
            bool: True if updated, False if not found
        """
        query = update(APIKey).where(
            APIKey.key == key, 
            APIKey.is_active == True
        ).values(
            last_used_at=datetime.now(timezone.utc)
        )
        
        result = await self.session.execute(query)
        
        return result.rowcount > 0
    
    async def delete_api_key(self, key_id: str) -> bool:
        """
        Delete an API key.
        
        Args:
            key_id: API key ID
            
        Returns:
            bool: True if deleted, False if not found
        """
        api_key = await self.get_api_key_by_id(key_id)
        if not api_key:
            return False
            
        await self.session.delete(api_key)
        
        return True
    
    async def deactivate_api_key(self, key_id: str) -> bool:
        """
        Deactivate an API key without deleting it.
        
        Args:
            key_id: API key ID
            
        Returns:
            bool: True if deactivated, False if not found
        """
        api_key = await self.get_api_key_by_id(key_id)
        if not api_key:
            return False
            
        api_key.is_active = False
        self.session.add(api_key)
        
        return True
</file>

<file path="app/db/session.py">
"""
Database session management.
"""
# app/db/session.py
import logging
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Any, Type, TypeVar

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from fastapi import Depends

from app.core.config import settings
from app.db.base import Base

logger = logging.getLogger("inboxerr.db")

# Create async engine with optimized pool settings
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,
    # Enhanced pool settings for better concurrency
    pool_size=20,
    max_overflow=30,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True
)

# Create async session factory
async_session_factory = sessionmaker(
    engine, class_=AsyncSession, expire_on_commit=False, autoflush=False
)

# Context manager for database sessions
@asynccontextmanager
async def get_session() -> AsyncGenerator[AsyncSession, None]:
    """
    Context manager for database sessions.
    
    Automatically handles commit/rollback and ensures session is closed.
    
    Usage:
        async with get_session() as session:
            # Use session here
    """
    session = async_session_factory()
    try:
        yield session
        await session.commit()
        logger.debug("Database session committed")
    except Exception as e:
        await session.rollback()
        logger.error(f"Database session rolled back due to: {str(e)}")
        raise
    finally:
        await session.close()
        logger.debug("Database session closed")

# Dependency function for FastAPI endpoints
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Get a database session for FastAPI endpoints via dependency injection.
    """
    async with get_session() as session:
        yield session

# Create a type variable for repository types
T = TypeVar('T')

# Factory function for repositories
def get_repository_factory(repo_type: Type[T]):
    """
    Create a repository factory for use with FastAPI dependency injection.
    
    Usage:
        @router.get("/")
        async def endpoint(repo = Depends(get_repository_factory(UserRepository))):
            # Use repo here
    """
    async def _get_repo(session: AsyncSession = Depends(get_db)) -> T:
        return repo_type(session)
    return _get_repo

# Context manager for repositories
@asynccontextmanager
async def get_repository_context(repo_type: Type[T]) -> AsyncGenerator[T, None]:
    """
    Get a repository with managed session lifecycle.
    
    Usage:
        async with get_repository_context(UserRepository) as repo:
            # Use repo here
    """
    async with get_session() as session:
        yield repo_type(session)

# Legacy function for backward compatibility
async def get_repository(repo_type: Type[T]) -> T:
    """
    Legacy repository factory (for backward compatibility).
    Warning: Session must be manually closed when using this function.
    
    This will be deprecated - use get_repository_context instead.
    """
    logger.warning(
        "Using deprecated get_repository function - session won't be automatically closed. "
        "Consider using get_repository_context instead."
    )
    session = async_session_factory()
    return repo_type(session)

async def initialize_database() -> None:
    """
    Initialize the database connection pool and run any startup tasks.
    
    This should be called during application startup.
    """
    logger.info("Initializing database connection pool")
    
    # Test database connection
    async with get_session() as session:
        try:
            # Use text() for raw SQL queries
            from sqlalchemy import text
            query = text("SELECT 1")
            await session.execute(query)
            logger.info("Database connection successful")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
            
    logger.info("Database initialization complete")


async def close_database_connections() -> None:
    """
    Close all database connections in the pool.
    
    This should be called during application shutdown.
    """
    logger.info("Closing database connections")
    
    # Dispose the engine to close all connections in the pool
    await engine.dispose()
    
    logger.info("Database connections closed")
</file>

<file path="app/models/base.py">
"""
Base database model with common fields and methods.
"""
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, Optional

from sqlalchemy import Column, DateTime, String
from sqlalchemy.ext.declarative import as_declarative, declared_attr


@as_declarative()
class Base:
    """Base class for all database models."""
    
    # Generate __tablename__ automatically from class name
    @declared_attr
    def __tablename__(cls) -> str:
        return cls.__name__.lower()
    
    # Common columns for all models
    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc), nullable=False)
    updated_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc), onupdate=lambda: datetime.now(timezone.utc), nullable=False)
    
    def dict(self) -> Dict[str, Any]:
        """Convert model to dictionary."""
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Base":
        """Create model instance from dictionary."""
        return cls(**{
            k: v for k, v in data.items() 
            if k in [c.name for c in cls.__table__.columns]
        })
</file>

<file path="app/models/webhook.py">
"""
Database models for webhook management.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text
from sqlalchemy.orm import relationship

from app.models.base import Base


class Webhook(Base):
    """Webhook configuration model."""
    
    # Webhook configuration
    name = Column(String, nullable=False)
    url = Column(String, nullable=False)
    event_types = Column(JSON, nullable=False)  # List of event types to send
    is_active = Column(Boolean, default=True, nullable=False)
    secret_key = Column(String, nullable=True)  # For signature validation
    
    # Ownership and association
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    gateway_webhook_id = Column(String, nullable=True)  # ID from SMS gateway
    
    # Stats
    last_triggered_at = Column(DateTime(timezone=True), nullable=True)
    success_count = Column(Integer, default=0, nullable=False)
    failure_count = Column(Integer, default=0, nullable=False)
    
    # Relationships
    user = relationship("User")
    deliveries = relationship("WebhookDelivery", back_populates="webhook", cascade="all, delete-orphan")


class WebhookDelivery(Base):
    """Model for tracking webhook delivery attempts."""
    
    webhook_id = Column(String, ForeignKey("webhook.id"), nullable=False, index=True)
    event_type = Column(String, nullable=False, index=True)
    message_id = Column(String, ForeignKey("message.id"), nullable=True, index=True)
    payload = Column(JSON, nullable=False)
    status_code = Column(Integer, nullable=True)
    is_success = Column(Boolean, nullable=False)
    error_message = Column(String, nullable=True)
    retry_count = Column(Integer, default=0, nullable=False)
    next_retry_at = Column(DateTime(timezone=True), nullable=True)
    
    # Relationships
    webhook = relationship("Webhook", back_populates="deliveries")
    message = relationship("Message")


class WebhookEvent(Base):
    """Model for tracking webhook events received from SMS gateway."""
    
    event_type = Column(String, nullable=False, index=True)
    phone_number = Column(String, nullable=True, index=True)
    message_id = Column(String, nullable=True, index=True)
    gateway_message_id = Column(String, nullable=True, index=True)
    payload = Column(JSON, nullable=False)
    processed = Column(Boolean, default=False, nullable=False)
    error_message = Column(String, nullable=True)
</file>

<file path="app/schemas/user.py">
"""
Pydantic schemas for user-related API operations.
"""
from typing import List, Optional
from datetime import datetime, timezone
from enum import Enum
from pydantic import BaseModel, Field, EmailStr, validator


class UserRole(str, Enum):
    """User role enum."""
    ADMIN = "admin"
    USER = "user"
    API = "api"


class UserBase(BaseModel):
    """Base user schema."""
    email: Optional[EmailStr] = Field(None, description="User email address")
    full_name: Optional[str] = Field(None, description="User's full name")
    is_active: Optional[bool] = Field(True, description="Whether the user is active")
    role: Optional[UserRole] = Field(UserRole.USER, description="User role")


class UserCreate(UserBase):
    """Schema for creating a new user."""
    email: EmailStr = Field(..., description="User email address")
    password: str = Field(..., description="User password")
    
    @validator("password")
    def validate_password(cls, v):
        """Validate password strength."""
        if len(v) < 8:
            raise ValueError("Password must be at least 8 characters long")
        if not any(char.isdigit() for char in v):
            raise ValueError("Password must contain at least one digit")
        if not any(char.isupper() for char in v):
            raise ValueError("Password must contain at least one uppercase letter")
        return v


class UserUpdate(UserBase):
    """Schema for updating a user."""
    password: Optional[str] = Field(None, description="User password")
    
    @validator("password")
    def validate_password(cls, v):
        """Validate password if provided."""
        if v is not None:
            if len(v) < 8:
                raise ValueError("Password must be at least 8 characters long")
            if not any(char.isdigit() for char in v):
                raise ValueError("Password must contain at least one digit")
            if not any(char.isupper() for char in v):
                raise ValueError("Password must contain at least one uppercase letter")
        return v


class User(UserBase):
    """Schema for user response."""
    id: str = Field(..., description="User ID")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class UserInDB(User):
    """Schema for user in database (with hashed password)."""
    hashed_password: str = Field(..., description="Hashed password")


class Token(BaseModel):
    """Schema for authentication token."""
    access_token: str
    token_type: str = "bearer"
    expires_at: datetime


class TokenData(BaseModel):
    """Schema for token payload."""
    sub: str  # User ID
    exp: Optional[datetime] = None
    role: Optional[str] = None


class APIKey(BaseModel):
    """Schema for API key."""
    id: str = Field(..., description="API key ID")
    key: str = Field(..., description="API key")
    name: str = Field(..., description="API key name")
    user_id: str = Field(..., description="User who owns the API key")
    created_at: datetime = Field(..., description="Creation timestamp")
    expires_at: Optional[datetime] = Field(None, description="Expiration timestamp")
    is_active: bool = Field(True, description="Whether the API key is active")
    last_used_at: Optional[datetime] = Field(None, description="Last usage timestamp")
    permissions: List[str] = Field(default=[], description="List of permissions")
    
    class Config:
        """Pydantic config."""
        from_attributes = True


class APIKeyCreate(BaseModel):
    """Schema for creating a new API key."""
    name: str = Field(..., description="API key name")
    expires_at: Optional[datetime] = Field(None, description="Expiration timestamp")
    permissions: Optional[List[str]] = Field(default=[], description="List of permissions")
</file>

<file path="app/services/campaigns/processor.py">
# app/services/campaigns/processor.py
import asyncio
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import uuid

from app.core.config import settings
from app.db.repositories.campaigns import CampaignRepository
from app.db.repositories.messages import MessageRepository
from app.schemas.message import MessageStatus
from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.services.sms.sender import SMSSender, get_sms_sender
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.campaigns")

class CampaignProcessor:
    """
    Service for processing SMS campaigns.
    
    Manages campaign execution, chunked processing, and status tracking.
    Uses context managers for database operations to prevent connection leaks.
    """
    
    def __init__(
        self,
        sms_sender: SMSSender,
        event_bus: Any
    ):
        """
        Initialize campaign processor with required dependencies.
        
        Removed repository dependencies to prevent long-lived connections.
        The repositories will be created as needed using context managers.
        
        Args:
            sms_sender: SMS sender service
            event_bus: Event bus for publishing events
        """
        self.sms_sender = sms_sender
        self.event_bus = event_bus
        self._processing_campaigns = set()
        self._chunk_size = settings.BATCH_SIZE  # Default from settings
        self._semaphore = asyncio.Semaphore(5)  # Limit concurrent campaigns
    
    async def start_campaign(self, campaign_id: str, user_id: str) -> bool:
        """
        Start a campaign.
        
        Args:
            campaign_id: Campaign ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if campaign was started, False otherwise
        """
        # Use context manager for repository access
        async with get_repository_context(CampaignRepository) as campaign_repository:
            # Get campaign
            campaign = await campaign_repository.get_by_id(campaign_id)
            if not campaign:
                return False
            
            # Validate ownership
            if campaign.user_id != user_id:
                return False
            
            # Check if campaign can be started
            if campaign.status != "draft" and campaign.status != "paused":
                return False
            
            # Update status to active
            updated = await campaign_repository.update_campaign_status(
                campaign_id=campaign_id,
                status="active",
                started_at=datetime.now(timezone.utc)
            )
            
            if not updated:
                return False
            
            # Get total_messages for the event
            total_messages = campaign.total_messages
        
        # Start processing in background
        asyncio.create_task(self._process_campaign(campaign_id))
        
        # Publish event
        await self.event_bus.publish(
            EventType.BATCH_CREATED,
            {
                "campaign_id": campaign_id,
                "user_id": user_id,
                "total_messages": total_messages
            }
        )
        
        return True
    
    async def pause_campaign(self, campaign_id: str, user_id: str) -> bool:
        """
        Pause a campaign.
        
        Args:
            campaign_id: Campaign ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if campaign was paused, False otherwise
        """
        # Use context manager for repository access
        async with get_repository_context(CampaignRepository) as campaign_repository:
            # Get campaign
            campaign = await campaign_repository.get_by_id(campaign_id)
            if not campaign:
                return False
            
            # Validate ownership
            if campaign.user_id != user_id:
                return False
            
            # Check if campaign can be paused
            if campaign.status != "active":
                return False
            
            # Update status to paused
            updated = await campaign_repository.update_campaign_status(
                campaign_id=campaign_id,
                status="paused"
            )
            
            return updated is not None
    
    async def cancel_campaign(self, campaign_id: str, user_id: str) -> bool:
        """
        Cancel a campaign.
        
        Args:
            campaign_id: Campaign ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if campaign was cancelled, False otherwise
        """
        # Use context manager for repository access
        async with get_repository_context(CampaignRepository) as campaign_repository:
            # Get campaign
            campaign = await campaign_repository.get_by_id(campaign_id)
            if not campaign:
                return False
            
            # Validate ownership
            if campaign.user_id != user_id:
                return False
            
            # Check if campaign can be cancelled
            if campaign.status in ["completed", "cancelled", "failed"]:
                return False
            
            # Update status to cancelled
            updated = await campaign_repository.update_campaign_status(
                campaign_id=campaign_id,
                status="cancelled",
                completed_at=datetime.now(timezone.utc)
            )
            
            return updated is not None
    
    async def _process_campaign(self, campaign_id: str) -> None:
        """
        Process a campaign in the background.
        
        Args:
            campaign_id: Campaign ID
        """
        if campaign_id in self._processing_campaigns:
            logger.warning(f"Campaign {campaign_id} is already being processed")
            return
        
        # Mark as processing
        self._processing_campaigns.add(campaign_id)
        
        try:
            # Check campaign status with context manager
            async with get_repository_context(CampaignRepository) as campaign_repository:
                campaign = await campaign_repository.get_by_id(campaign_id)
                if not campaign or campaign.status != "active":
                    return
            
            # Process in chunks until complete
            async with self._semaphore:
                await self._process_campaign_chunks(campaign_id)
                
        except Exception as e:
            logger.error(f"Error processing campaign {campaign_id}: {e}", exc_info=True)
            # Update campaign status to failed with a new context manager
            try:
                async with get_repository_context(CampaignRepository) as campaign_repository:
                    await campaign_repository.update_campaign_status(
                        campaign_id=campaign_id,
                        status="failed",
                        completed_at=datetime.now(timezone.utc)
                    )
            except Exception as update_error:
                logger.error(f"Failed to update campaign status: {update_error}")
        finally:
            # Remove from processing set
            self._processing_campaigns.remove(campaign_id)
    
    async def _process_campaign_chunks(self, campaign_id: str) -> None:
        """
        Process campaign messages in chunks.
        
        Args:
            campaign_id: Campaign ID
        """
        # Query pending messages in chunks
        offset = 0
        
        while True:
            # Check if campaign is still active with context manager
            campaign = None
            async with get_repository_context(CampaignRepository) as campaign_repository:
                campaign = await campaign_repository.get_by_id(campaign_id)
                if not campaign or campaign.status != "active":
                    logger.info(f"Campaign {campaign_id} is no longer active, stopping processing")
                    return
            
            # Get next chunk of messages with context manager
            messages = []
            total = 0
            async with get_repository_context(MessageRepository) as message_repository:
                messages, total = await message_repository.get_messages_for_campaign(
                    campaign_id=campaign_id,
                    status=MessageStatus.PENDING,
                    skip=offset,
                    limit=self._chunk_size
                )
            
            # If no more messages, campaign is complete
            if not messages:
                logger.info(f"No more pending messages for campaign {campaign_id}")
                async with get_repository_context(CampaignRepository) as campaign_repository:
                    await campaign_repository.update_campaign_status(
                        campaign_id=campaign_id,
                        status="completed",
                        completed_at=datetime.now(timezone.utc)
                    )
                return
            
            # Process this chunk
            await self._process_message_chunk(campaign_id, messages)
            
            # Update offset for next chunk
            offset += len(messages)
            
            # Small delay between chunks to avoid overloading
            await asyncio.sleep(0.5)
    
    async def _process_message_chunk(self, campaign_id: str, messages: List[Any]) -> None:
        """
        Process a chunk of messages with proper context management.
        
        Args:
            campaign_id: Campaign ID
            messages: List of message objects
        """
        # Process each message in the chunk
        success_count = 0
        fail_count = 0
        
        for message in messages:
            try:
                # Use SMS sender to send the message
                result = await self.sms_sender._send_to_gateway(
                    phone_number=message.phone_number,
                    message_text=message.message,
                    custom_id=message.custom_id or str(uuid.uuid4())
                )
                
                # Update message status with context manager
                async with get_repository_context(MessageRepository) as message_repository:
                    await message_repository.update_message_status(
                        message_id=message.id,
                        status=result.get("status", MessageStatus.PENDING),
                        event_type="campaign_process",
                        gateway_message_id=result.get("gateway_message_id"),
                        data=result
                    )
                
                success_count += 1
                
                # Add delay between messages to avoid overloading gateway
                await asyncio.sleep(settings.DELAY_BETWEEN_SMS)
                
            except Exception as e:
                logger.error(f"Error processing message {message.id}: {e}")
                
                # Update message status to failed with context manager
                try:
                    async with get_repository_context(MessageRepository) as message_repository:
                        await message_repository.update_message_status(
                            message_id=message.id,
                            status=MessageStatus.FAILED,
                            event_type="campaign_process_error",
                            reason=str(e),
                            data={"error": str(e)}
                        )
                except Exception as update_error:
                    logger.error(f"Failed to update message status: {update_error}")
                
                fail_count += 1
        
        # Update campaign stats with context manager
        if success_count > 0 or fail_count > 0:
            try:
                async with get_repository_context(CampaignRepository) as campaign_repository:
                    await campaign_repository.update_campaign_stats(
                        campaign_id=campaign_id,
                        increment_sent=success_count,
                        increment_failed=fail_count
                    )
            except Exception as update_error:
                logger.error(f"Failed to update campaign stats: {update_error}")
        
        logger.info(f"Processed chunk for campaign {campaign_id}: {success_count} sent, {fail_count} failed")


# Dependency injection function
async def get_campaign_processor():
    """
    Get campaign processor service instance.
    
    Uses the SMS sender with its dedicated context management but doesn't create
    long-lived repository instances. Each operation in the processor will
    create repositories within context managers as needed.
    """
    from app.services.event_bus.bus import get_event_bus
    from app.services.sms.sender import get_sms_sender
    
    sms_sender = await get_sms_sender()
    event_bus = get_event_bus()
    
    return CampaignProcessor(
        sms_sender=sms_sender,
        event_bus=event_bus
    )
</file>

<file path="docs/tofix.md">
1.Title: Clarify /tasks/{id} vs real batch tracking
Note: Current endpoint actually returns MessageBatch progress (batch-…) but route name implies background task. Rename or add real task tracker later so status polling for big sends (e.g., 50 k messages) is accurate.





2.CSV import can exhaust memory on large files
await file.read() loads the entire upload into RAM. For big CSVs (> ~50 MB) this will block the event-loop and may kill small instances.



2.


1. Problem Statement
All three “send” endpoints in app/api/v1/endpoints/messages.py accept a BackgroundTasks object but never enqueue tasks with it.

py
Copy
Edit
async def send_message(..., background_tasks: BackgroundTasks, ...)
    ...
    result = await sms_sender.send_message(...)   # ← blocks request
As a result, every request waits for the SMS gateway (or bulk CSV parsing) before returning, contradicting the 202 “accepted / processing” contract and limiting throughput.

2. Impact
Area	Consequence
Latency / UX	Client sits idle for seconds on large batches; 202 is misleading.
Throughput	Long-running awaits tie up the event loop → fewer concurrent requests.
Scalability	Memory spike when importing large CSVs (await file.read() loads whole file in RAM).
Task progress	/tasks/{id} endpoint often returns 404 because no task records are created.
...








important for mvp:


Inboxerr – Campaign Message Personalization & Integration Update
1. Objective Overview
Enforce every campaign includes message content (raw text or template) at creation.

Enable dynamic, per-recipient message personalization (“merge fields”) via CSV import (e.g., {{name}}, {{order_no}}, etc.).

Flow: Create Campaign (w/ message/template) → Attach contacts (with variables) → Start → Generate/send personalized Message records.

2. Schema Changes (Pydantic & DB)
CampaignCreate, CampaignCreateFromCSV:

Add: message_content: Optional[str]

Add: template_id: Optional[str]

Enforce at least one required (via root validator).

CSV Import: Accept extra columns as variables; for each contact, store variables JSON.

Message Model:

Add: variables: JSON column per message for per-recipient substitution.

CampaignResponse:

Return message_content, template_id (whichever used).

DB:

Migrate Campaign and Message tables as above.

3. Endpoint Behavior Updates
POST /api/v1/campaigns

Accepts message_content or template_id (reject if neither).

Stores accordingly.

POST /api/v1/campaigns/from-csv

Accept campaign data (with content or template), plus CSV with extra columns mapped as variables per row.

For each recipient: create Message with correct variables.

POST /api/v1/campaigns/{id}/start

For each Message:

If template_id: render message using stored variables JSON per recipient.

If message_content: use as-is (no variables).

4. Edge Cases & Validations
Both fields provided: template takes precedence or return error.

Only template_id: validate existence, ownership, and activeness.

Only message_content: validate length, reject if empty.

CSV upload: error if required columns (e.g., phone) missing, or variable columns don’t match template variables.

5. Security & Permissions
User can only reference their own template_id.

Only owner can create/edit campaigns/messages.

6. Testing & Docs
Cover:

Campaign creation failure (no content/template).

Success with either.

Multiple message records, correct variable mapping.

Template variable substitution logic.

Update API docs/OpenAPI for new fields and logic.

7. Sample Flow
Campaign create (template_id or content required).

CSV upload:

Phone, name, order_no columns (for example).

Store:

phone: "+15555555555"

variables: { "name": "Bob", "order_no": "12345" }

Start campaign:

For each message, render template with per-recipient variables, send.

Inboxerr – Campaign Message Personalization & Integration Update
1. Objective Overview
Enforce every campaign includes message content (raw text or template) at creation.

Enable dynamic, per-recipient message personalization (“merge fields”) via CSV import (e.g., {{name}}, {{order_no}}, etc.).

Flow: Create Campaign (w/ message/template) → Attach contacts (with variables) → Start → Generate/send personalized Message records.

2. Schema Changes (Pydantic & DB)
CampaignCreate, CampaignCreateFromCSV:

Add: message_content: Optional[str]

Add: template_id: Optional[str]

Enforce at least one required (via root validator).

CSV Import: Accept extra columns as variables; for each contact, store variables JSON.

Message Model:

Add: variables: JSON column per message for per-recipient substitution.

CampaignResponse:

Return message_content, template_id (whichever used).

DB:

Migrate Campaign and Message tables as above.

3. Endpoint Behavior Updates
POST /api/v1/campaigns

Accepts message_content or template_id (reject if neither).

Stores accordingly.

POST /api/v1/campaigns/from-csv

Accept campaign data (with content or template), plus CSV with extra columns mapped as variables per row.

For each recipient: create Message with correct variables.

POST /api/v1/campaigns/{id}/start

For each Message:

If template_id: render message using stored variables JSON per recipient.

If message_content: use as-is (no variables).

4. Edge Cases & Validations
Both fields provided: template takes precedence or return error.

Only template_id: validate existence, ownership, and activeness.

Only message_content: validate length, reject if empty.

CSV upload: error if required columns (e.g., phone) missing, or variable columns don’t match template variables.

5. Security & Permissions
User can only reference their own template_id.

Only owner can create/edit campaigns/messages.

6. Testing & Docs
Cover:

Campaign creation failure (no content/template).

Success with either.

Multiple message records, correct variable mapping.

Template variable substitution logic.

Update API docs/OpenAPI for new fields and logic.

7. Sample Flow
Campaign create (template_id or content required).

CSV upload:

Phone, name, order_no columns (for example).

Store:

phone: "+15555555555"

variables: { "name": "Bob", "order_no": "12345" }

Start campaign:

For each message, render template with per-recipient variables, send.





------------CSSVV-------


CSV → Contacts → Queued Messages — Backend Architecture Specification (FINAL)
(Share this with the backend team; it contains every decision agreed so far, including the library pick.)

1 — Objectives
Goal	Success Metric
MVP CSV upload	100 k-row file parses & queues in < 3 min on a single mid-tier VM
Zero RAM spikes	RSS remains < 300 MB during import
No raw-file retention	Temp file deleted immediately after parse; only SHA-256 stored
Future-proof queue layer	Parsing & dispatch logic unchanged when moving from in-process tasks to Celery/RQ

2 — End-to-End Flow
POST /imports/csv
Streams file to /tmp/{import_id}.csv, computes SHA-256, creates import_jobs row (status=processing), returns 202.

BackgroundTasks schedules process_csv(import_id, path) immediately.

Parser (process_csv)
Reads with csv.DictReader, validates, bulk-inserts Contacts every 1 000 rows, updates counters.

Deletes temp file, updates import_jobs → status=success | error.

Campaign creation accepts import_id; a single INSERT … SELECT pre-creates Message rows (status=queued).

Dispatcher loop (async task in app-lifespan) dequeues 50 queued messages at a time using FOR UPDATE SKIP LOCKED, calls sms_sender, updates status, repeats.

(Later Celery/RQ upgrade: only the runner that invokes process_csv and the dispatcher registration change; the inner logic stays identical.)

3 — Data Model Additions
Table	Key Columns	Notes
import_jobs	id UUID, filename, sha256, `status enum(processing	success
contacts	id UUID, import_id FK, phone, name, tags[], created_at; unique (import_id, phone)	
(optional Phase-2) campaign_batches	id UUID, campaign_id FK, range_start, range_end, status	

messages already exists; add an optional import_id FK for lineage.

4 — API Surface
Method	Path	Description
POST	/api/v1/imports/csv	Multipart CSV upload → {import_id} & 202
GET	/api/v1/imports/{id}	Progress (status, row counts, sha256)
GET	/api/v1/imports/{id}/contacts	Paginated preview (first 100 rows)
POST	/api/v1/campaigns	Existing payload + import_id (alternative to contact list)

5 — Responsibilities & File Map
Component	File (suggested)	Notes
Upload handler	app/api/v1/endpoints/imports.py	Streams, hashes, enqueues parser
Parser	app/services/imports/parser.py	Validates rows, bulk inserts, deletes temp file
ImportJob model	app/models/import_job.py	Enum ImportStatus shared in constants
Dispatcher loop	app/services/sms/dispatcher.py	Async loop, FOR UPDATE SKIP LOCKED, asyncio.Semaphore
Alembic migration	alembic/versions/<ts>_add_import_tables.py	Adds tables & FKs

6 — Library Decision (MVP)
Library	Pros	Cons	Decision
Std-lib csv.DictReader	Zero new deps, true streaming, constant memory, parses 100 k rows ≈ 2 s	Pure-Python (slower for > 1 M rows)	✅ Use now
pandas read_csv(chunksize)	Fast C engine, familiar API	Heavy wheel (~35 MB), higher RAM, unnecessary for current scale	Park for analytics phase
Polars Arrow read_csv(streaming)	Extremely fast, out-of-core	New binary dep, team unfamiliar	Evaluate in Phase-2 (millions of rows)

→ Implement parser with csv.DictReader; swapping to Pandas/Polars later is a one-liner.

7 — Scaling & Switch-over Plan
Stage	Trigger	Action
Phase 0 — BackgroundTasks	Works to ~200 k rows/day on one VM	Monitor p95 latency, CPU
Phase 1 — Celery/RQ	p95 > 5 min or CPU > 75 % sustained	Add Redis/Rabbit, 2-4 worker containers
Phase 2 — Horizontal scale	> 1 M rows/day	Partition messages, adopt campaign_batches, more workers

8 — Security & Compliance
Raw CSV deleted post-parse; only SHA-256 retained.

/tmp mounted with noexec,nosuid; daily cron cleans orphan files.

Future audit need → stream to S3 instead of /tmp (30-day lifecycle), same parser.

9 — Engineering Task List
DB migrations — import_jobs, contacts, add import_id FK to messages.

Add ImportStatus enum in shared constants.

Implement upload endpoint with streaming + SHA-256.

Build parser service with bulk inserts (1 000 rows/commit) and file deletion.

Extend campaign creation to accept import_id and pre-create queued messages.

Implement dispatcher loop with FOR UPDATE SKIP LOCKED, asyncio.Semaphore.

Unit & load tests:

100 k-row synthetic CSV completes < 3 min.

Dispatcher sends 10 k messages, no duplicates/deadlocks.

Add Prometheus counters: import_rows_processed_total, messages_sent_total, dispatcher_loop_duration_seconds.
</file>

<file path="README.md">
# Inboxerr Backend

API backend for SMS management and delivery.

## Features

- ✅ Send individual and batch SMS messages
- ✅ Track message delivery status
- ✅ Import contacts from CSV
- ✅ Scheduled message delivery
- ✅ Webhook integration for real-time updates
- ✅ User authentication and API key management
- ✅ Message templates
- ✅ Comprehensive retry handling
- ✅ Event-driven architecture

## Technology Stack

- **Framework**: FastAPI
- **Database**: PostgreSQL with SQLAlchemy (async)
- **Authentication**: JWT and API keys
- **Containerization**: Docker & Docker Compose
- **API Documentation**: OpenAPI/Swagger
- **Testing**: pytest
- **SMS Gateway Integration**: Android SMS Gateway

## Getting Started

### Prerequisites

- Docker and Docker Compose
- Python 3.10+
- Android SMS Gateway credentials

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/inboxerr-backend.git
   cd inboxerr-backend
   ```

2. Copy the example environment file:
   ```bash
   cp .env.example .env
   ```
   
3. Update the `.env` file with your configuration:
   ```
   SMS_GATEWAY_URL=https://endpointnumber1.work.gd/api/3rdparty/v1
   SMS_GATEWAY_LOGIN=your_login
   SMS_GATEWAY_PASSWORD=your_password
   SECRET_KEY=your_secret_key
   ```

4. Start the application with Docker Compose:
   ```bash
   docker-compose up -d
   ```

5. Run database migrations:
   ```bash
   docker-compose exec api alembic upgrade head
   ```

6. Access the API at `http://localhost:8000/api/docs`

### Development Setup

For local development without Docker:

1. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   pip install -r requirements-dev.txt
   ```

3. Set up environment variables:
   ```bash
   export DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/inboxerr
   export SMS_GATEWAY_URL=https://endpointnumber1.work.gd/api/3rdparty/v1
   export SMS_GATEWAY_LOGIN=your_login
   export SMS_GATEWAY_PASSWORD=your_password
   ```

4. Run the application:
   ```bash
   uvicorn app.main:app --reload
   ```

## API Endpoints

### Authentication

- `POST /api/v1/auth/token` - Get access token
- `POST /api/v1/auth/register` - Register new user
- `GET /api/v1/auth/me` - Get current user info
- `POST /api/v1/auth/keys` - Create API key
- `GET /api/v1/auth/keys` - List API keys

### Messages

- `POST /api/v1/messages/send` - Send a single message
- `POST /api/v1/messages/batch` - Send batch of messages
- `POST /api/v1/messages/import` - Import contacts and send messages
- `GET /api/v1/messages/{message_id}` - Get message details
- `GET /api/v1/messages` - List messages
- `PUT /api/v1/messages/{message_id}/status` - Update message status
- `DELETE /api/v1/messages/{message_id}` - Delete message

### Webhooks

- `GET /api/v1/webhooks` - List webhooks
- `POST /api/v1/webhooks` - Register webhook
- `DELETE /api/v1/webhooks/{webhook_id}` - Delete webhook
- `GET /api/v1/webhooks/logs` - Get webhook delivery logs

## Project Structure

```
/inboxerr-backend/
│
├── /app/                      # Application package
│   ├── __init__.py            # Package initializer
│   ├── main.py                # FastAPI application entry point
│   │
│   ├── /api/                  # API endpoints and routing
│   │   ├── __init__.py
│   │   ├── router.py          # Main API router
│   │   ├── /v1/               # API version 1
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/     # API endpoints by resource
│   │   │   │   ├── __init__.py
│   │   │   │   ├── auth.py    # Authentication endpoints
│   │   │   │   ├── messages.py # SMS message endpoints
│   │   │   │   ├── webhooks.py # Webhook endpoints
│   │   │   │   └── metrics.py  # Metrics and reporting endpoints
│   │   │   └── dependencies.py # API-specific dependencies
│   │
│   ├── /core/                 # Core application components
│   │   ├── __init__.py
│   │   ├── config.py          # Application configuration
│   │   ├── security.py        # Security utilities (auth, encryption)
│   │   ├── events.py          # Event handlers for application lifecycle
│   │   └── exceptions.py      # Custom exception classes
│   │
│   ├── /db/                   # Database related code
│   │   ├── __init__.py
│   │   ├── base.py            # Base DB session setup
│   │   ├── session.py         # DB session management
│   │   └── repositories/      # Repository pattern implementations
│   │       ├── __init__.py
│   │       ├── base.py        # Base repository class
│   │       ├── messages.py    # Message repository
│   │       └── users.py       # User repository
│   │
│   ├── /models/               # Database models
│   │   ├── __init__.py
│   │   ├── base.py            # Base model class
│   │   ├── message.py         # SMS message model
│   │   ├── user.py            # User model
│   │   └── webhook.py         # Webhook model
│   │
│   ├── /schemas/              # Pydantic schemas for API
│   │   ├── __init__.py
│   │   ├── base.py            # Base schema
│   │   ├── message.py         # Message schemas
│   │   ├── user.py            # User schemas
│   │   ├── webhook.py         # Webhook schemas
│   │   └── metrics.py         # Metrics schemas
│   │
│   ├── /services/             # Business logic services
│   │   ├── __init__.py
│   │   ├── sms/               # SMS related services
│   │   │   ├── __init__.py
│   │   │   ├── sender.py      # SMS sender implementation
│   │   │   ├── validator.py   # Phone/message validation
│   │   │   └── gateway.py     # SMS gateway client
│   │   │
│   │   ├── event_bus/         # Event management
│   │   │   ├── __init__.py
│   │   │   ├── bus.py         # Event bus implementation
│   │   │   ├── events.py      # Event definitions
│   │   │   └── handlers/      # Event handlers
│   │   │       ├── __init__.py
│   │   │       ├── message_handlers.py
│   │   │       └── system_handlers.py
│   │   │
│   │   ├── webhooks/          # Webhook handling
│   │   │   ├── __init__.py
│   │   │   ├── handler.py     # Webhook processor
│   │   │   └── manager.py     # Webhook registration/management
│   │   │
│   │   └── metrics/           # Metrics collection
│   │       ├── __init__.py
│   │       └── collector.py   # Metrics collector
│   │
│   └── /utils/                # Utility functions and helpers
│       ├── __init__.py
│       ├── phone.py           # Phone number utilities
│       ├── logging.py         # Logging configuration
│       └── pagination.py      # Pagination utilities
│
├── /alembic/                  # Database migrations
│   ├── env.py                 # Alembic environment
│   ├── README                 # Alembic readme
│   ├── script.py.mako         # Migration script template
│   └── /versions/             # Migration scripts
│
├── /tests/                    # Test suite
│   ├── __init__.py
│   ├── conftest.py            # Test configuration and fixtures
│   ├── /unit/                 # Unit tests
│   │   ├── __init__.py
│   │   ├── /services/         # Tests for services
│   │   └── /api/              # Tests for API endpoints
│   └── /integration/          # Integration tests
│       ├── __init__.py
│       └── /api/              # API integration tests
│
├── /scripts/                  # Utility scripts
│   ├── seed_db.py             # Database seeding script
│   └── generate_keys.py       # Generate security keys
│
├── .env.example               # Example environment variables
├── .gitignore                 # Git ignore file
├── docker-compose.yml         # Docker Compose configuration
├── Dockerfile                 # Docker build configuration
├── pyproject.toml             # Python project metadata
├── requirements.txt           # Python dependencies
├── requirements-dev.txt       # Development dependencies
└── README.md                  # Project documentation
```

## Usage Examples

### Sending a Single SMS

```python
import requests
import json

url = "http://localhost:8000/api/v1/messages/send"
headers = {
    "Authorization": "Bearer YOUR_ACCESS_TOKEN",
    "Content-Type": "application/json"
}
data = {
    "phone_number": "+1234567890",
    "message": "Hello from Inboxerr!"
}

response = requests.post(url, headers=headers, data=json.dumps(data))
print(response.json())
```

### Importing Contacts from CSV

```python
import requests

url = "http://localhost:8000/api/v1/messages/import"
headers = {
    "Authorization": "Bearer YOUR_ACCESS_TOKEN"
}
files = {
    "file": open("contacts.csv", "rb")
}
data = {
    "message_template": "Hello {{name}}, this is a test message!"
}

response = requests.post(url, headers=headers, files=files, data=data)
print(response.json())
```

# Inboxerr API Updates

## Message Template System

The Inboxerr API now includes a robust Message Template System, allowing you to:

- Create reusable templates with variable placeholders
- Apply variables to templates and preview the results
- Send messages using templates with personalized data
- Manage templates (create, update, delete, list)

### Getting Started with Templates

1. **Create a new template**:
   ```bash
   curl -X POST "http://localhost:8000/api/v1/templates" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "name": "Welcome Template",
       "content": "Hello {{name}}, welcome to our service!",
       "description": "Welcome message for new users"
     }'
   ```

2. **Send a message using a template**:
   ```bash
   curl -X POST "http://localhost:8000/api/v1/templates/send" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "template_id": "YOUR_TEMPLATE_ID",
       "phone_number": "+1234567890",
       "variables": {
         "name": "John"
       }
     }'
   ```

See the full [Message Template System User Guide](path/to/message-template-system-user-guide.md) for more details.

## Database Management

We've added tools to simplify database migration and setup:

### Generating Migrations

To generate a new migration after changing your models:

```bash
python scripts/generate_migration.py "Description of your changes"
```

### Setting Up a Test Database

To set up a test database with sample data:

```bash
python scripts/setup_test_db.py
```

This will:
1. Create a test database if it doesn't exist
2. Run all migrations
3. Create a test user and sample templates

Test user credentials:
- Email: test@example.com
- Password: Test1234!

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="app/api/router.py">
"""
Main API router that includes all endpoint routers.
"""
from fastapi import APIRouter

from app.api.v1.endpoints import auth, messages, webhooks, metrics, campaigns, templates, imports


# Create main API router
api_router = APIRouter()

# Include all endpoint routers with appropriate tags
api_router.include_router(
    auth.router, 
    prefix="/auth", 
    tags=["Authentication"]
)
api_router.include_router(
    messages.router, 
    prefix="/messages", 
    tags=["Messages"]
)
api_router.include_router(
    campaigns.router, 
    prefix="/campaigns", 
    tags=["Campaigns"]
)
api_router.include_router(
    templates.router, 
    prefix="/templates", 
    tags=["Templates"]
)
api_router.include_router(
    webhooks.router, 
    prefix="/webhooks", 
    tags=["Webhooks"]
)
api_router.include_router(
    metrics.router, 
    prefix="/metrics", 
    tags=["Metrics"]
)
api_router.include_router(
    imports.router, 
    prefix="/imports", 
    tags=["Imports"]
)
</file>

<file path="app/api/v1/endpoints/metrics.py">
"""
API endpoints for metrics and reporting.
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Dict, Any, Optional

from app.api.v1.dependencies import get_current_user
from app.schemas.user import User
from app.schemas.metrics import (
    DashboardMetricsResponse, 
    UsageMetricsResponse,
    SystemMetricsResponse
)
from app.services.metrics.collector import get_user_metrics, get_system_metrics

router = APIRouter()


@router.get("/", response_model=SystemMetricsResponse)
async def get_metrics(
    current_user: User = Depends(get_current_user)
):
    """
    Get system metrics and statistics.
    Admin users get system-wide metrics, regular users get their own metrics.
    """
    if current_user.role == "admin":
        # Admins get system-wide metrics
        return await get_system_metrics()
    else:
        # Regular users get their dashboard metrics instead
        user_metrics = await get_user_metrics(current_user.id)
        
        # Transform user metrics to system format for consistency
        return {
            "messages": {
                "total": user_metrics["summary"]["messages"]["sent"],
                "sent": user_metrics["summary"]["messages"]["sent"],
                "delivered": user_metrics["summary"]["messages"]["delivered"],
                "failed": user_metrics["summary"]["messages"]["failed"],
                "last_24h": 0  # User-specific doesn't have this breakdown
            },
            "users": {
                "total": 1,  # Just the current user
                "active": 1,
                "new_today": 0
            },
            "campaigns": {
                "total": user_metrics["summary"]["campaigns"]["created"],
                "active": user_metrics["summary"]["campaigns"]["active"],
                "completed_today": 0
            }
        }


@router.get("/usage", response_model=UsageMetricsResponse)
async def get_usage_metrics(
    current_user: User = Depends(get_current_user)
):
    """
    Get usage metrics for the current user.
    """
    user_metrics = await get_user_metrics(current_user.id)
    
    # Format for the usage endpoint
    return {
        "message_count": user_metrics["summary"]["messages"]["sent"],
        "delivery_rate": user_metrics["summary"]["messages"]["delivery_rate"],
        "quota": {
            "used": user_metrics["summary"]["quota"]["used"],
            "total": user_metrics["summary"]["quota"]["total"],
            "percent": user_metrics["summary"]["quota"]["percent"]
        }
    }


@router.get("/dashboard", response_model=DashboardMetricsResponse)
async def get_dashboard_metrics(
    period: str = Query("week", description="Time period: day, week, month, year"),
    current_user: User = Depends(get_current_user)
):
    """
    Get metrics formatted for dashboard display.
    """
    metrics = await get_user_metrics(current_user.id, period=period)
    
    # Return data already formatted for dashboard
    return metrics
</file>

<file path="app/db/base.py">
"""
Import all models here to ensure they are registered with SQLAlchemy.
"""
# Import Base
from app.models.base import Base

# Import all models
from app.models.user import User, APIKey
from app.models.campaign import Campaign
from app.models.message import Message, MessageEvent, MessageBatch, MessageTemplate
from app.models.webhook import Webhook, WebhookDelivery, WebhookEvent

# Metrics Models
from app.models.metrics import UserMetrics

# NEW: Import pipeline models
from app.models.import_job import ImportJob
from app.models.contact import Contact

# This allows alembic to auto-discover all models when creating migrations
</file>

<file path="app/models/user.py">
"""
Database models for user management.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Boolean, Column, String, DateTime, JSON, ForeignKey
from sqlalchemy.orm import relationship

from app.models.base import Base


class User(Base):
    """User model for authentication and authorization."""
    
    email = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    full_name = Column(String, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    role = Column(String, default="user", nullable=False)
    
    # Relationships
    api_keys = relationship("APIKey", back_populates="user", cascade="all, delete-orphan")
    messages = relationship("Message", back_populates="user", cascade="all, delete-orphan")
    metrics = relationship("UserMetrics", back_populates="user", cascade="all, delete-orphan")



class APIKey(Base):
    """API key model for API authentication."""
    
    key = Column(String, unique=True, index=True, nullable=False)
    name = Column(String, nullable=False)
    user_id = Column(String, ForeignKey("user.id"), nullable=False)
    expires_at = Column(DateTime(timezone=True), nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    last_used_at = Column(DateTime(timezone=True), nullable=True)
    permissions = Column(JSON, default=list, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="api_keys")
</file>

<file path="app/services/event_bus/events.py">
"""
Event type definitions for the event bus.
"""
from enum import Enum, auto
from typing import Dict, Any, Optional
from datetime import datetime, timezone


class EventType(str, Enum):
    """Event types for the event bus."""
    
    # System events
    SYSTEM_STARTUP = "system:startup"
    SYSTEM_SHUTDOWN = "system:shutdown"
    
    # Message events
    MESSAGE_CREATED = "message:created"
    MESSAGE_UPDATED = "message:updated"
    MESSAGE_SENT = "message:sent"
    MESSAGE_DELIVERED = "message:delivered"
    MESSAGE_FAILED = "message:failed"
    MESSAGE_SCHEDULED = "message:scheduled"
    MESSAGE_RETRIED = "message:retried"
    MESSAGE_RETRY_FAILED = "message:retry_failed"
    
    # Batch events
    BATCH_CREATED = "batch:created"
    BATCH_UPDATED = "batch:updated"
    BATCH_COMPLETED = "batch:completed"

    # Campaign events
    CAMPAIGN_CREATED = "campaign:created"
    CAMPAIGN_UPDATED = "campaign:updated"
    CAMPAIGN_STARTED = "campaign:started"
    CAMPAIGN_PAUSED = "campaign:paused"
    CAMPAIGN_COMPLETED = "campaign:completed"
    CAMPAIGN_CANCELLED = "campaign:cancelled"
    CAMPAIGN_FAILED = "campaign:failed"
    
    # SMS Gateway events
    SMS_RECEIVED = "sms:received"
    SMS_SENT = "sms:sent"
    SMS_DELIVERED = "sms:delivered"
    SMS_FAILED = "sms:failed"
    
    # Webhook events
    WEBHOOK_RECEIVED = "webhook:received"
    WEBHOOK_PROCESSED = "webhook:processed"
    
    # User events
    USER_CREATED = "user:created"
    USER_UPDATED = "user:updated"
    USER_DELETED = "user:deleted"
    
    # API events
    API_REQUEST = "api:request"
    API_RESPONSE = "api:response"
    API_ERROR = "api:error"

    # Template events
    TEMPLATE_CREATED = "template:created"
    TEMPLATE_UPDATED = "template:updated"
    TEMPLATE_USED = "template:used"


class Event:
    """
    Base event class.
    
    Contains common event data and helper methods.
    """
    
    def __init__(
        self,
        event_type: EventType,
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None
    ):
        """
        Initialize event.
        
        Args:
            event_type: Event type
            data: Event data
            timestamp: Event timestamp
        """
        self.event_type = event_type
        self.data = data
        self.timestamp = timestamp or datetime.now(timezone.utc)
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert event to dictionary.
        
        Returns:
            Dict: Event data
        """
        return {
            "event_type": self.event_type,
            "data": self.data,
            "timestamp": self.timestamp.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Event":
        """
        Create event from dictionary.
        
        Args:
            data: Event data
            
        Returns:
            Event: Event instance
        """
        event_type = data.get("event_type")
        if isinstance(event_type, str):
            event_type = EventType(event_type)
        
        timestamp = data.get("timestamp")
        if isinstance(timestamp, str):
            timestamp = datetime.fromisoformat(timestamp)
        
        return cls(
            event_type=event_type,
            data=data.get("data", {}),
            timestamp=timestamp
        )


class MessageEvent(Event):
    """
    Message event class.
    
    Contains message-specific event data.
    """
    
    def __init__(
        self,
        event_type: EventType,
        message_id: str,
        user_id: str,
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None
    ):
        """
        Initialize message event.
        
        Args:
            event_type: Event type
            message_id: Message ID
            user_id: User ID
            data: Event data
            timestamp: Event timestamp
        """
        # Add message ID and user ID to data
        data = data.copy()
        data["message_id"] = message_id
        data["user_id"] = user_id
        
        super().__init__(event_type, data, timestamp)
    
    @property
    def message_id(self) -> str:
        """Get message ID."""
        return self.data.get("message_id", "")
    
    @property
    def user_id(self) -> str:
        """Get user ID."""
        return self.data.get("user_id", "")


class WebhookEvent(Event):
    """
    Webhook event class.
    
    Contains webhook-specific event data.
    """
    
    def __init__(
        self,
        event_type: EventType,
        webhook_id: str,
        payload: Dict[str, Any],
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None
    ):
        """
        Initialize webhook event.
        
        Args:
            event_type: Event type
            webhook_id: Webhook ID
            payload: Webhook payload
            data: Event data
            timestamp: Event timestamp
        """
        # Add webhook ID and payload to data
        data = data.copy()
        data["webhook_id"] = webhook_id
        data["payload"] = payload
        
        super().__init__(event_type, data, timestamp)
    
    @property
    def webhook_id(self) -> str:
        """Get webhook ID."""
        return self.data.get("webhook_id", "")
    
    @property
    def payload(self) -> Dict[str, Any]:
        """Get webhook payload."""
        return self.data.get("payload", {})
</file>

<file path="app/services/metrics/collector.py">
# app/services/metrics/collector.py
"""
Metrics collection service.
"""
import asyncio
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime, date, timezone, timedelta

from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.metrics")

async def initialize_metrics() -> None:
    """Initialize metrics collector."""
    logger.info("Initializing metrics collector")
    
    # Subscribe to events
    event_bus = get_event_bus()
    
    # Message events
    await event_bus.subscribe(
        EventType.MESSAGE_CREATED,
        _handle_message_created,
        "metrics.message_created"
    )
    
    await event_bus.subscribe(
        EventType.MESSAGE_SENT,
        _handle_message_sent,
        "metrics.message_sent"
    )
    
    await event_bus.subscribe(
        EventType.MESSAGE_DELIVERED,
        _handle_message_delivered,
        "metrics.message_delivered"
    )
    
    await event_bus.subscribe(
        EventType.MESSAGE_FAILED,
        _handle_message_failed,
        "metrics.message_failed"
    )
    
    # Campaign events
    await event_bus.subscribe(
        EventType.CAMPAIGN_CREATED,
        _handle_campaign_created,
        "metrics.campaign_created"
    )
    
    await event_bus.subscribe(
        EventType.CAMPAIGN_COMPLETED,
        _handle_campaign_completed,
        "metrics.campaign_completed"
    )
    
    # Template events
    await event_bus.subscribe(
        EventType.TEMPLATE_CREATED,
        _handle_template_created,
        "metrics.template_created"
    )
    
    await event_bus.subscribe(
        EventType.TEMPLATE_USED,
        _handle_template_used,
        "metrics.template_used"
    )
    
    logger.info("Metrics collector initialized")

# Event handlers
async def _handle_message_created(data: Dict[str, Any]) -> None:
    """Handle message created event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_scheduled",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for message_created: {e}")

async def _handle_message_sent(data: Dict[str, Any]) -> None:
    """Handle message sent event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_sent",
                increment=1
            )
            
            # Decrement scheduled count if it was scheduled
            scheduled = data.get("scheduled", False)
            if scheduled:
                await metrics_repo.increment_metric(
                    user_id=user_id,
                    date=today,
                    metric_name="messages_scheduled",
                    increment=-1
                )
    except Exception as e:
        logger.error(f"Error updating metrics for message_sent: {e}")

async def _handle_message_delivered(data: Dict[str, Any]) -> None:
    """Handle message delivered event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_delivered",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for message_delivered: {e}")

async def _handle_message_failed(data: Dict[str, Any]) -> None:
    """Handle message failed event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="messages_failed",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for message_failed: {e}")

async def _handle_campaign_created(data: Dict[str, Any]) -> None:
    """Handle campaign created event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_created",
                increment=1
            )
            
            # Increment active campaigns too
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_active",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for campaign_created: {e}")

async def _handle_campaign_completed(data: Dict[str, Any]) -> None:
    """Handle campaign completed event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_completed",
                increment=1
            )
            
            # Decrement active campaigns
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="campaigns_active",
                increment=-1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for campaign_completed: {e}")

async def _handle_template_created(data: Dict[str, Any]) -> None:
    """Handle template created event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="templates_created",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for template_created: {e}")

async def _handle_template_used(data: Dict[str, Any]) -> None:
    """Handle template used event."""
    user_id = data.get("user_id")
    if not user_id:
        return
    
    # Get today's date
    today = datetime.now(timezone.utc).date()
    
    # Update metrics
    from app.db.repositories.metrics import MetricsRepository
    
    try:
        async with get_repository_context(MetricsRepository) as metrics_repo:
            await metrics_repo.increment_metric(
                user_id=user_id,
                date=today,
                metric_name="templates_used",
                increment=1
            )
    except Exception as e:
        logger.error(f"Error updating metrics for template_used: {e}")

async def get_user_metrics(
    user_id: str,
    period: str = "week"
) -> Dict[str, Any]:
    """
    Get metrics for a specific user.
    
    Args:
        user_id: User ID
        period: Time period ("day", "week", "month", "year")
        
    Returns:
        Dict[str, Any]: User metrics
    """
    from app.db.repositories.metrics import MetricsRepository
    
    # Calculate date range based on period
    end_date = datetime.now(timezone.utc).date()
    
    if period == "day":
        start_date = end_date
    elif period == "week":
        start_date = end_date - timedelta(days=7)
    elif period == "month":
        start_date = end_date - timedelta(days=30)
    elif period == "year":
        start_date = end_date - timedelta(days=365)
    else:
        # Default to week
        start_date = end_date - timedelta(days=7)
    
    # Get metrics
    async with get_repository_context(MetricsRepository) as metrics_repo:
        # Get summary metrics
        summary = await metrics_repo.get_summary_metrics(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
        
        # Get daily metrics for charting
        metrics_list = await metrics_repo.get_metrics_range(
            user_id=user_id,
            start_date=start_date,
            end_date=end_date
        )
        
        # Format for response
        daily_data = []
        for metric in metrics_list:
            daily_data.append({
                "date": metric.date.isoformat(),
                "sent": metric.messages_sent,
                "delivered": metric.messages_delivered,
                "failed": metric.messages_failed
            })
        
        # Combine data
        result = {
            "summary": summary,
            "daily_data": daily_data,
            "period": period
        }
        
        return result

async def get_system_metrics() -> Dict[str, Any]:
    """
    Get system-wide metrics (for admin).
    
    Returns:
        Dict[str, Any]: System metrics formatted for SystemMetricsResponse
    """
    # For system metrics, we'll query the database directly
    from app.db.repositories.messages import MessageRepository
    from app.db.repositories.users import UserRepository
    from app.db.repositories.campaigns import CampaignRepository
    
    system_metrics = {
        "messages": {},
        "users": {},
        "campaigns": {}
    }
    
    # Query message stats
    async with get_repository_context(MessageRepository) as message_repo:
        from sqlalchemy import func, select
        from app.models.message import Message
        
        # Total messages
        total_query = select(func.count(Message.id))
        result = await message_repo.session.execute(total_query)
        total_messages = result.scalar_one_or_none() or 0
        
        # Messages by status
        from app.schemas.message import MessageStatus
        status_counts = {}
        for status in [MessageStatus.SENT, MessageStatus.DELIVERED, MessageStatus.FAILED]:
            status_query = select(func.count(Message.id)).where(Message.status == status)
            result = await message_repo.session.execute(status_query)
            status_counts[status] = result.scalar_one_or_none() or 0
        
        # Last 24 hours
        yesterday = datetime.now(timezone.utc) - timedelta(days=1)
        recent_query = select(func.count(Message.id)).where(Message.created_at >= yesterday)
        result = await message_repo.session.execute(recent_query)
        recent_messages = result.scalar_one_or_none() or 0
        
        system_metrics["messages"] = {
            "total": total_messages,
            "sent": status_counts.get(MessageStatus.SENT, 0),
            "delivered": status_counts.get(MessageStatus.DELIVERED, 0),
            "failed": status_counts.get(MessageStatus.FAILED, 0),
            "last_24h": recent_messages
        }
    
    # Query user stats
    async with get_repository_context(UserRepository) as user_repo:
        from app.models.user import User
        
        # Total users
        total_query = select(func.count(User.id))
        result = await user_repo.session.execute(total_query)
        total_users = result.scalar_one_or_none() or 0
        
        # Active users
        active_query = select(func.count(User.id)).where(User.is_active == True)
        result = await user_repo.session.execute(active_query)
        active_users = result.scalar_one_or_none() or 0
        
        # New users today
        today_start = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
        new_today_query = select(func.count(User.id)).where(User.created_at >= today_start)
        result = await user_repo.session.execute(new_today_query)
        new_today = result.scalar_one_or_none() or 0
        
        system_metrics["users"] = {
            "total": total_users,
            "active": active_users,
            "new_today": new_today
        }
    
    # Query campaign stats
    async with get_repository_context(CampaignRepository) as campaign_repo:
        from app.models.campaign import Campaign
        
        # Total campaigns
        total_query = select(func.count(Campaign.id))
        result = await campaign_repo.session.execute(total_query)
        total_campaigns = result.scalar_one_or_none() or 0
        
        # Active campaigns
        active_query = select(func.count(Campaign.id)).where(Campaign.status == "active")
        result = await campaign_repo.session.execute(active_query)
        active_campaigns = result.scalar_one_or_none() or 0
        
        # Campaigns completed today
        today_start = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
        completed_today_query = select(func.count(Campaign.id)).where(
            Campaign.completed_at >= today_start
        )
        result = await campaign_repo.session.execute(completed_today_query)
        completed_today = result.scalar_one_or_none() or 0
        
        system_metrics["campaigns"] = {
            "total": total_campaigns,
            "active": active_campaigns,
            "completed_today": completed_today
        }
    
    return system_metrics

async def schedule_metrics_update() -> None:
    """
    Schedule regular metrics updates.
    This function is meant to be run as a background task.
    """
    from app.db.repositories.metrics import MetricsRepository
    
    while True:
        try:
            # Calculate time until the next run (midnight UTC)
            now = datetime.now(timezone.utc)
            tomorrow = (now + timedelta(days=1)).replace(
                hour=0, minute=0, second=0, microsecond=0
            )
            seconds_until_midnight = (tomorrow - now).total_seconds()
            
            # Sleep until midnight
            logger.info(f"Metrics update scheduled for {tomorrow.isoformat()}")
            await asyncio.sleep(seconds_until_midnight)
            
            # Run the update for yesterday
            yesterday = now.date() - timedelta(days=1)
            logger.info(f"Running scheduled metrics update for {yesterday.isoformat()}")
            
            async with get_repository_context(MetricsRepository) as metrics_repo:
                updated_count = await metrics_repo.update_daily_metrics(day=yesterday)
                logger.info(f"Updated metrics for {updated_count} users")
                
        except Exception as e:
            logger.error(f"Error in scheduled metrics update: {e}")
            # Sleep for a while before retrying
            await asyncio.sleep(3600)  # 1 hour
</file>

<file path="app/services/sms/retry_engine.py">
# app/services/sms/retry_engine.py
import asyncio
import logging
import uuid
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional

from app.core.config import settings
from app.db.repositories.messages import MessageRepository
from app.schemas.message import MessageStatus
from app.services.event_bus.events import EventType
from app.services.event_bus.bus import get_event_bus
from app.core.exceptions import SMSGatewayError, RetryableError
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.retry")

class RetryEngine:
    """
    Service for retrying failed messages.
    
    Periodically checks for failed messages and attempts to resend them.
    Uses proper context management for database connections to prevent leaks.
    """
    
    def __init__(self, event_bus: Any, sms_sender: Any):
        """
        Initialize retry engine with required dependencies.
        
        Args:
            event_bus: Event bus for publishing events
            sms_sender: SMS sender service for resending messages
        """
        self.event_bus = event_bus
        self.sms_sender = sms_sender
        self._running = False
        self._semaphore = asyncio.Semaphore(5)  # Limit concurrent retries
        
    async def start(self) -> None:
        """Start the retry engine."""
        if self._running:
            return
            
        self._running = True
        logger.info("Starting retry engine")
        
        while self._running:
            try:
                await self._process_retries()
            except Exception as e:
                logger.error(f"Error in retry engine: {e}", exc_info=True)
                
            # Wait before next cycle
            await asyncio.sleep(settings.RETRY_INTERVAL_SECONDS)
            
    async def stop(self) -> None:
        """Stop the retry engine."""
        self._running = False
        logger.info("Retry engine stopped")
    
    async def _process_retries(self) -> None:
        """Process messages pending retry using context managers for DB access."""
        # Get messages that need retry - using context manager
        retry_candidates = await self._get_retry_candidates()
        
        if not retry_candidates:
            logger.debug("No messages to retry")
            return
            
        logger.info(f"Found {len(retry_candidates)} messages to retry")
        
        # Process retries with concurrency limit
        tasks = []
        for message in retry_candidates:
            tasks.append(self._retry_message(message))
            
        if tasks:
            # Process retries concurrently but with limit
            for i in range(0, len(tasks), 5):  # Process in chunks of 5
                chunk = tasks[i:i+5]
                await asyncio.gather(*chunk)
                await asyncio.sleep(1)  # Short delay between chunks
    
    async def _get_retry_candidates(self) -> List[Dict[str, Any]]:
        """
        Get messages that are candidates for retry.
        
        Uses context manager to ensure database connections are properly closed.
        
        Returns:
            List[Dict]: List of messages that should be retried
        """
        # Parameters for retry candidate selection
        now = datetime.now(timezone.utc)
        max_retries = settings.RETRY_MAX_ATTEMPTS
        retry_candidates = []
        
        try:
            # Use context manager for database access
            async with get_repository_context(MessageRepository) as message_repository:
                # Query for messages that:
                # 1. Are in a failed state
                # 2. Have not exceeded max retry attempts
                # 3. Have a retryable error or no error specified
                # 4. Last retry attempt was long enough ago (based on exponential backoff)
                
                failed_messages = await message_repository.get_retryable_messages(
                    max_retries=max_retries,
                    limit=50  # Limit number of messages to process in one cycle
                )
                
                # Filter messages based on retry delay (exponential backoff)
                for message in failed_messages:
                    # Get retry attempt count (from message metadata or events)
                    retry_count = self._get_retry_count(message)
                    
                    # Calculate backoff delay - 30s, 2m, 8m, 30m, 2h, etc.
                    backoff_seconds = 30 * (2 ** retry_count)
                    
                    # Get timestamp of last attempt
                    last_attempt = message.failed_at or message.updated_at
                    
                    # Check if enough time has passed for retry
                    if now - last_attempt > timedelta(seconds=backoff_seconds):
                        retry_candidates.append(message)
            
            return retry_candidates
        
        except Exception as e:
            logger.error(f"Error getting retry candidates: {e}", exc_info=True)
            # Return empty list on error to prevent crashing the retry engine
            return []
    
    def _get_retry_count(self, message: Any) -> int:
        """
        Get the retry count for a message.
        
        Args:
            message: Message object
            
        Returns:
            int: Number of retry attempts
        """
        # Check if retry count is in metadata
        meta_data = getattr(message, 'meta_data', {}) or {}
        if isinstance(meta_data, dict) and 'retry_count' in meta_data:
            return meta_data.get('retry_count', 0)
            
        # Fallback - count events of type "retry"
        retry_events = [e for e in getattr(message, 'events', []) 
                       if getattr(e, 'event_type', '') == 'retry']
        return len(retry_events)
    
    async def _retry_message(self, message: Any) -> None:
        """
        Retry sending a message with proper context management.
        
        Uses context managers for all database operations to prevent connection leaks.
        
        Args:
            message: Message to retry
        """
        message_id = message.id
        phone_number = message.phone_number
        message_text = message.message
        custom_id = message.custom_id
        
        # Get current retry count
        retry_count = self._get_retry_count(message)
        
        try:
            logger.info(f"Retrying message {message_id} (attempt {retry_count + 1})")
            
            # Use semaphore to limit concurrent retries
            async with self._semaphore:
                # Reset status to pending for retry - using context manager
                async with get_repository_context(MessageRepository) as message_repository:
                    # Update message status
                    await message_repository.update_message_status(
                        message_id=message_id,
                        status=MessageStatus.PENDING,
                        event_type="retry",
                        data={
                            "retry_count": retry_count + 1,
                            "previous_error": message.reason
                        }
                    )
                    
                    # Update metadata to track retry count
                    meta_data = getattr(message, 'meta_data', {}) or {}
                    if isinstance(meta_data, dict):
                        meta_data['retry_count'] = retry_count + 1
                        await message_repository.update(
                            id=message_id,
                            obj_in={"meta_data": meta_data}
                        )
                
                # Attempt to send again
                result = await self.sms_sender._send_to_gateway(
                    phone_number=phone_number,
                    message_text=message_text,
                    custom_id=custom_id or str(uuid.uuid4())
                )
                
                # Update message status with new context manager
                async with get_repository_context(MessageRepository) as message_repository:
                    await message_repository.update_message_status(
                        message_id=message_id,
                        status=result.get("status", MessageStatus.PENDING),
                        event_type="retry_success",
                        gateway_message_id=result.get("gateway_message_id"),
                        data=result
                    )
                
                # Publish event - doesn't need database connection
                await self.event_bus.publish(
                    EventType.MESSAGE_RETRIED,
                    {
                        "message_id": message_id,
                        "phone_number": phone_number,
                        "retry_count": retry_count + 1,
                        "status": result.get("status", MessageStatus.PENDING)
                    }
                )
                
                logger.info(f"Successfully retried message {message_id}")
                
        except Exception as e:
            logger.error(f"Error retrying message {message_id}: {e}")
            
            # Update status to failed with incremented retry count - in a new context
            try:
                error_message = str(e)
                is_retryable = isinstance(e, RetryableError)
                
                async with get_repository_context(MessageRepository) as message_repository:
                    await message_repository.update_message_status(
                        message_id=message_id,
                        status=MessageStatus.FAILED,
                        event_type="retry_failed",
                        reason=error_message,
                        data={
                            "retry_count": retry_count + 1,
                            "retryable": is_retryable
                        }
                    )
                
                # Publish event
                await self.event_bus.publish(
                    EventType.MESSAGE_RETRY_FAILED,
                    {
                        "message_id": message_id,
                        "phone_number": phone_number,
                        "retry_count": retry_count + 1,
                        "error": error_message,
                        "retryable": is_retryable
                    }
                )
            except Exception as update_error:
                logger.error(f"Failed to update error status: {update_error}")


# Singleton instance
_retry_engine = None

async def get_retry_engine():
    """
    Get the singleton retry engine instance.
    
    Note: This doesn't maintain any long-lived database connections,
    as the repository is now created within context managers for each operation.
    """
    global _retry_engine
    
    if _retry_engine is None:
        from app.services.event_bus.bus import get_event_bus
        from app.services.sms.sender import get_sms_sender
        
        event_bus = get_event_bus()
        sms_sender = await get_sms_sender()
        
        _retry_engine = RetryEngine(
            event_bus=event_bus,
            sms_sender=sms_sender
        )
        
    return _retry_engine
</file>

<file path="requirements.txt">
aiosqlite==0.21.0
alembic==1.15.2
android-sms-gateway==2.0.0
annotated-types==0.7.0
anyio==4.9.0
asgi-lifespan==2.1.0
asyncpg==0.30.0
bcrypt==4.3.0
certifi==2025.1.31
cffi==1.17.1
click==8.1.8
colorama==0.4.6
coverage==7.8.0
cryptography==44.0.2
dnspython==2.7.0
ecdsa==0.19.1
email_validator==2.2.0
fastapi==0.115.12
greenlet==3.2.1
h11==0.14.0
httpcore==1.0.8
httptools==0.6.4
httpx==0.28.1
idna==3.10
iniconfig==2.1.0
Mako==1.3.10
MarkupSafe==3.0.2
packaging==25.0
passlib==1.7.4
phonenumbers==9.0.3
pluggy==1.5.0
psutil=7.0.0
psycopg2-binary==2.9.10
pyasn1==0.4.8
pycparser==2.22
pydantic==2.11.3
pydantic-settings==2.9.1
pydantic_core==2.33.1
PyJWT==2.10.1
pytest==8.3.5
pytest-asyncio==0.26.0
pytest-cov==6.1.1
pytest-mock==3.14.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-jose==3.4.0
python-multipart==0.0.20
pytz==2025.2
PyYAML==6.0.2
rsa==4.9.1
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.40
starlette==0.46.2
typing-inspection==0.4.0
typing_extensions==4.13.2
uvicorn==0.34.2
watchfiles==1.0.5
websockets==15.0.1
</file>

<file path="tests/conftest.py">
import asyncio
import os
from datetime import datetime
from collections.abc import AsyncGenerator

import pytest
import pytest_asyncio
from httpx import AsyncClient
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from app.main import app
from app.db.repositories.users import UserRepository
from app.db.repositories.templates import TemplateRepository
from app.core.security import get_password_hash
from app.models import base as models
from app.schemas.user import User
from app.api.v1 import dependencies
from app.models.user import User as UserModel


# Use test DB from env or fallback to SQLite
TEST_DATABASE_URL = os.getenv("DATABASE_URL", "sqlite+aiosqlite:///:memory:")
async_engine = create_async_engine(TEST_DATABASE_URL, echo=False, future=True)
async_session_factory = sessionmaker(async_engine, class_=AsyncSession, expire_on_commit=False)

TEST_USER = {
    "email": "test@example.com",
    "password": "Test1234!",
    "full_name": "Test User",
    "role": "user"
}

TEST_TEMPLATES = [
    {
        "name": "Welcome Message",
        "content": "Hi {{name}}, welcome to our service! We're glad you've joined us.",
        "description": "Template for welcoming new users"
    },
    {
        "name": "OTP Verification",
        "content": "Your verification code is {{code}}. It will expire in {{minutes}} minutes.",
        "description": "Template for sending OTP codes"
    },
    {
        "name": "Appointment Reminder",
        "content": "Hi {{name}}, this is a reminder for your appointment on {{date}} at {{time}}. Reply YES to confirm or call {{phone}} to reschedule.",
        "description": "Template for appointment reminders"
    }
]

@pytest_asyncio.fixture(autouse=True)
def override_auth():
    def fake_user():
        return User(
            id="test-user-id",
            email="test@example.com",
            is_active=True,
            is_superuser=False,
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
    app.dependency_overrides[dependencies.get_current_user] = lambda: fake_user()
    yield
    app.dependency_overrides.clear()

@pytest.fixture(scope="session", autouse=True)
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest_asyncio.fixture(scope="session", autouse=True)
async def initialize_test_db():
    print("⚙️  Initializing test DB and seeding data...")
    async with async_engine.begin() as conn:
        await conn.run_sync(models.Base.metadata.drop_all)
        await conn.run_sync(models.Base.metadata.create_all)

    async with async_session_factory() as session:
        user_repo = UserRepository(session)
        existing_user = await user_repo.get_by_email(TEST_USER["email"])
        if not existing_user:
            user = UserModel(
                id="test-user-id",
                email=TEST_USER["email"],
                hashed_password=get_password_hash(TEST_USER["password"]),
                full_name=TEST_USER["full_name"],
                role=TEST_USER["role"],
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )
            session.add(user)
        else:
            user = existing_user

        template_repo = TemplateRepository(session)
        for template in TEST_TEMPLATES:
            await template_repo.create_template(
                name=template["name"],
                content=template["content"],
                description=template["description"],
                user_id=user.id
            )

        await session.commit()
    
            # --- Add existing message and task for tests that depend on them ---
        from app.db.repositories.messages import MessageRepository
        from datetime import timezone

        now = datetime.now(timezone.utc)
        message_repo = MessageRepository(session)


        await message_repo.create_message(
            phone_number="+1234567890",
            message_text="Test message seeded for unit tests",
            user_id=user.id,
            custom_id="existing-msg-id",
            metadata={}
        )


        # Create a batch using repository logic
        batch = await message_repo.create_batch(
            user_id=user.id,
            name="Test Batch",
            total=1  # One message in batch
        )
        # Override ID for the test that expects it
        batch.id = "existing-task-id"

        # Add a message linked to this batch
        await message_repo.create_message(
            phone_number="+1234567890",
            message_text="Hello from seeded batch",
            user_id=user.id,
            custom_id="msg-in-batch",
            metadata={"batch_id": batch.id}
        )

        await session.commit()

    print(f"✅ Test DB seeded at {TEST_DATABASE_URL}")

@pytest_asyncio.fixture()
async def async_client() -> AsyncGenerator[AsyncClient, None]:
    from httpx import ASGITransport
    transport = ASGITransport(app=app)
    async with AsyncClient(base_url="http://testserver", transport=transport) as client:
        yield client

@pytest_asyncio.fixture()
async def db_session() -> AsyncGenerator[AsyncSession, None]:
    async with async_session_factory() as session:
        yield session
</file>

<file path="app/core/config.py">
"""
Application settings and configuration management.
"""
from typing import List, Optional, Union
from pydantic import AnyHttpUrl, validator, field_validator
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """Application settings."""
    # Base
    PROJECT_NAME: str = "Inboxerr Backend"
    PROJECT_DESCRIPTION: str = "API backend for SMS management and delivery"
    VERSION: str = "0.1.0"
    API_PREFIX: str = "/api/v1"
    DEBUG: bool = False
    
    # CORS
    BACKEND_CORS_ORIGINS: List[Union[str, AnyHttpUrl]] = []

    @field_validator("BACKEND_CORS_ORIGINS", mode='before')
    def assemble_cors_origins(cls, v: Union[str, List[str]]) -> Union[List[str], str]:
        """Parse CORS origins from string or list."""
        if isinstance(v, str) and not v.startswith("["):
            return [i.strip() for i in v.split(",")]
        elif isinstance(v, (list, str)):
            return v
        raise ValueError(v)
    
    # Authentication
    SECRET_KEY: str = "CHANGEME_IN_PRODUCTION"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24  # 1 day
    API_KEY_HEADER: str = "X-API-Key"

    # Database
    DATABASE_URL: str = "postgresql+asyncpg://postgres:admin@localhost:5432/inboxerr"
    
    # SMS Gateway
    SMS_GATEWAY_URL: str = "https://endpointnumber1.work.gd/api/3rdparty/v1"
    SMS_GATEWAY_LOGIN: str = ""
    SMS_GATEWAY_PASSWORD: str = ""
    
    # Webhook
    API_BASE_URL: str = "http://localhost:8000"  # Base URL for webhooks
    WEBHOOK_SIGNATURE_KEY: Optional[str] = None
    WEBHOOK_TIMESTAMP_TOLERANCE: int = 300  # 5 minutes
    
    # SMS Processing
    BATCH_SIZE: int = 100
    DELAY_BETWEEN_SMS: float = 0.3  # seconds
    RETRY_ENABLED: bool = False
    RETRY_MAX_ATTEMPTS: int = 3
    RETRY_INTERVAL_SECONDS: int = 60
    
    # Metrics
    METRICS_ENABLED: bool = True

    # Logging
    LOG_LEVEL: str = "INFO"
    
    class Config:
        """Pydantic config."""
        case_sensitive = True
        env_file = ".env"

    # Mock settings for testing purposes
    SMS_GATEWAY_MOCK: bool = True  # For development and testing

# Create singleton settings instance
settings = Settings()
</file>

<file path="app/models/campaign.py">
# app/models/campaign.py
from datetime import datetime, timezone
from typing import List, Optional

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text
from sqlalchemy.orm import relationship

from app.models.base import Base


class Campaign(Base):
    """Campaign model for bulk SMS messaging."""
    
    # Basic campaign information
    name = Column(String, nullable=False, index=True)
    description = Column(Text, nullable=True)
    
    # Campaign status
    status = Column(String, nullable=False, default="draft", index=True)  # draft, active, paused, completed, cancelled, failed
    
    # Campaign statistics
    total_messages = Column(Integer, default=0, nullable=False)
    sent_count = Column(Integer, default=0, nullable=False)
    delivered_count = Column(Integer, default=0, nullable=False)
    failed_count = Column(Integer, default=0, nullable=False)
    
    # Campaign configuration
    scheduled_start_at = Column(DateTime(timezone=True), nullable=True, index=True)
    scheduled_end_at = Column(DateTime(timezone=True), nullable=True)
    started_at = Column(DateTime(timezone=True), nullable=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Campaign settings
    settings = Column(JSON, nullable=True, default=dict)  # Store campaign-specific settings

    # NEW: Personalization fields
    message_content = Column(Text, nullable=True)  # Actual message content for personalization
    template_id = Column(String, ForeignKey("messagetemplate.id"), nullable=True, index=True)  # Reference to template used
    
    # Ownership
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    
    # Relationships
    user = relationship("User")
    # In Campaign model:
    messages = relationship("Message", back_populates="campaign", cascade="all, delete-orphan")
    template = relationship("MessageTemplate")  #Reference to template used

    
    # Helper properties
    @property
    def progress_percentage(self) -> float:
        """Calculate the campaign progress percentage."""
        if self.total_messages == 0:
            return 0
        return round((self.sent_count / self.total_messages) * 100, 2)
    
    @property
    def delivery_success_rate(self) -> float:
        """Calculate the delivery success rate."""
        if self.sent_count == 0:
            return 0
        return round((self.delivered_count / self.sent_count) * 100, 2)
</file>

<file path="app/models/message.py">
"""
Database models for SMS messages.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

from sqlalchemy import Column, String, DateTime, Boolean, JSON, Integer, ForeignKey, Text
from sqlalchemy.orm import relationship

from app.models.base import Base


class Message(Base):
    """SMS message model."""
    
    # Core message data
    custom_id = Column(String, unique=True, index=True, nullable=True)
    phone_number = Column(String, nullable=False, index=True)
    message = Column(Text, nullable=False)
    status = Column(String, nullable=False, default="pending", index=True)

    # Campaign relationship
    campaign_id = Column(String, ForeignKey("campaign.id"), nullable=True, index=True)
    campaign = relationship("Campaign", back_populates="messages")
    
    # Timestamps for status tracking
    scheduled_at = Column(DateTime(timezone=True), nullable=True, index=True)
    sent_at = Column(DateTime(timezone=True), nullable=True)
    delivered_at = Column(DateTime(timezone=True), nullable=True)
    failed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Additional data
    reason = Column(String, nullable=True)
    gateway_message_id = Column(String, nullable=True, index=True)
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    meta_data = Column(JSON, nullable=True)  # Changed from 'metadata' to 'meta_data' SQLAlchemy reserves metadata

    # Personalization and import tracking
    variables = Column(JSON, nullable=True)  # Store personalization variables as JSONB
    import_id = Column(String, ForeignKey("importjob.id"), nullable=True, index=True)  # Reference to import job
    
    
    # SMS parts tracking
    parts_count = Column(Integer, default=1, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="messages")
    events = relationship("MessageEvent", back_populates="message", cascade="all, delete-orphan")
    batch_id = Column(String, ForeignKey("messagebatch.id"), nullable=True, index=True)
    batch = relationship("MessageBatch", back_populates="messages")
    import_job = relationship("ImportJob")  # NEW: Reference to import job



class MessageEvent(Base):
    """Model for tracking message events and status changes."""
    
    message_id = Column(String, ForeignKey("message.id"), nullable=False, index=True)
    event_type = Column(String, nullable=False, index=True)
    status = Column(String, nullable=False)
    data = Column(JSON, nullable=True)
    
    # Relationships
    message = relationship("Message", back_populates="events")


class MessageBatch(Base):
    """Model for tracking message batches."""
    
    name = Column(String, nullable=True)
    total = Column(Integer, default=0, nullable=False)
    processed = Column(Integer, default=0, nullable=False)
    successful = Column(Integer, default=0, nullable=False)
    failed = Column(Integer, default=0, nullable=False)
    status = Column(String, nullable=False, default="pending", index=True)
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    # Relationships
    messages = relationship("Message", back_populates="batch")
    user = relationship("User")


class MessageTemplate(Base):
    """Model for storing reusable message templates."""
    
    name = Column(String, nullable=False, index=True)
    content = Column(Text, nullable=False)
    description = Column(String, nullable=True)
    is_active = Column(Boolean, default=True, nullable=False)
    user_id = Column(String, ForeignKey("user.id"), nullable=False, index=True)
    variables = Column(JSON, nullable=True)  # Define expected variables in the template
    
    # Relationships
    user = relationship("User")
</file>

<file path="app/schemas/campaign.py">
# app/schemas/campaign.py
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from uuid import UUID
from enum import Enum

from pydantic import BaseModel, Field, validator



class CampaignStatus(str, Enum):
    """
    Campaign status enum.
    
    Defines all possible states a campaign can be in during its lifecycle.
    Uses string enum to ensure JSON serialization compatibility and type safety.
    """
    DRAFT = "draft"
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"


class CampaignBase(BaseModel):
    """Base schema for campaign data."""
    name: str = Field(..., description="Campaign name")
    description: Optional[str] = Field(None, description="Campaign description")
    scheduled_start_at: Optional[datetime] = Field(None, description="Scheduled start time")
    scheduled_end_at: Optional[datetime] = Field(None, description="Scheduled end time")
    settings: Optional[Dict[str, Any]] = Field(default={}, description="Campaign settings")
    # Personalization fields
    message_content: Optional[str] = Field(None, description="Message content for personalization")
    template_id: Optional[str] = Field(None, description="Template ID if using a template")


class CampaignCreate(CampaignBase):
    """Schema for creating a new campaign."""
    pass


class CampaignCreateFromCSV(BaseModel):
    """Schema for creating a campaign from CSV file."""
    name: str = Field(..., description="Campaign name")
    description: Optional[str] = Field(None, description="Campaign description")
    message_template: str = Field(..., description="Message template to send")
    scheduled_start_at: Optional[datetime] = Field(None, description="Scheduled start time")
    scheduled_end_at: Optional[datetime] = Field(None, description="Scheduled end time")
    settings: Optional[Dict[str, Any]] = Field(default={}, description="Campaign settings")


class CampaignUpdate(BaseModel):
    """Schema for updating a campaign."""
    name: Optional[str] = Field(None, description="Campaign name")
    description: Optional[str] = Field(None, description="Campaign description")
    scheduled_start_at: Optional[datetime] = Field(None, description="Scheduled start time")
    scheduled_end_at: Optional[datetime] = Field(None, description="Scheduled end time")
    settings: Optional[Dict[str, Any]] = Field(None, description="Campaign settings")
    #Personalization fields
    message_content: Optional[str] = Field(None, description="Message content for personalization")
    template_id: Optional[str] = Field(None, description="Template ID if using a template")


class CampaignResponse(CampaignBase):
    """Schema for campaign response."""
    id: str = Field(..., description="Campaign ID")
    status: CampaignStatus = Field(..., description="Campaign status")
    total_messages: int = Field(..., description="Total number of messages")
    sent_count: int = Field(..., description="Number of sent messages")
    delivered_count: int = Field(..., description="Number of delivered messages")
    failed_count: int = Field(..., description="Number of failed messages")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    started_at: Optional[datetime] = Field(None, description="Start timestamp")
    completed_at: Optional[datetime] = Field(None, description="Completion timestamp")
    user_id: str = Field(..., description="User ID")

    # Personalization fields
    message_content: Optional[str] = Field(None, description="Message content for personalization")
    template_id: Optional[str] = Field(None, description="Template ID if using a template")
    
    
    # Add computed fields
    progress_percentage: float = Field(0, description="Progress percentage")
    delivery_success_rate: float = Field(0, description="Delivery success rate")
    
    class Config:
        """Pydantic config."""
        from_attributes = True
</file>

<file path="app/services/webhooks/manager.py">
# app/services/webhooks/manager.py
import logging
import hmac
import hashlib
import time
import json
from typing import Dict, Any, Optional, List, Tuple
import httpx
from fastapi.encoders import jsonable_encoder
from datetime import datetime, timezone 

from app.core.config import settings
from app.core.exceptions import SMSGatewayError
from app.db.repositories.messages import MessageRepository
from app.db.repositories.webhooks import WebhookRepository
from app.schemas.message import MessageStatus
from app.services.event_bus.bus import get_event_bus
from app.services.event_bus.events import EventType
from app.services.webhooks.models import (
    WebhookPayload, SmsReceivedPayload, SmsSentPayload, 
    SmsDeliveredPayload, SmsFailedPayload, SystemPingPayload
)
from app.db.session import get_repository_context

logger = logging.getLogger("inboxerr.webhooks")

# Track registered webhooks
_registered_webhooks: Dict[str, str] = {}  # event_type -> webhook_id
_initialized = False

async def initialize_webhook_manager() -> None:
    """Initialize the webhook manager and register with SMS Gateway."""
    global _initialized
    
    if _initialized:
        return
        
    logger.info("Initializing webhook manager")
    
    # Register webhooks for each event type
    events_to_register = [
        "sms:sent", 
        "sms:delivered", 
        "sms:failed"
    ]
    
    for event_type in events_to_register:
        webhook_id = await register_webhook_with_gateway(event_type)
        if webhook_id:
            _registered_webhooks[event_type] = webhook_id
    
    _initialized = True
    logger.info(f"Webhook manager initialized, registered webhooks: {len(_registered_webhooks)}")

async def shutdown_webhook_manager() -> None:
    """Clean up webhook manager resources."""
    logger.info("Shutting down webhook manager")
    
    # Unregister all webhooks
    for event_type, webhook_id in _registered_webhooks.items():
        await unregister_webhook_from_gateway(webhook_id)
    
    _registered_webhooks.clear()
    logger.info("Webhook manager shutdown complete")

async def register_webhook_with_gateway(event_type: str) -> Optional[str]:
    """
    Register a webhook for a specific event type.
    
    Args:
        event_type: Event type to register for
        
    Returns:
        str: Webhook ID if registration successful
    """
    if not settings.SMS_GATEWAY_URL or not settings.SMS_GATEWAY_LOGIN or not settings.SMS_GATEWAY_PASSWORD:
        logger.warning("SMS Gateway credentials not configured, skipping webhook registration")
        return None
    
    # Webhook URL for the Gateway to call
    webhook_url = f"{settings.API_BASE_URL}{settings.API_PREFIX}/webhooks/gateway"
    
    try:
        # Create httpx client with authentication
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{settings.SMS_GATEWAY_URL}/webhooks",
                auth=(settings.SMS_GATEWAY_LOGIN, settings.SMS_GATEWAY_PASSWORD),
                json={
                    "id": f"inboxerr-{event_type}",  # Custom ID for tracking
                    "url": webhook_url,
                    "event": event_type
                },
                timeout=10.0
            )
            
            if response.status_code in (200, 201):
                webhook_data = response.json()
                webhook_id = webhook_data.get("id")
                logger.info(f"Successfully registered webhook for {event_type}: {webhook_id}")
                return webhook_id
            else:
                logger.error(f"Failed to register webhook for {event_type}: {response.status_code} - {response.text}")
                return None
                
    except Exception as e:
        logger.error(f"Error registering webhook for {event_type}: {e}")
        return None

async def unregister_webhook_from_gateway(webhook_id: str) -> bool:
    """
    Unregister a webhook from SMS Gateway.
    
    Args:
        webhook_id: Webhook ID
        
    Returns:
        bool: True if unregistration successful
    """
    if not settings.SMS_GATEWAY_URL or not settings.SMS_GATEWAY_LOGIN or not settings.SMS_GATEWAY_PASSWORD:
        logger.warning("SMS Gateway credentials not configured, skipping webhook unregistration")
        return False
    
    try:
        # Create httpx client with authentication
        async with httpx.AsyncClient() as client:
            response = await client.delete(
                f"{settings.SMS_GATEWAY_URL}/webhooks/{webhook_id}",
                auth=(settings.SMS_GATEWAY_LOGIN, settings.SMS_GATEWAY_PASSWORD),
                timeout=10.0
            )
            
            if response.status_code in (200, 204):
                logger.info(f"Successfully unregistered webhook: {webhook_id}")
                return True
            else:
                logger.error(f"Failed to unregister webhook: {response.status_code} - {response.text}")
                return False
                
    except Exception as e:
        logger.error(f"Error unregistering webhook: {e}")
        return False

async def process_gateway_webhook(raw_body: bytes, headers: Dict[str, str]) -> Tuple[bool, Dict[str, Any]]:
    """
    Process a webhook received from the SMS Gateway with enhanced error handling.
    
    Uses context managers for all database operations to prevent connection leaks.
    
    Args:
        raw_body: Raw request body
        headers: Request headers
        
    Returns:
        Tuple[bool, Dict]: (success, processed_data)
    """
    # Decode raw body for payload processing
    try:
        payload_str = raw_body.decode('utf-8')
    except UnicodeDecodeError:
        logger.error("Failed to decode webhook payload")
        return False, {"error": "Invalid payload encoding"}
    
    try:
        # Parse JSON
        try:
            payload_dict = json.loads(payload_str)
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in webhook: {e}")
            return False, {"error": "Invalid JSON payload", "details": str(e)}
        
        # Verify webhook signature if enabled
        if settings.WEBHOOK_SIGNATURE_KEY:
            signature_valid, signature_error = verify_webhook_signature(payload_str, headers)
            if not signature_valid:
                logger.warning(f"Invalid webhook signature: {signature_error}")
                return False, {"error": "Invalid signature", "details": signature_error}
        
        # Validate basic payload structure
        try:
            base_payload = WebhookPayload(**payload_dict)
        except Exception as e:
            logger.error(f"Invalid webhook payload structure: {e}")
            return False, {"error": "Invalid payload structure", "details": str(e)}
        
        event_type = base_payload.event
        gateway_id = base_payload.id
        
        logger.info(f"Processing webhook event: {event_type}, gateway ID: {gateway_id}")
        
        # Record the incoming webhook event in database using context manager
        webhook_event_id = None
        try:
            async with get_repository_context(WebhookRepository) as webhook_repo:
                # Create webhook event record
                webhook_event = await webhook_repo.create_webhook_event(
                    event_type=event_type,
                    payload=payload_dict,
                    phone_number=payload_dict.get("payload", {}).get("phoneNumber"),
                    gateway_message_id=gateway_id
                )
                if webhook_event:
                    webhook_event_id = webhook_event.id
        except Exception as e:
            # Log error but continue processing - this is just for auditing
            logger.error(f"Error recording webhook event: {e}")
        
        # Process based on event type
        try:
            if event_type == "sms:received":
                payload = SmsReceivedPayload(**payload_dict["payload"])
                result = await process_sms_received(base_payload, payload)
            elif event_type == "sms:sent":
                payload = SmsSentPayload(**payload_dict["payload"])
                result = await process_sms_sent(base_payload, payload)
            elif event_type == "sms:delivered":
                payload = SmsDeliveredPayload(**payload_dict["payload"])
                result = await process_sms_delivered(base_payload, payload)
            elif event_type == "sms:failed":
                payload = SmsFailedPayload(**payload_dict["payload"])
                result = await process_sms_failed(base_payload, payload)
            elif event_type == "system:ping":
                payload = SystemPingPayload(**payload_dict["payload"])
                result = await process_system_ping(base_payload, payload)
            else:
                logger.warning(f"Unknown webhook event type: {event_type}")
                return False, {"error": "Unknown event type", "event_type": event_type}
                
            # Log successful processing
            logger.info(f"Successfully processed webhook event: {event_type}, gateway ID: {gateway_id}")
            
            # Mark event as processed if we created one - using context manager
            if webhook_event_id:
                try:
                    async with get_repository_context(WebhookRepository) as webhook_repo:
                        await webhook_repo.mark_event_processed(event_id=webhook_event_id)
                except Exception as e:
                    logger.error(f"Error marking webhook event as processed: {e}")
            
            return True, result
            
        except Exception as e:
            logger.error(f"Error processing webhook event {event_type}: {e}", exc_info=True)
            
            # Try to handle specific event processing errors gracefully
            error_details = {"error_type": type(e).__name__, "gateway_id": gateway_id}
            
            # Publish error event
            try:
                event_bus = get_event_bus()
                await event_bus.publish(
                    EventType.WEBHOOK_PROCESSED,
                    {
                        "success": False,
                        "event_type": event_type,
                        "gateway_id": gateway_id,
                        "error": str(e),
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
                )
            except Exception as publish_error:
                logger.error(f"Error publishing webhook error event: {publish_error}")
                
            return False, {"error": f"Error processing {event_type} event", "details": str(e), **error_details}
            
    except Exception as e:
        logger.error(f"Unexpected error in webhook processing: {e}", exc_info=True)
        return False, {"error": "Unexpected error", "details": str(e)}


def verify_webhook_signature(payload: str, headers: Dict[str, str]) -> Tuple[bool, Optional[str]]:
    """
    Verify webhook signature from SMS Gateway with detailed error reporting.
    
    Args:
        payload: Webhook payload string
        headers: Request headers
        
    Returns:
        Tuple[bool, Optional[str]]: (is_valid, error_message_or_none)
    """
    signature = headers.get("X-Signature")
    timestamp = headers.get("X-Timestamp")
    
    if not signature:
        return False, "Missing X-Signature header"
    
    if not timestamp:
        return False, "Missing X-Timestamp header"
    
    # Verify timestamp is recent (within tolerance)
    try:
        ts = int(timestamp)
        current_time = int(time.time())
        if abs(current_time - ts) > settings.WEBHOOK_TIMESTAMP_TOLERANCE:
            return False, f"Timestamp too old: {timestamp} (current: {current_time})"
    except (ValueError, TypeError):
        return False, f"Invalid timestamp format: {timestamp}"
    
    # Calculate expected signature
    message = (payload + timestamp).encode()
    expected_signature = hmac.new(
        settings.WEBHOOK_SIGNATURE_KEY.encode(),
        message,
        hashlib.sha256
    ).hexdigest()
    
    # Compare signatures (constant-time comparison)
    is_valid = hmac.compare_digest(expected_signature, signature)
    
    return is_valid, None if is_valid else "Signature mismatch"


async def process_sms_received(base_payload: WebhookPayload, payload: SmsReceivedPayload) -> Dict[str, Any]:
    """Process SMS received event."""
    # For inbound messages - not the main focus for now
    logger.info(f"Received SMS: {payload.phone_number} -> '{payload.message}'")
    
    # Publish event for other components - no DB operations needed
    event_bus = get_event_bus()
    await event_bus.publish(
        EventType.SMS_RECEIVED,
        {
            "gateway_id": base_payload.id,
            "device_id": base_payload.device_id,
            "phone_number": payload.phone_number,
            "message": payload.message,
            "timestamp": payload.received_at.isoformat()
        }
    )
    
    return {
        "status": "processed",
        "event": "sms:received",
        "phone_number": payload.phone_number
    }


async def process_sms_sent(base_payload: WebhookPayload, payload: SmsSentPayload) -> Dict[str, Any]:
    """
    Process SMS sent event with proper context management.
    
    Args:
        base_payload: Base webhook payload
        payload: SMS sent payload
        
    Returns:
        Dict[str, Any]: Processing result
    """
    logger.info(f"SMS sent to {payload.phone_number}, gateway ID: {base_payload.id}")
    
    # Extract gateway message ID
    gateway_id = base_payload.id
    user_id = None
    message_id = None
    
    # Find message by gateway ID - using context manager
    try:
        async with get_repository_context(MessageRepository) as message_repo:
            message = await message_repo.get_by_gateway_id(gateway_id)
            if not message:
                # This could be normal if we didn't originate this message
                logger.info(f"No matching message found for gateway ID: {gateway_id}")
                return {
                    "status": "acknowledged",
                    "event": "sms:sent",
                    "message_found": False
                }
            
            # Store these for use outside the context
            message_id = message.id
            user_id = message.user_id
            
            # Update message status
            updated_message = await message_repo.update_message_status(
                message_id=message_id,
                status=MessageStatus.SENT,
                event_type="webhook",
                gateway_message_id=gateway_id,
                data=jsonable_encoder(base_payload)
            )
            
            if not updated_message:
                logger.warning(f"Failed to update message status for ID: {message_id}")
                return {
                    "status": "error",
                    "event": "sms:sent",
                    "message_id": message_id,
                    "error": "Failed to update message status"
                }
    except Exception as e:
        logger.error(f"Error processing sms:sent webhook: {e}")
        return {
            "status": "error",
            "event": "sms:sent",
            "error": str(e)
        }
    
    # Publish event - outside the database context
    try:
        event_bus = get_event_bus()
        await event_bus.publish(
            EventType.MESSAGE_SENT,
            {
                "message_id": message_id,
                "gateway_id": gateway_id,
                "phone_number": payload.phone_number,
                "user_id": user_id,
                "timestamp": payload.sent_at.isoformat()
            }
        )
    except Exception as e:
        logger.error(f"Error publishing event for sms:sent: {e}")
    
    return {
        "status": "processed",
        "event": "sms:sent",
        "message_id": message_id,
        "phone_number": payload.phone_number
    }


async def process_sms_delivered(base_payload: WebhookPayload, payload: SmsDeliveredPayload) -> Dict[str, Any]:
    """
    Process SMS delivered event with proper context management.
    
    Args:
        base_payload: Base webhook payload
        payload: SMS delivered payload
        
    Returns:
        Dict[str, Any]: Processing result
    """
    logger.info(f"SMS delivered to {payload.phone_number}, gateway ID: {base_payload.id}")
    
    # Extract gateway message ID
    gateway_id = base_payload.id
    user_id = None
    message_id = None
    
    # Find message by gateway ID - using context manager
    try:
        async with get_repository_context(MessageRepository) as message_repo:
            message = await message_repo.get_by_gateway_id(gateway_id)
            if not message:
                logger.info(f"No matching message found for gateway ID: {gateway_id}")
                return {
                    "status": "acknowledged",
                    "event": "sms:delivered",
                    "message_found": False
                }
            
            # Store these for use outside the context
            message_id = message.id
            user_id = message.user_id
            
            # Update message status
            updated_message = await message_repo.update_message_status(
                message_id=message_id,
                status=MessageStatus.DELIVERED,
                event_type="webhook",
                gateway_message_id=gateway_id,
                data=jsonable_encoder(base_payload)
            )
            
            if not updated_message:
                logger.warning(f"Failed to update message status for ID: {message_id}")
                return {
                    "status": "error",
                    "event": "sms:delivered",
                    "message_id": message_id,
                    "error": "Failed to update message status"
                }
    except Exception as e:
        logger.error(f"Error processing sms:delivered webhook: {e}")
        return {
            "status": "error",
            "event": "sms:delivered",
            "error": str(e)
        }
    
    # Publish event - outside the database context
    try:
        event_bus = get_event_bus()
        await event_bus.publish(
            EventType.MESSAGE_DELIVERED,
            {
                "message_id": message_id,
                "gateway_id": gateway_id,
                "phone_number": payload.phone_number,
                "user_id": user_id,
                "timestamp": payload.delivered_at.isoformat()
            }
        )
    except Exception as e:
        logger.error(f"Error publishing event for sms:delivered: {e}")
    
    return {
        "status": "processed",
        "event": "sms:delivered",
        "message_id": message_id,
        "phone_number": payload.phone_number
    }


async def process_sms_failed(base_payload: WebhookPayload, payload: SmsFailedPayload) -> Dict[str, Any]:
    """
    Process SMS failed event with proper context management.
    
    Args:
        base_payload: Base webhook payload
        payload: SMS failed payload
        
    Returns:
        Dict[str, Any]: Processing result
    """
    logger.info(f"SMS failed for {payload.phone_number}, reason: {payload.reason}, gateway ID: {base_payload.id}")
    
    # Extract gateway message ID and failure reason
    gateway_id = base_payload.id
    reason = payload.reason
    user_id = None
    message_id = None
    
    # Find message by gateway ID - using context manager
    try:
        async with get_repository_context(MessageRepository) as message_repo:
            message = await message_repo.get_by_gateway_id(gateway_id)
            if not message:
                logger.info(f"No matching message found for gateway ID: {gateway_id}")
                return {
                    "status": "acknowledged",
                    "event": "sms:failed",
                    "message_found": False
                }
            
            # Store these for use outside the context
            message_id = message.id
            user_id = message.user_id
            
            # Update message status
            updated_message = await message_repo.update_message_status(
                message_id=message_id,
                status=MessageStatus.FAILED,
                event_type="webhook",
                reason=reason,
                gateway_message_id=gateway_id,
                data=jsonable_encoder(base_payload)
            )
            
            if not updated_message:
                logger.warning(f"Failed to update message status for ID: {message_id}")
                return {
                    "status": "error",
                    "event": "sms:failed",
                    "message_id": message_id,
                    "error": "Failed to update message status"
                }
    except Exception as e:
        logger.error(f"Error processing sms:failed webhook: {e}")
        return {
            "status": "error",
            "event": "sms:failed",
            "error": str(e)
        }
    
    # Publish event - outside the database context
    try:
        event_bus = get_event_bus()
        await event_bus.publish(
            EventType.MESSAGE_FAILED,
            {
                "message_id": message_id,
                "gateway_id": gateway_id,
                "phone_number": payload.phone_number,
                "user_id": user_id,
                "reason": reason,
                "timestamp": payload.failed_at.isoformat()
            }
        )
    except Exception as e:
        logger.error(f"Error publishing event for sms:failed: {e}")
    
    return {
        "status": "processed",
        "event": "sms:failed",
        "message_id": message_id,
        "phone_number": payload.phone_number,
        "reason": reason
    }


async def process_system_ping(base_payload: WebhookPayload, payload: SystemPingPayload) -> Dict[str, Any]:
    """Process system ping event."""
    logger.info(f"System ping received from device: {base_payload.device_id}")
    
    # Simple acknowledgment - no database operations needed
    return {
        "status": "acknowledged",
        "event": "system:ping",
        "device_id": base_payload.device_id
    }


async def fetch_registered_webhooks_from_gateway() -> List[Dict[str, Any]]:
    """
    Fetch registered webhooks from the SMS Gateway.
    Returns a list of registered webhooks or raises an error.
    """
    if not settings.SMS_GATEWAY_URL or not settings.SMS_GATEWAY_LOGIN or not settings.SMS_GATEWAY_PASSWORD:
        raise SMSGatewayError("SMS Gateway credentials not configured", code="SMS_GATEWAY_CONFIG_MISSING")

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{settings.SMS_GATEWAY_URL}/webhooks",
                auth=(settings.SMS_GATEWAY_LOGIN, settings.SMS_GATEWAY_PASSWORD),
                timeout=10.0
            )
            response.raise_for_status()
            return response.json()
    except Exception as e:
        logger.error(f"Error fetching registered webhooks: {e}")
        raise
</file>

<file path="app/api/v1/endpoints/campaigns.py">
# app/api/v1/endpoints/campaigns.py
from typing import List, Optional, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, Query, Path, status
from fastapi.responses import JSONResponse
import logging

from app.api.v1.dependencies import get_current_user, get_rate_limiter
from app.core.exceptions import ValidationError, NotFoundError
from app.schemas.campaign import (
    CampaignCreate,
    CampaignUpdate,
    CampaignResponse,
    CampaignStatus,
)
from app.schemas.user import User
from app.schemas.message import MessageResponse, CampaignBulkDeleteRequest, BulkDeleteResponse
from app.schemas.import_job import ImportJobResponse, ImportStatus
from app.utils.pagination import PaginationParams, paginate_response, PaginatedResponse, PageInfo
from app.services.campaigns.processor import get_campaign_processor
from app.db.session import get_repository_context
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.utils.datetime import utc_now

# Import Repository classes
from app.db.repositories.campaigns import CampaignRepository
from app.db.repositories.messages import MessageRepository  
from app.db.repositories.import_jobs import ImportJobRepository
from app.db.repositories.contacts import ContactRepository

router = APIRouter()
logger = logging.getLogger("inboxerr.endpoint")

@router.post("/", response_model=CampaignResponse, status_code=status.HTTP_201_CREATED)
async def create_campaign(
    campaign: CampaignCreate,
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Create a new campaign.
    
    This creates a campaign in draft status. Messages can be added later.
    """
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "create_campaign")
    
    try:
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Create campaign
            result = await campaign_repo.create_campaign(
                name=campaign.name,
                description=campaign.description,
                user_id=current_user.id,
                scheduled_start_at=campaign.scheduled_start_at,
                scheduled_end_at=campaign.scheduled_end_at,
                settings=campaign.settings
            )
            
            return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error creating campaign: {str(e)}")


@router.post("/from-import/{import_job_id}", response_model=CampaignResponse, status_code=status.HTTP_201_CREATED)
async def create_campaign_from_import(
    import_job_id: str,
    campaign: CampaignCreate,
    message_template: str = Query(..., description="Message template for the campaign"),
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
) -> CampaignResponse:
    """
    Create a campaign from an existing successful import job.

    **Phase 2A Workflow:**
    1. Upload CSV: POST /imports/upload (automatic processing)
    2. Monitor Progress: GET /imports/jobs/{job_id}  
    3. Create Campaign: POST /campaigns/from-import/{import_job_id} (this endpoint)
    
    This is the recommended approach for creating campaigns from CSV data.

    """
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "create_campaign")
    
    try:
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Verify import job exists and is successful
            import_job = await import_repo.get_by_id(import_job_id)
            if not import_job:
                raise NotFoundError(f"Import job {import_job_id} not found")
            
            # Check ownership
            if import_job.owner_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to use this import job"
                )
            
            # Check import job status
            if import_job.status != ImportStatus.SUCCESS:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Import job must be in SUCCESS status, currently {import_job.status.value}"
                )
        
        async with get_repository_context(ContactRepository) as contact_repo:
            # Count contacts from import
            contact_count = await contact_repo.get_contacts_count_by_import(import_job_id)
            
            if contact_count == 0:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Import job has no contacts to create campaign from"
                )
        
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Create campaign with import reference
            new_campaign = await campaign_repo.create_campaign(
                name=campaign.name,
                description=campaign.description,
                message_content=message_template,
                user_id=current_user.id,
                scheduled_start_at=campaign.scheduled_start_at,
                scheduled_end_at=campaign.scheduled_end_at,
                settings={
                    **campaign.settings,
                    "import_job_id": import_job_id,
                    "created_from_import": True
                },
                total_messages=contact_count
            )
        
        # Create messages from contacts using separate context
        async with get_repository_context() as session:
            await create_messages_from_contacts(
                session, new_campaign.id, import_job_id, message_template, current_user.id
            )
            await session.commit()
        
        # Get updated campaign
        async with get_repository_context(CampaignRepository) as campaign_repo:
            updated_campaign = await campaign_repo.get_by_id(new_campaign.id)
        
        logger.info(
            f"Created campaign {new_campaign.id} from import job {import_job_id} "
            f"with {contact_count} contacts"
        )
        
        return updated_campaign
        
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating campaign from import: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error creating campaign from import: {str(e)}"
        )


@router.get("/{campaign_id}/import-status", status_code=status.HTTP_200_OK)
async def get_campaign_import_status(
    campaign_id: str = Path(..., description="Campaign ID"),
    current_user: User = Depends(get_current_user),
) -> Dict[str, Any]:
    """
    Get import status for a campaign created from CSV.
    """
    try:
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Get campaign
            campaign = await campaign_repo.get_by_id(campaign_id)
            if not campaign:
                raise NotFoundError(f"Campaign {campaign_id} not found")
            
            # Check ownership
            if campaign.user_id != current_user.id:
                raise HTTPException(
                    status_code=status.HTTP_403_FORBIDDEN,
                    detail="Not authorized to access this campaign"
                )
            
            # Check if campaign was created from import
            import_job_id = campaign.settings.get("import_job_id") if campaign.settings else None
            if not import_job_id:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Campaign was not created from CSV import"
                )
        
        async with get_repository_context(ImportJobRepository) as import_repo:
            # Get import job status
            import_job = await import_repo.get_by_id(import_job_id)
            if not import_job:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Associated import job not found"
                )
            
            return {
                "campaign_id": campaign_id,
                "import_job_id": import_job_id,
                "import_status": import_job.status.value,
                "progress_percentage": import_job.progress_percentage,
                "rows_total": import_job.rows_total,
                "rows_processed": import_job.rows_processed,
                "error_count": import_job.error_count,
                "has_errors": import_job.has_errors,
                "created_from_csv": campaign.settings.get("created_from_csv", False),
                "import_started_at": import_job.started_at.isoformat() if import_job.started_at else None,
                "import_completed_at": import_job.completed_at.isoformat() if import_job.completed_at else None
            }
            
    except NotFoundError as e:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error getting campaign import status: {str(e)}"
        )


@router.get("/", response_model=PaginatedResponse[CampaignResponse])
async def list_campaigns(
    pagination: PaginationParams = Depends(),
    status: Optional[str] = Query(None, description="Filter by campaign status"),
    current_user: User = Depends(get_current_user),
):
    """
    List campaigns for the current user.
    
    Returns a paginated list of campaigns.
    """
    try:
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        
        async with get_repository_context(CampaignRepository) as campaign_repo:
            # Get campaigns with pagination
            campaigns, total = await campaign_repo.get_campaigns_for_user(
                user_id=current_user.id,
                status=status,
                skip=pagination.skip,
                limit=pagination.limit
            )
            
            # Calculate pagination info
            total_pages = (total + pagination.limit - 1) // pagination.limit
            
            # Return paginated response
            return paginate_response(campaigns, total, pagination)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing campaigns: {str(e)}")


@router.get("/{campaign_id}", response_model=CampaignResponse)
async def get_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
):
   """
   Get details of a specific campaign.
   """
   try:
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           # Get campaign
           campaign = await campaign_repo.get_by_id(campaign_id)
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           # Check authorization
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to access this campaign")
           
           return campaign
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error retrieving campaign: {str(e)}")


@router.put("/{campaign_id}", response_model=CampaignResponse)
async def update_campaign(
   campaign_update: CampaignUpdate,
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
):
   """
   Update campaign details.
   
   Only draft campaigns can be fully updated. Active campaigns can only have their description updated.
   """
   try:
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           # Get campaign
           campaign = await campaign_repo.get_by_id(campaign_id)
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           # Check authorization
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to update this campaign")
           
           # Check if campaign can be updated
           if campaign.status != "draft" and any([
               campaign_update.scheduled_start_at is not None,
               campaign_update.scheduled_end_at is not None,
               campaign_update.name is not None
           ]):
               raise HTTPException(
                   status_code=400, 
                   detail="Only draft campaigns can have name or schedule updated"
               )
           
           # Convert to dict and remove None values
           update_data = {k: v for k, v in campaign_update.dict().items() if v is not None}
           
           # Update campaign
           updated = await campaign_repo.update(id=campaign_id, obj_in=update_data)
           if not updated:
               raise HTTPException(status_code=500, detail="Failed to update campaign")
           
           return updated
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error updating campaign: {str(e)}")


@router.post("/{campaign_id}/start", response_model=CampaignResponse)
async def start_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
   campaign_processor = Depends(get_campaign_processor),
):
   """
   Start a campaign.
   
   This will change the campaign status to active and begin sending messages.
   """
   try:
       # Start campaign - campaign_processor already uses context managers internally
       success = await campaign_processor.start_campaign(
           campaign_id=campaign_id,
           user_id=current_user.id
       )
       
       if not success:
           raise HTTPException(status_code=400, detail="Failed to start campaign")
       
       # Get updated campaign - use context manager for this separate operation
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           return campaign
       
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error starting campaign: {str(e)}")


@router.post("/{campaign_id}/pause", response_model=CampaignResponse)
async def pause_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
   campaign_processor = Depends(get_campaign_processor),
):
   """
   Pause a campaign.
   
   This will change the campaign status to paused and stop sending messages.
   The campaign can be resumed later.
   """
   try:
       # Pause campaign - campaign_processor already uses context managers internally
       success = await campaign_processor.pause_campaign(
           campaign_id=campaign_id,
           user_id=current_user.id
       )
       
       if not success:
           raise HTTPException(status_code=400, detail="Failed to pause campaign")
       
       # Get updated campaign - use context manager for this separate operation
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           return campaign
       
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error pausing campaign: {str(e)}")


@router.post("/{campaign_id}/cancel", response_model=CampaignResponse)
async def cancel_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
   campaign_processor = Depends(get_campaign_processor),
):
   """
   Cancel a campaign.
   
   This will change the campaign status to cancelled and stop sending messages.
   The campaign cannot be resumed after cancellation.
   """
   try:
       # Cancel campaign - campaign_processor already uses context managers internally
       success = await campaign_processor.cancel_campaign(
           campaign_id=campaign_id,
           user_id=current_user.id
       )
       
       if not success:
           raise HTTPException(status_code=400, detail="Failed to cancel campaign")
       
       # Get updated campaign - use context manager for this separate operation
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           return campaign
       
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error cancelling campaign: {str(e)}")


@router.get("/{campaign_id}/messages", response_model=PaginatedResponse[MessageResponse])
async def get_campaign_messages(
   campaign_id: str = Path(..., description="Campaign ID"),
   pagination: PaginationParams = Depends(),
   status: Optional[str] = Query(None, description="Filter by message status"),
   current_user: User = Depends(get_current_user),
):
   """
   Get messages for a campaign.
   
   Returns a paginated list of messages for the specified campaign.
   """
   try:
       # Use repository context for proper connection management
       from app.db.repositories.campaigns import CampaignRepository
       from app.db.repositories.messages import MessageRepository
       
       # First check if campaign exists and belongs to user
       async with get_repository_context(CampaignRepository) as campaign_repo:
           campaign = await campaign_repo.get_by_id(campaign_id)
           
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to access this campaign")
       
       # Get messages for campaign - in a separate context to avoid long transactions
       async with get_repository_context(MessageRepository) as message_repo:
           messages, total = await message_repo.get_messages_for_campaign(
               campaign_id=campaign_id,
               status=status,
               skip=pagination.skip,
               limit=pagination.limit
           )

           # Calculate proper pagination info
           total_pages = (total + pagination.limit - 1) // pagination.limit

           # Create proper PageInfo object
           page_info = PageInfo(
               current_page=pagination.page,
               total_pages=total_pages,
               page_size=pagination.limit,
               total_items=total,
               has_previous=pagination.page > 1,
               has_next=pagination.page < total_pages
           )
           
           # Return PaginatedResponse object
           return PaginatedResponse(
               items=messages,  # Direct Message objects - FastAPI will serialize them
               page_info=page_info
           )
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error retrieving campaign messages: {str(e)}")


@router.delete("/{campaign_id}", status_code=204)
async def delete_campaign(
   campaign_id: str = Path(..., description="Campaign ID"),
   current_user: User = Depends(get_current_user),
):
   """
   Delete a campaign.
   
   Only draft campaigns can be deleted. Active, paused, or completed campaigns cannot be deleted.
   """
   try:
       from app.db.repositories.campaigns import CampaignRepository
       
       async with get_repository_context(CampaignRepository) as campaign_repo:
           # Get campaign
           campaign = await campaign_repo.get_by_id(campaign_id)
           if not campaign:
               raise NotFoundError(message=f"Campaign {campaign_id} not found")
           
           # Check authorization
           if campaign.user_id != current_user.id:
               raise HTTPException(status_code=403, detail="Not authorized to delete this campaign")
           
           # Check if campaign can be deleted
           if campaign.status != "draft":
               raise HTTPException(
                   status_code=400, 
                   detail="Only draft campaigns can be deleted"
               )
           
           # Delete campaign
           success = await campaign_repo.delete(id=campaign_id)
           if not success:
               raise HTTPException(status_code=500, detail="Failed to delete campaign")
           
           return JSONResponse(status_code=204, content=None)
       
   except NotFoundError as e:
       raise HTTPException(status_code=404, detail=str(e))
   except Exception as e:
       raise HTTPException(status_code=500, detail=f"Error deleting campaign: {str(e)}")
   

@router.delete("/{campaign_id}/messages/bulk", response_model=BulkDeleteResponse)
async def bulk_delete_campaign_messages(
    request: CampaignBulkDeleteRequest,
    campaign_id: str = Path(..., description="Campaign ID"),
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Bulk delete messages from a campaign with event safety and server stability.
    
    This endpoint efficiently deletes multiple messages belonging to a specific campaign
    with optional filtering by status and date range. Enhanced with delivery event safety
    checking and batched processing for high-volume operations handling 10K-30K messages.
    
    **Event Safety Features:**
    - Pre-deletion check for delivery events
    - Requires explicit confirmation to delete tracking data
    - Clear warnings about data loss implications
    - Two-phase deletion (events first, then messages)
    
    **Server Stability Features:**
    - Batched processing prevents server overload
    - Configurable batch sizes for different load scenarios
    - Inter-batch delays prevent database lock contention
    - Graceful handling of partial failures
    
    **Business Safety Features:**
    - Campaign ownership validation
    - User authorization checks  
    - Active campaign protection
    - Comprehensive audit logging
    
    **Performance:**
    - Single SQL query per batch
    - Optimized for large datasets
    - 30K deletions in batches for stability
    
    **Business Use Cases:**
    - Clean up failed messages from campaigns
    - Remove test messages before campaign launch
    - Compliance-driven message deletion (with event confirmation)
    - Campaign optimization and cleanup
    
    Args:
        campaign_id: ID of the campaign containing messages to delete
        request: Bulk delete request with filters, confirmation, and force options
        current_user: Authenticated user (injected by dependency)
        rate_limiter: Rate limiting for bulk operations (injected by dependency)
    
    Returns:
        BulkDeleteResponse: Detailed results including event safety information
        
    Raises:
        HTTPException 400: Invalid request, campaign state, or missing confirmation
        HTTPException 403: User not authorized for this campaign
        HTTPException 404: Campaign not found
        HTTPException 429: Rate limit exceeded
        HTTPException 500: Database or internal server error
    """
    import time
    
    # Check rate limits for bulk operations
    await rate_limiter.check_rate_limit(current_user.id, "bulk_delete_campaign")
    
    start_time = time.time()
    
    try:
        # Use repository context for proper connection management
        from app.db.repositories.campaigns import CampaignRepository
        from app.db.repositories.messages import MessageRepository
        
        # First validate campaign exists and user has access
        async with get_repository_context(CampaignRepository) as campaign_repo:
            campaign = await campaign_repo.get_by_id(campaign_id)
            
            if not campaign:
                raise NotFoundError(message=f"Campaign {campaign_id} not found")
            
            # Check authorization
            if campaign.user_id != current_user.id:
                raise HTTPException(
                    status_code=403, 
                    detail="Not authorized to delete messages from this campaign"
                )
            
            # Safety check - prevent deletion from active campaigns unless force delete
            if campaign.status == "active" and not request.force_delete:
                raise HTTPException(
                    status_code=400,
                    detail="Cannot bulk delete messages from active campaign. Pause the campaign first or use force_delete."
                )
        
        # Perform bulk deletion with event safety
        async with get_repository_context(MessageRepository) as message_repo:
            # Convert datetime objects to ISO strings for repository method
            from_date_str = request.from_date.isoformat() if request.from_date else None
            to_date_str = request.to_date.isoformat() if request.to_date else None
            
            # Execute bulk deletion with enhanced safety
            deleted_count, failed_message_ids, metadata = await message_repo.bulk_delete_campaign_messages(
                campaign_id=campaign_id,
                user_id=current_user.id,
                status=request.status.value if request.status else None,
                from_date=from_date_str,
                to_date=to_date_str,
                limit=request.limit,
                force_delete=request.force_delete,
                batch_size=request.batch_size
            )
            
            # Calculate execution time
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # Build applied filters for audit trail
            filters_applied = {}
            if request.status:
                filters_applied["status"] = request.status.value
            if request.from_date:
                filters_applied["from_date"] = from_date_str
            if request.to_date:
                filters_applied["to_date"] = to_date_str
            filters_applied["limit"] = request.limit
            filters_applied["force_delete"] = request.force_delete
            filters_applied["batch_size"] = request.batch_size
            
            # Build response with enhanced metadata
            response = BulkDeleteResponse(
                deleted_count=deleted_count,
                campaign_id=campaign_id,
                failed_count=len(failed_message_ids),
                errors=[f"Failed to delete message: {msg_id}" for msg_id in failed_message_ids],
                operation_type="campaign",
                filters_applied=filters_applied,
                execution_time_ms=execution_time_ms,
                requires_confirmation=metadata.get("requires_confirmation", False),
                events_count=metadata.get("events_count"),
                events_deleted=metadata.get("events_deleted", 0),
                safety_warnings=metadata.get("safety_warnings", []),
                batch_info=metadata.get("batch_info")
            )
            
            # Log successful operation for audit
            logger.info(
                f"User {current_user.id} bulk deleted {deleted_count} messages "
                f"from campaign {campaign_id} in {execution_time_ms}ms "
                f"with filters: {filters_applied}. Events deleted: {metadata.get('events_deleted', 0)}"
            )
            
            return response
            
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        # Log error and return generic error message
        logger.error(f"Error in bulk_delete_campaign_messages: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Error performing bulk deletion: {str(e)}"
        )
    
async def create_messages_from_contacts(
    session,
    campaign_id: str,
    import_job_id: str,
    message_template: str,
    user_id: Optional[str] = None
) -> int:
    """
    Create messages from imported contacts.
    
    **Phase 2A Pattern:** This function creates campaign messages from contacts
    that were already imported via the /imports/upload pipeline.
    """
    try:
        # Use proper repository classes
        contact_repo = ContactRepository(session)
        message_repo = MessageRepository(session)
        
        # Get all contacts from import
        contacts, _ = await contact_repo.get_by_import_id(import_job_id, limit=10000)
        
        if not contacts:
            logger.warning(f"No contacts found for import job {import_job_id}")
            return 0
        
        # Get user_id from campaign if not provided
        if not user_id:
            from app.models.campaign import Campaign
            from sqlalchemy import select
            campaign_result = await session.execute(
                select(Campaign.user_id).where(Campaign.id == campaign_id)
            )
            user_id = campaign_result.scalar()
        
        # Create messages with personalization
        messages_created = 0
        
        for contact in contacts:
            # Create personalized message using simple template substitution
            personalized_message = message_template
            
            # Variable substitution
            if contact.name:
                personalized_message = personalized_message.replace("{{name}}", contact.name)
                personalized_message = personalized_message.replace("{{contact_name}}", contact.name)
            
            personalized_message = personalized_message.replace("{{phone}}", contact.phone)
            
            # Create message using repository
            await message_repo.create_message(
                phone_number=contact.phone,
                message_text=personalized_message,
                user_id=user_id,
                campaign_id=campaign_id,
                metadata={
                    "contact_name": contact.name,
                    "import_job_id": import_job_id,
                    "contact_tags": contact.tags or []
                }
            )
            messages_created += 1
        
        logger.info(
            f"Created {messages_created} messages from {len(contacts)} contacts "
            f"for campaign {campaign_id}"
        )
        return messages_created
        
    except Exception as e:
        logger.error(f"Error creating messages from contacts: {str(e)}")
        raise
</file>

<file path="app/api/v1/endpoints/messages.py">
"""
API endpoints for SMS message management.
"""
import csv
import io
from datetime import datetime
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, UploadFile, File, Query, Path, status
import logging


from app.api.v1.dependencies import get_current_user, get_rate_limiter, verify_api_key
from app.core.exceptions import ValidationError, SMSGatewayError, NotFoundError
from app.schemas.message import (
    MessageCreate,
    MessageResponse, 
    MessageSendAcceptedResponse,
    BatchSendAcceptedResponse,
    BatchMessageRequest, 
    BatchMessageResponse,
    BatchOptions,
    MessageStatus,
    MessageStatusUpdate,
    GlobalBulkDeleteRequest,
    BulkDeleteResponse
)
from app.services.sms.sender import get_sms_sender
from app.schemas.user import User
from app.utils.pagination import PaginationParams, paginate_response
from app.utils.pagination import PaginatedResponse
from app.db.session import get_repository_context

router = APIRouter()
logger = logging.getLogger("inboxerr.endpoint")

# ===========================
# COLLECTION OPERATIONS
# ===========================

@router.get("/", response_model=PaginatedResponse[MessageResponse])
async def list_messages(
    pagination: PaginationParams = Depends(),
    status: Optional[str] = Query(None, description="Filter by message status"),
    phone_number: Optional[str] = Query(None, description="Filter by phone number"),
    from_date: Optional[str] = Query(None, description="Filter from date (ISO format)"),
    to_date: Optional[str] = Query(None, description="Filter to date (ISO format)"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    List messages with optional filtering.
    
    Returns a paginated list of messages for the current user.
    """
    try:
        filters = {
            "status": status,
            "phone_number": phone_number,
            "from_date": from_date,
            "to_date": to_date,
            "user_id": current_user.id
        }
        
        # Get messages with pagination
        messages, total = await sms_sender.list_messages(
            filters=filters,
            skip=pagination.skip,
            limit=pagination.limit
        )
        
        # Return paginated response
        return paginate_response(
            items=messages,
            total=total,
            pagination=pagination
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error listing messages: {str(e)}")

@router.post("/send", response_model=MessageSendAcceptedResponse, status_code=202)
async def send_message(
    message: MessageCreate,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Send a single SMS message.
    
    - **phone_number**: Recipient phone number in E.164 format (e.g., +1234567890)
    - **message**: Content of the SMS message
    - **scheduled_at**: Optional timestamp to schedule the message for future delivery
    
    Returns 202 immediately with task_id for tracking progress.
    """
    # Check rate limits
    await rate_limiter.check_rate_limit(current_user.id, "send_message")
    
    # Validate phone number early (before background task)
    from app.utils.phone import validate_phone
    is_valid, formatted_number, error, _ = validate_phone(message.phone_number)
    if not is_valid:
        raise HTTPException(status_code=422, detail=f"Invalid phone number: {error}")
    
    # Generate task ID for tracking
    from app.utils.ids import generate_prefixed_id, IDPrefix
    task_id = generate_prefixed_id(IDPrefix.TASK) # 	Merely a UUID you log so you can match logs to HTTP requests. You’re not using it elsewhere.
    
    # Add to background tasks - this returns immediately
    background_tasks.add_task(
        _send_message_background,
        sms_sender=sms_sender,
        phone_number=message.phone_number,
        message_text=message.message,
        user_id=current_user.id,
        scheduled_at=message.scheduled_at,
        custom_id=message.custom_id,
        task_id=task_id
    )
    
    # Return 202 immediately
    return {
        "status": "accepted",
        "message": "Message queued for sending",
        "task_id": task_id,
        "phone_number": formatted_number
    }


@router.post("/batch", response_model=BatchSendAcceptedResponse, status_code=202)
async def send_batch(
    batch: BatchMessageRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Send a batch of SMS messages.
    
    - **messages**: List of messages to send
    - **options**: Optional batch processing options
    
    Returns 202 immediately with batch_id for tracking progress.
    """
    # Check rate limits - higher limit for batch operations
    await rate_limiter.check_rate_limit(current_user.id, "send_batch")
    
    if not batch.messages:
        raise ValidationError(message="Batch contains no messages")
    
    # Validate messages early (before background task)
    from app.utils.phone import validate_phone
    invalid_numbers = []
    for i, msg in enumerate(batch.messages):
        is_valid, _, error, _ = validate_phone(msg.phone_number)
        if not is_valid:
            invalid_numbers.append(f"Message {i}: {error}")
    
    if invalid_numbers:
        raise HTTPException(
            status_code=422, 
            detail=f"Invalid phone numbers: {'; '.join(invalid_numbers[:3])}"
        )
    
    # Generate batch/task ID for tracking
    from app.utils.ids import generate_prefixed_id, IDPrefix
    batch_id = generate_prefixed_id(IDPrefix.BATCH)
    task_id = generate_prefixed_id(IDPrefix.TASK) #	Merely a UUID you log so you can match logs to HTTP requests. You’re not using it elsewhere.
    
    # Add to background tasks - this returns immediately
    background_tasks.add_task(
        _send_batch_background,
        sms_sender=sms_sender,
        messages=batch.messages,
        user_id=current_user.id,
        options=batch.options,
        batch_id=batch_id,
        task_id=task_id
    )
    
    # Return 202 immediately
    return {
        "status": "accepted", 
        "message": f"Batch of {len(batch.messages)} messages queued for processing",
        "batch_id": batch_id,
        "task_id": task_id,
        "total": len(batch.messages),
        "processed": 0,
        "successful": 0,
        "failed": 0
    }


@router.delete("/bulk", response_model=BulkDeleteResponse)
async def bulk_delete_messages(
    request: GlobalBulkDeleteRequest,
    current_user: User = Depends(get_current_user),
    rate_limiter = Depends(get_rate_limiter),
):
    """
    Global bulk delete messages by message IDs with event safety.
    
    This endpoint efficiently deletes multiple messages by their specific IDs across
    campaigns or for orphaned message cleanup. Enhanced with delivery event safety
    checking for edge cases, power user operations, and system maintenance tasks.
    
    **Event Safety Features:**
    - Pre-deletion check for delivery events
    - Requires explicit confirmation to delete tracking data
    - Clear warnings about analytics data loss
    - Two-phase deletion (events first, then messages)
    
    **Safety Features:**
    - User authorization (can only delete own messages)
    - Smaller batch limits (1K vs 10K for campaign-scoped)
    - Optional campaign context validation
    - Detailed failure reporting for partial operations
    - Comprehensive audit logging
    
    **Use Cases:**
    - Cross-campaign message cleanup by power users
    - Frontend multi-select bulk operations
    - Orphaned message removal during maintenance
    - Compliance-driven deletion by specific message IDs
    - System administration tasks
    
    **Performance:**
    - Handles up to 1K message deletions efficiently
    - Uses optimized IN clause with PostgreSQL
    - Smaller batches for safety vs campaign operations
    - Batched processing prevents server overload
    
    Args:
        request: Global bulk delete request with message IDs, confirmation, and force options
        current_user: Authenticated user (injected by dependency)
        rate_limiter: Rate limiting for bulk operations (injected by dependency)
    
    Returns:
        BulkDeleteResponse: Detailed results including event safety information
        
    Raises:
        HTTPException 400: Invalid request or missing confirmation
        HTTPException 403: User not authorized for specified messages
        HTTPException 429: Rate limit exceeded
        HTTPException 500: Database or internal server error
    """
    import time
    
    # Check rate limits for global bulk operations
    await rate_limiter.check_rate_limit(current_user.id, "bulk_delete_global")
    
    start_time = time.time()
    
    try:
        # Use repository context for proper connection management
        from app.db.repositories.messages import MessageRepository
        
        # Perform global bulk deletion with event safety
        async with get_repository_context(MessageRepository) as message_repo:
            # Execute bulk deletion with enhanced safety
            deleted_count, failed_message_ids, metadata = await message_repo.bulk_delete_messages(
                message_ids=request.message_ids,
                user_id=current_user.id,
                campaign_id=request.campaign_id,
                force_delete=request.force_delete
            )
            
            # Calculate execution time
            execution_time_ms = int((time.time() - start_time) * 1000)
            
            # Build applied filters for audit trail
            filters_applied = {
                "message_count": len(request.message_ids),
                "unique_messages": len(set(request.message_ids)),
                "force_delete": request.force_delete
            }
            if request.campaign_id:
                filters_applied["campaign_context"] = request.campaign_id
            
            # Build detailed error messages for failed deletions
            error_messages = []
            if failed_message_ids:
                if metadata.get("requires_confirmation"):
                    # Failed due to event safety check
                    error_messages = metadata.get("safety_warnings", [])
                else:
                    # Failed due to other reasons (not found, not owned, etc.)
                    error_messages = [
                        f"Failed to delete message: {msg_id} (not found or not owned by user)" 
                        for msg_id in failed_message_ids
                    ]
            
            # Build response with enhanced metadata
            response = BulkDeleteResponse(
                deleted_count=deleted_count,
                campaign_id=request.campaign_id,
                failed_count=len(failed_message_ids),
                errors=error_messages,
                operation_type="global",
                filters_applied=filters_applied,
                execution_time_ms=execution_time_ms,
                requires_confirmation=metadata.get("requires_confirmation", False),
                events_count=metadata.get("events_count"),
                events_deleted=metadata.get("events_deleted", 0),
                safety_warnings=metadata.get("safety_warnings", []),
                batch_info=None  # Global operations use smaller fixed batches
            )
            
            # Log successful operation for audit
            logger.info(
                f"User {current_user.id} performed global bulk delete of {deleted_count} messages "
                f"in {execution_time_ms}ms. Failed: {len(failed_message_ids)}. "
                f"Campaign context: {request.campaign_id}. Events deleted: {metadata.get('events_deleted', 0)}. "
                f"Force delete: {request.force_delete}"
            )
            
            return response
            
    except Exception as e:
        # Log error and return generic error message
        logger.error(f"Error in global bulk_delete_messages: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Error performing global bulk deletion: {str(e)}"
        )

# ===========================
# NESTED RESOURCES SECTION
# ===========================

@router.get("/tasks/{task_id}", response_model=dict)
async def get_task_status(
    task_id: str = Path(..., description="Task ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Check the status of a background task.
    
    Used for tracking progress of batch operations and imports.
    """
    try:
        task_status = await sms_sender.get_task_status(task_id, user_id=current_user.id)
        if not task_status:
            raise NotFoundError(message=f"Task {task_id} not found")
        
        return task_status
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving task status: {str(e)}")
    

# ===========================
# INDIVIDUAL RESOURCES (ALWAYS LAST)
# ===========================


@router.get("/{message_id}", response_model=MessageResponse)
async def get_message(
    message_id: str = Path(..., description="Message ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Get details of a specific message.
    """
    try:
        message = await sms_sender.get_message(message_id, user_id=current_user.id)
        if not message:
            raise NotFoundError(message=f"Message {message_id} not found")
        return message
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving message: {str(e)}")



@router.put("/{message_id}/status", response_model=MessageResponse)
async def update_message_status(
    status_update: MessageStatusUpdate,
    message_id: str = Path(..., description="Message ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Update the status of a message.
    
    This is primarily for administrative purposes or handling external status updates.
    """
    try:
        # Verify the user has permission to update this message
        message = await sms_sender.get_message(message_id, user_id=current_user.id)
        if not message:
            raise NotFoundError(message=f"Message {message_id} not found")
        
        # Update the status
        updated_message = await sms_sender.update_message_status(
            message_id=message_id,
            status=status_update.status,
            reason=status_update.reason,
            user_id=current_user.id
        )
        
        return updated_message
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValidationError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating message status: {str(e)}")


@router.delete("/{message_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_message(
    message_id: str = Path(..., description="Message ID"),
    current_user: User = Depends(get_current_user),
    sms_sender = Depends(get_sms_sender),
):
    """
    Delete a message.
    
    This will only remove it from the database, but cannot recall messages already sent.
    """
    try:
        # Verify the message exists and belongs to the user
        message = await sms_sender.get_message(message_id, user_id=current_user.id)
        if not message:
            raise NotFoundError(message=f"Message {message_id} not found")
        
        # Delete the message
        success = await sms_sender.delete_message(message_id, user_id=current_user.id)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to delete message")
        
        return None  # No content response
        
    except NotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting message: {str(e)}")


# Background task functions
async def _send_message_background(
    sms_sender,
    phone_number: str,
    message_text: str,
    user_id: str,
    scheduled_at: Optional[datetime] = None,
    custom_id: Optional[str] = None,
    task_id: Optional[str] = None
):
    """
    Background task for sending a single message.
    """
    try:
        result = await sms_sender.send_message(
            phone_number=phone_number,
            message_text=message_text,
            user_id=user_id,
            scheduled_at=scheduled_at,
            custom_id=custom_id,
        )
        logger.info(f"Background message send completed for task {task_id}: {result.get('id', 'unknown')}")
    except Exception as e:
        logger.error(f"Background message send failed for task {task_id}: {str(e)}")

async def _send_batch_background(
    sms_sender,
    messages: List[MessageCreate],
    user_id: str,
    options: Optional[BatchOptions] = None,
    batch_id: Optional[str] = None,
    task_id: Optional[str] = None,
):
    """
    Background task for sending a batch of messages.
    """
    try:
        result = await sms_sender.send_batch(
            messages=messages,
            user_id=user_id,
            options=options,
            batch_id=batch_id
        )
        logger.info(f"Background batch send completed for task {task_id} (batch {batch_id}): {result.get('batch_id', 'unknown')}")
    except Exception as e:
        logger.error(f"Background batch send failed for task {task_id} (batch {batch_id}): {str(e)}")
</file>

<file path="app/services/sms/sender.py">
"""
SMS sender service for interacting with the Android SMS Gateway.
"""
import asyncio
import logging
import uuid
import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Tuple, Union
from httpx import HTTPStatusError

from app.core.config import settings
from app.core.exceptions import ValidationError, SMSGatewayError, RetryableError, SMSAuthError
from app.utils.phone import validate_phone
from app.db.repositories.templates import TemplateRepository
from app.db.repositories.messages import MessageRepository
from app.schemas.message import MessageCreate, MessageStatus, BatchMessageRequest, BatchOptions
from app.services.event_bus.events import EventType
from app.db.session import get_repository_context, get_repository, get_session

# Lazy import of android_sms_gateway to avoid import errors if not installed
try:
    from android_sms_gateway import client, domain
    SMS_GATEWAY_AVAILABLE = True
except ImportError:
    SMS_GATEWAY_AVAILABLE = False


logger = logging.getLogger("inboxerr.sms")


class SMSSender:
    """
    Service for sending SMS messages through the Android SMS Gateway.
    """
    
    def __init__(
        self,
        event_bus: Any
    ):
        """
        Initialize SMS sender service.
        
        Args:
            message_repository: Repository for message storage
            event_bus: Event bus for publishing events
        """
        self.event_bus = event_bus
        self._semaphore = asyncio.Semaphore(10)  # Limit concurrent requests
        self._last_send_time = 0
        
        # Check if gateway client is available
        if not SMS_GATEWAY_AVAILABLE:
            logger.warning("Android SMS Gateway client not installed. SMS sending will be simulated.")
    
    async def send_message(
        self,
        *,
        phone_number: str,
        message_text: str,
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        custom_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        campaign_id: Optional[str] = None,
        priority: int = 0,
        ttl: Optional[int] = None,
        sim_number: Optional[int] = None,
        is_encrypted: bool = False
    ) -> Dict[str, Any]:
        """
        Send a single SMS message.
        
        Args:
            phone_number: Recipient phone number
            message_text: Message content
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            custom_id: Optional custom ID for tracking
            metadata: Optional additional data
            campaign_id: Optional campaign ID
            priority: Message priority (0-127, ≥100 bypasses limits)
            ttl: Time-to-live in seconds
            sim_number: SIM card to use (1-3)
            is_encrypted: Whether message is encrypted
            
        Returns:
            Dict: Message details with status
            
        Raises:
            ValidationError: If phone number is invalid
            SMSGatewayError: If there's an error sending the message
        """
        # Validate phone number
        is_valid, formatted_number, error, _ = validate_phone(phone_number)
        if not is_valid:
            raise ValidationError(message=f"Invalid phone number: {error}")
        
        # Generate id to track message in the system.
        custom_id = custom_id or str(uuid.uuid4())

        # Use context manager for repository
        async with get_repository_context(MessageRepository) as repo:
            # Create message in database
            db_message = await repo.create_message(
                phone_number=formatted_number,
                message_text=message_text,
                user_id=user_id,
                custom_id=custom_id,
                scheduled_at=scheduled_at,
                metadata=metadata or {},
                campaign_id=campaign_id
            )
            
            # ✅ ALWAYS publish MESSAGE_CREATED event (this was missing!)
            await self.event_bus.publish(
                EventType.MESSAGE_CREATED,
                {
                    "message_id": db_message.id,
                    "phone_number": formatted_number,
                    "user_id": user_id,
                    "campaign_id": campaign_id
                }
            )
            
            # If scheduled for future, return message details
            if scheduled_at and scheduled_at > datetime.now(timezone.utc):
                logger.info(f"Message {db_message.id} scheduled for {scheduled_at}")
                
                # Publish scheduled event
                await self.event_bus.publish(
                    EventType.MESSAGE_SCHEDULED,
                    {
                        "message_id": db_message.id,
                        "phone_number": formatted_number,
                        "scheduled_at": scheduled_at.isoformat(),
                        "user_id": user_id
                    }
                )
                
                return db_message.dict()
            
            # Otherwise, send immediately
            try:
                result = await self._send_to_gateway(
                    phone_number=formatted_number,
                    message_text=message_text,
                    custom_id=db_message.custom_id,
                    priority=priority,
                    ttl=ttl,
                    sim_number=sim_number,
                    is_encrypted=is_encrypted
                )
                
                # Update message status
                await repo.update_message_status(
                    message_id=db_message.id,
                    status=result.get("status", MessageStatus.PENDING),
                    event_type="gateway_response",
                    gateway_message_id=result.get("gateway_message_id"),
                    data=result
                )
                
                # ✅ Publish MESSAGE_SENT event (this was missing!)
                await self.event_bus.publish(
                    EventType.MESSAGE_SENT,
                    {
                        "message_id": db_message.id,
                        "phone_number": formatted_number,
                        "user_id": user_id,
                        "gateway_message_id": result.get("gateway_message_id")
                    }
                )
                
                # Get updated message
                updated_message = await repo.get_by_id(db_message.id)
                return updated_message.dict()
                
            except Exception as e:
                # Handle error
                error_status = MessageStatus.FAILED
                error_message = str(e)
                logger.error(f"Error sending message {db_message.id}: {error_message}")
                
                # Update message status
                await repo.update_message_status(
                    message_id=db_message.id,
                    status=error_status,
                    event_type="send_error",
                    reason=error_message,
                    data={"error": error_message}
                )
                
                # ✅ Publish MESSAGE_FAILED event (this was missing!)
                await self.event_bus.publish(
                    EventType.MESSAGE_FAILED,
                    {
                        "message_id": db_message.id,
                        "phone_number": formatted_number,
                        "user_id": user_id,
                        "reason": error_message
                    }
                )
                
                # Re-raise as SMSGatewayError
                if isinstance(e, RetryableError):
                    raise SMSGatewayError(message=error_message, code="GATEWAY_ERROR", status_code=503)
                else:
                    raise SMSGatewayError(message=error_message, code="GATEWAY_ERROR")

    async def send_batch(
        self,
        *,
        messages: List[MessageCreate],
        user_id: str,
        options: Optional[BatchOptions] = None,
        campaign_id: Optional[str] = None,
        batch_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a batch of SMS messages.
        
        Args:
            messages: List of messages to send
            user_id: User ID
            options: Optional batch processing options
            campaign_id: Optional campaign ID
            
        Returns:
            Dict: Batch details with status
            
        Raises:
            ValidationError: If any phone number is invalid
            SMSGatewayError: If there's an error sending the messages
        """
        if not messages:
            raise ValidationError(message="No messages provided")
        
        # Set default options
        if not options:
            options = BatchOptions(
                delay_between_messages=0.3,
                fail_on_first_error=False,
                retry_failed=True
            )
        
        # Create batch in database using context manager
        async with get_repository_context(MessageRepository) as repo:
            batch = await repo.create_batch(
                user_id=user_id,
                name=f"Batch {datetime.now(timezone.utc).isoformat()}",
                total=len(messages),
                batch_id=batch_id
            )
            
            # Process in background
            asyncio.create_task(
                self._process_batch(
                    messages=messages,
                    user_id=user_id,
                    batch_id=batch.id,
                    campaign_id=campaign_id,
                    options=options
                )
            )
            
            # Return batch details
            return {
                "batch_id": batch.id,
                "total": batch.total,
                "processed": 0,
                "successful": 0,
                "failed": 0,
                "status": batch.status,
                "created_at": batch.created_at
            }

    async def _process_batch(
        self,
        *,
        messages: List[MessageCreate],
        user_id: str,
        batch_id: str,
        campaign_id: Optional[str] = None,
        options: BatchOptions
    ) -> None:
        """
        Process a batch of messages with improved concurrency handling.
        
        Args:
            messages: List of messages to send
            user_id: User ID
            batch_id: Batch ID
            campaign_id: Optional campaign ID
            options: Batch processing options
        """
        processed = 0
        successful = 0
        failed = 0
        
        # Create a semaphore to limit concurrent processing
        send_semaphore = asyncio.Semaphore(5)  # Limit concurrent sends
        
        try:
            # Calculate optimal chunk size - smaller for better reliability
            total_messages = len(messages)
            chunk_size = min(20, max(5, total_messages // 10))  # Smaller chunks
            
            logger.info(f"Processing batch {batch_id} with {total_messages} messages in chunks of {chunk_size}")
            
            # Process in chunks for better performance
            for i in range(0, total_messages, chunk_size):
                chunk = messages[i:i+chunk_size]

                # Process all messages in this chunk concurrently
                tasks = [
                    self._process_single_message_safely(
                        message=msg,
                        user_id=user_id,
                        batch_id=batch_id,
                        campaign_id=campaign_id,
                        semaphore=send_semaphore
                    )
                    for msg in chunk
                ]
            
                # Wait for all tasks in this chunk
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # Count successes and failures
                chunk_successful = sum(1 for r in results if r is True)
                chunk_failed = sum(1 for r in results if r is False or isinstance(r, Exception))

                # Update batch progress with a proper context-managed repository
                async with get_repository_context(MessageRepository) as repo:
                    await repo.update_batch_progress(
                        batch_id=batch_id,
                        increment_processed=len(chunk),
                        increment_successful=chunk_successful,
                        increment_failed=chunk_failed
                    )

                # Update counters
                processed += len(chunk)
                successful += chunk_successful
                failed += chunk_failed

                # Report progress
                progress_pct = (processed / total_messages) * 100
                logger.info(f"Batch {batch_id} progress: {progress_pct:.1f}% ({processed}/{total_messages})")
                
                # Add delay between chunks
                if i + chunk_size < total_messages:
                    await asyncio.sleep(options.delay_between_messages * 2)  # Double the delay for stability
            
            # Final update with status
            final_status = MessageStatus.PROCESSED
            if processed == 0:
                final_status = MessageStatus.FAILED
            elif failed > 0:
                final_status = "partial"
            
            async with get_repository_context(MessageRepository) as repo:
                await repo.update_batch_progress(
                    batch_id=batch_id,
                    status=final_status
                )
            
            # Publish event - this doesn't need a database connection
            await self.event_bus.publish(
                EventType.BATCH_COMPLETED,
                {
                    "batch_id": batch_id,
                    "campaign_id": campaign_id,
                    "total": total_messages,
                    "processed": processed,
                    "successful": successful,
                    "failed": failed,
                    "status": final_status,
                    "user_id": user_id
                }
            )
            
        except Exception as e:
            logger.error(f"Batch processing error: {e}", exc_info=True)
            
            # Update batch status to error state with proper connection handling
            try:
                async with get_repository_context(MessageRepository) as repo:
                    await repo.update_batch_progress(
                        batch_id=batch_id,
                        status=MessageStatus.FAILED
                    )
            except Exception as update_error:
                logger.error(f"Failed to update batch status after error: {update_error}")

    # This method looks good with your changes!
    async def _process_single_message_safely(
        self,
        *,
        message: MessageCreate,
        user_id: str,
        batch_id: str,
        campaign_id: Optional[str] = None,
        semaphore: asyncio.Semaphore
    ) -> bool:
        """
        Process a single message with proper connection handling.
        
        Args:
            message: Message to process
            user_id: User ID
            batch_id: Batch ID
            campaign_id: Optional campaign ID
            semaphore: Semaphore for concurrency control
            
        Returns:
            bool: True if successful, False if failed
        """
        # Use the context manager to ensure proper session lifecycle
        try:
            async with get_repository_context(MessageRepository) as repo:
                # Build metadata
                metadata = {"batch_id": batch_id}
                if campaign_id:
                    metadata["campaign_id"] = campaign_id
                
                # Create message in database
                db_message = await repo.create_message(
                    phone_number=message.phone_number,
                    message_text=message.message,
                    user_id=user_id,
                    custom_id=message.custom_id or str(uuid.uuid4()),
                    scheduled_at=message.scheduled_at,
                    metadata=metadata,
                    batch_id=batch_id,
                    campaign_id=campaign_id
                )

                # Skip scheduled messages
                if db_message.scheduled_at and db_message.scheduled_at > datetime.now(timezone.utc):
                    return True
                
                # Process this message with rate limiting
                async with semaphore:
                    # Send message
                    result = await self._send_to_gateway(
                        phone_number=db_message.phone_number,
                        message_text=db_message.message,
                        custom_id=db_message.custom_id
                    )
                    
                    # Update message status in the same session
                    await repo.update_message_status(
                        message_id=db_message.id,
                        status=result.get("status", MessageStatus.PENDING),
                        event_type="gateway_response",
                        gateway_message_id=result.get("gateway_message_id"),
                        data=result
                    )
                    
                    return True
                    
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            
            # Try to record the failure if we have a message
            if 'db_message' in locals():
                try:
                    async with get_repository_context(MessageRepository) as err_repo:
                        await err_repo.update_message_status(
                            message_id=db_message.id,
                            status=MessageStatus.FAILED,
                            event_type="send_error",
                            reason=str(e),
                            data={"error": str(e)}
                        )
                except Exception as update_error:
                    logger.error(f"Failed to update error status: {update_error}")
            
            return False

    async def _process_single_message(
        self,
        *,
        message: MessageCreate,
        user_id: str,
        batch_id: Optional[str] = None,
        campaign_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Process a single message within a batch.
        
        Args:
            message: Message to send
            user_id: User ID
            batch_id: Optional batch ID
            campaign_id: Optional campaign ID
            
        Returns:
            Dict: Result of message processing
            
        Raises:
            Exception: Any error during processing
        """
        # Build metadata
        metadata = {}
        if batch_id:
            metadata["batch_id"] = batch_id
        if campaign_id:
            metadata["campaign_id"] = campaign_id
            
        # Set priority based on campaign
        # Campaigns get slightly higher priority but still below urgent messages
        priority = 50 if campaign_id else 0
            
        # Send message
        return await self.send_message(
            phone_number=message.phone_number,
            message_text=message.message,
            user_id=user_id,
            scheduled_at=message.scheduled_at,
            custom_id=message.custom_id,
            metadata=metadata,
            campaign_id=campaign_id,
            priority=priority
        )
    
    async def send_messages_bulk(
        self,
        *,
        messages: List[Dict[str, Any]],
        user_id: str,
        campaign_id: Optional[str] = None,
        batch_id: Optional[str] = None,
        chunk_size: int = 50
    ) -> List[Dict[str, Any]]:
        """
        Send multiple messages efficiently in bulk.
        
        Args:
            messages: List of message dictionaries with recipient and content
            user_id: User ID
            campaign_id: Optional campaign ID
            batch_id: Optional batch ID
            chunk_size: Number of messages to process in each chunk
            
        Returns:
            List[Dict]: List of results for each message
        """
        results = []
        
        # Process in chunks
        for i in range(0, len(messages), chunk_size):
            chunk = messages[i:i+chunk_size]
            chunk_results = await self._process_message_chunk(
                messages=chunk,
                user_id=user_id,
                campaign_id=campaign_id,
                batch_id=batch_id
            )
            results.extend(chunk_results)
            
            # Small delay between chunks to prevent overloading
            if i + chunk_size < len(messages):
                await asyncio.sleep(1)
        
        return results
    
    async def _process_message_chunk(
        self, 
        messages: List[Dict[str, Any]],
        user_id: str,
        campaign_id: Optional[str] = None,
        batch_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Process a chunk of messages concurrently.
        
        Args:
            messages: List of message dictionaries to process
            user_id: User ID
            campaign_id: Optional campaign ID
            batch_id: Optional batch ID
            
        Returns:
            List[Dict]: Results for each message
        """
        # Create tasks for all messages
        tasks = []
        created_messages = []
        
        # Use a single session for all initial creation
        async with get_repository_context(MessageRepository) as repo:
            for msg in messages:
                try:
                    # Create database entries first to get IDs
                    db_message = await repo.create_message(
                        phone_number=msg["phone_number"],
                        message_text=msg["message_text"],
                        user_id=user_id,
                        custom_id=msg.get("custom_id"),
                        scheduled_at=msg.get("scheduled_at"),
                        metadata=msg.get("metadata", {}),
                        campaign_id=campaign_id
                    )
                    
                    created_messages.append(db_message)
                    
                except Exception as e:
                    logger.error(f"Error creating message: {e}")
        
        # Process messages with separate tasks
        for db_message in created_messages:
            # Skip if scheduled for the future
            if db_message.scheduled_at and db_message.scheduled_at > datetime.now(timezone.utc):
                tasks.append(asyncio.create_task(
                    asyncio.sleep(0)  # Dummy task for scheduled messages
                ))
                continue
                
            # Create task to send via gateway
            task = asyncio.create_task(
                self._send_message_with_error_handling(
                    db_message=db_message,
                    phone_number=db_message.phone_number,
                    message_text=db_message.message,
                    priority=0,  # Default priority
                    ttl=None,
                    sim_number=None,
                    is_encrypted=False
                )
            )
            tasks.append(task)
        
        # Wait for all tasks to complete
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return [r for r in results if not isinstance(r, Exception)]
        
        return []
    
    async def _send_message_with_error_handling(
        self,
        *,
        db_message: Any,
        phone_number: str,
        message_text: str,
        priority: int = 0,
        ttl: Optional[int] = None,
        sim_number: Optional[int] = None,
        is_encrypted: bool = False
    ) -> Dict[str, Any]:
        """
        Send message with error handling and status updates.
        
        Args:
            db_message: Database message object
            phone_number: Recipient phone number
            message_text: Message content
            priority: Message priority
            ttl: Time-to-live in seconds
            sim_number: SIM card to use
            is_encrypted: Whether message is encrypted
            
        Returns:
            Dict: Result of message sending
        """
        try:
            # Send to gateway
            result = await self._send_to_gateway(
                phone_number=phone_number,
                message_text=message_text,
                custom_id=db_message.custom_id,
                priority=priority,
                ttl=ttl,
                sim_number=sim_number,
                is_encrypted=is_encrypted
            )
            
            # Update status with a fresh session
            async with get_repository_context(MessageRepository) as repo:
                # Update message status
                await repo.update_message_status(
                    message_id=db_message.id,
                    status=result.get("status", MessageStatus.PENDING),
                    event_type="gateway_response",
                    gateway_message_id=result.get("gateway_message_id"),
                    data=result
                )
            
            return result
            
        except Exception as e:
            # Update status to failed with a fresh session
            try:
                async with get_repository_context(MessageRepository) as repo:
                    await repo.update_message_status(
                        message_id=db_message.id,
                        status=MessageStatus.FAILED,
                        event_type="send_error",
                        reason=str(e),
                        data={"error": str(e)}
                    )
            except Exception as update_error:
                # If even the error update fails, just log it
                logger.error(f"Failed to update error status for message {db_message.id}: {update_error}")
            
            # Log and re-raise
            logger.error(f"Error sending message {db_message.id}: {e}")
            raise
    
    async def schedule_batch_from_numbers(
        self,
        *,
        phone_numbers: List[str],
        message_text: str,
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        campaign_id: Optional[str] = None
    ) -> str:
        """
        Schedule a batch of messages from a list of phone numbers.
        
        Args:
            phone_numbers: List of phone numbers
            message_text: Message content
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            campaign_id: Optional campaign ID
            
        Returns:
            str: Batch ID
            
        Raises:
            ValidationError: If any phone number is invalid
        """
        if not phone_numbers:
            raise ValidationError(message="No phone numbers provided")
        
        # Create messages
        messages = []
        for phone in phone_numbers:
            # Basic validation
            is_valid, formatted_number, error, _ = validate_phone(phone)
            if is_valid:
                messages.append(
                    MessageCreate(
                        phone_number=formatted_number,
                        message=message_text,
                        scheduled_at=scheduled_at,
                        custom_id=str(uuid.uuid4())
                    )
                )
        
        if not messages:
            raise ValidationError(message="No valid phone numbers found")
        
        # Create and process batch
        result = await self.send_batch(
            messages=messages,
            user_id=user_id,
            campaign_id=campaign_id,
            options=BatchOptions(
                delay_between_messages=settings.DELAY_BETWEEN_SMS,
                fail_on_first_error=False,
                retry_failed=True
            )
        )
        
        return result["batch_id"]
    
    async def get_message(self, message_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """
        Get message details.
        
        Args:
            message_id: Message ID or custom ID
            user_id: User ID for authorization
            
        Returns:
            Dict: Message details or None if not found
        """
        async with get_repository_context(MessageRepository) as repo:
            # Try to get by ID first
            message = await repo.get_by_id(message_id)
            
            # If not found, try custom ID
            if not message:
                message = await repo.get_by_custom_id(message_id)
                
            # If not found, try gateway ID
            if not message:
                message = await repo.get_by_gateway_id(message_id)
            
            # Check authorization
            if message and str(message.user_id) != str(user_id):
                return None
            
            return message.dict() if message else None
    
    async def list_messages(
        self,
        *,
        filters: Dict[str, Any],
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        List messages with filtering and pagination.
        
        Args:
            filters: Filter criteria
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Dict], int]: List of messages and total count
        """
        # Extract user_id from filters
        user_id = filters.pop("user_id", None)
        if not user_id:
            return [], 0
        
        async with get_repository_context(MessageRepository) as repo:
            # Get messages
            messages, total = await repo.list_messages_for_user(
                user_id=user_id,
                status=filters.get("status"),
                phone_number=filters.get("phone_number"),
                from_date=filters.get("from_date"),
                to_date=filters.get("to_date"),
                campaign_id=filters.get("campaign_id"),  # Support filtering by campaign
                skip=skip,
                limit=limit
            )
            
            # Convert to dict
            message_dicts = [message.dict() for message in messages]
            
            return message_dicts, total
    
    async def update_message_status(
        self,
        *,
        message_id: str,
        status: str,
        reason: Optional[str] = None,
        user_id: str
    ) -> Optional[Dict[str, Any]]:
        """
        Update message status.
        
        Args:
            message_id: Message ID
            status: New status
            reason: Reason for status change
            user_id: User ID for authorization
            
        Returns:
            Dict: Updated message or None if not found
        """
        async with get_repository_context(MessageRepository) as repo:
            # Get message
            message = await repo.get_by_id(message_id)
            if not message:
                return None
            
            # Check authorization
            if str(message.user_id) != str(user_id):
                return None
            
            # Update status
            updated = await repo.update_message_status(
                message_id=message_id,
                status=status,
                event_type="manual_update",
                reason=reason,
                data={"updated_by": user_id}
            )
            
            return updated.dict() if updated else None
    
    async def delete_message(self, message_id: str, user_id: str) -> bool:
        """
        Delete a message.
        
        Args:
            message_id: Message ID
            user_id: User ID for authorization
            
        Returns:
            bool: True if deleted, False otherwise
        """
        async with get_repository_context(MessageRepository) as repo:
            # Get message
            message = await repo.get_by_id(message_id)
            if not message:
                return False
            
            # Check authorization
            if str(message.user_id) != str(user_id):
                return False
            
            # Delete message
            return await repo.delete(id=message_id)
    
    async def get_task_status(self, task_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """
        Get status of a background task (batch).
        
        Args:
            task_id: Task/batch ID
            user_id: User ID for authorization
            
        Returns:
            Dict: Task status or None if not found
        """
        async with get_repository_context(MessageRepository) as repo:
            # Get batch
            batch = await repo.get_by_id(task_id)
            if not batch:
                return None
            
            # Check authorization
            if str(batch.user_id) != str(user_id):
                return None
            
            # Get message stats
            messages, total = await repo.get_messages_for_batch(
                batch_id=task_id,
                limit=5  # Just get the first few for preview
            )
            
            # Convert to dict
            message_previews = [message.dict() for message in messages]
            
            return {
                "id": batch.id,
                "status": batch.status,
                "total": batch.total,
                "processed": batch.processed,
                "successful": batch.successful,
                "failed": batch.failed,
                "created_at": batch.created_at,
                "completed_at": batch.completed_at,
                "message_previews": message_previews
            }

    async def _send_to_gateway(
        self,
        *,
        phone_number: str,
        message_text: str,
        custom_id: str,
        priority: int = 0,
        ttl: Optional[int] = None,
        sim_number: Optional[int] = None,
        is_encrypted: bool = False
    ) -> Dict[str, Any]:
        """
        Send message to SMS gateway with high-volume optimizations.
        """
        # High-volume systems should avoid synchronized rate limiting
        # Instead, we'll use semaphores for concurrency control
        
        # Check if gateway client is available or mock mode enabled
        if not SMS_GATEWAY_AVAILABLE or getattr(settings, "SMS_GATEWAY_MOCK", False):
            # Simulate sending for development/testing
            logger.info(f"[MOCK] Sending SMS to {phone_number}: {message_text[:30]}...")
            # Fast simulation for high volume
            await asyncio.sleep(0.05)
            
            # Return simulated response
            return {
                "status": MessageStatus.SENT,
                "gateway_message_id": f"mock_{uuid.uuid4()}",
                "phone_number": phone_number,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        
        # Use semaphore to limit concurrent requests to gateway
        async with self._semaphore:
            try:
                # Create client with optimized connection settings
                connect_timeout = 5.0  # shorter timeout for high volume
                timeout = 10.0
                
                async with client.AsyncAPIClient(
                    login=settings.SMS_GATEWAY_LOGIN,
                    password=settings.SMS_GATEWAY_PASSWORD,
                    base_url=settings.SMS_GATEWAY_URL,
                    timeout=timeout,
                    connect_timeout=connect_timeout
                ) as sms_client:
                    # Build message
                    message_params = {
                        "id": custom_id,
                        "message": message_text,
                        "phone_numbers": [phone_number],
                        "with_delivery_report": True,
                    }
                    
                    # Add optional parameters if provided
                    if ttl is not None:
                        message_params["ttl"] = ttl
                    if sim_number is not None:
                        message_params["sim_number"] = sim_number
                    if is_encrypted:
                        message_params["is_encrypted"] = True
                    
                    # Create message object
                    message = domain.Message(**message_params)
                    
                    # Send with timing metrics for performance monitoring
                    start_time = time.time()
                    response = await sms_client.send(message)
                    elapsed = time.time() - start_time
                    
                    # Log timing for performance monitoring
                    if elapsed > 1.0:
                        logger.warning(f"Slow gateway response: {elapsed:.2f}s for message to {phone_number}")
                    
                    # Check for errors in recipients
                    recipient_state = response.recipients[0] if response.recipients else None
                    if recipient_state and recipient_state.error:
                        raise SMSGatewayError(message=recipient_state.error)
                    
                    # Extract status
                    status = str(response.state.value).lower() if hasattr(response, 'state') else MessageStatus.PENDING
                    gateway_id = getattr(response, 'id', None)
                    
                    return {
                        "status": status,
                        "gateway_message_id": gateway_id,
                        "phone_number": phone_number,
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }
            except HTTPStatusError as e:
                if e.response.status_code == 401:
                    logger.error("❌ Invalid SMS gateway credentials (401 Unauthorized)")
                    raise SMSAuthError()
                
                # High volume optimization: determine if error is retryable
                retryable_status_codes = [429, 503, 504]
                if e.response.status_code in retryable_status_codes:
                    retry_after = int(e.response.headers.get('Retry-After', "30"))
                    raise RetryableError(
                        message=f"Gateway rate limiting or temporary unavailability (status={e.response.status_code})",
                        retry_after=retry_after
                    )
                    
                raise SMSGatewayError(message=f"SMS gateway error: {str(e)}")

            except Exception as e:
                logger.error(f"Unexpected gateway exception: {type(e).__name__}: {str(e)}")

                # Improved retryable error detection for high-volume systems
                retryable_exceptions = ["ConnectionError", "Timeout", "CancelledError", "ServiceUnavailable"]
                if any(ex_type in str(type(e)) for ex_type in retryable_exceptions):
                    raise RetryableError(
                        message=f"Temporary SMS gateway issue: {str(e)}",
                        retry_after=30
                    )

                raise SMSGatewayError(message=f"SMS gateway error: {str(e)}")

    async def _enforce_rate_limit(self) -> None:
        """
        Enforce rate limiting for SMS sending.
        
        Adds dynamic delay based on settings.DELAY_BETWEEN_SMS.
        """
        current_time = asyncio.get_event_loop().time()
        elapsed = current_time - self._last_send_time
        remaining_delay = max(0, settings.DELAY_BETWEEN_SMS - elapsed)
        
        if remaining_delay > 0:
            await asyncio.sleep(remaining_delay)
        
        self._last_send_time = asyncio.get_event_loop().time()

    async def send_with_template(
        self,
        *,
        template_id: str,
        phone_number: str,
        variables: Dict[str, str],
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        custom_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Send a message using a template.
        
        Args:
            template_id: Template ID
            phone_number: Recipient phone number
            variables: Dictionary of variable values
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            custom_id: Optional custom ID for tracking
            
        Returns:
            Dict: Message details with status
            
        Raises:
            ValidationError: If phone number is invalid or template is not found
            SMSGatewayError: If there's an error sending the message
        """
        # Use context manager for template repository
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            if not template:
                raise ValidationError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != user_id:
                raise ValidationError(message="Not authorized to use this template")
            
            # Apply template
            message_text = await template_repo.apply_template(
                template_id=template_id,
                variables=variables
            )
            
            # Check for missing variables
            import re
            missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", message_text)
            if missing_vars:
                raise ValidationError(
                    message="Missing template variables", 
                    details={"missing_variables": missing_vars}
                )
        
        # Send message
        metadata = {
            "template_id": template_id,
            "template_variables": variables
        }
        
        return await self.send_message(
            phone_number=phone_number,
            message_text=message_text,
            user_id=user_id,
            scheduled_at=scheduled_at,
            custom_id=custom_id,
            metadata=metadata
        )

    async def send_batch_with_template(
        self,
        *,
        template_id: str,
        recipients: List[Dict[str, Any]],
        user_id: str,
        scheduled_at: Optional[datetime] = None,
        options: Optional[BatchOptions] = None
    ) -> Dict[str, Any]:
        """
        Send a batch of messages using a template.
        
        Args:
            template_id: Template ID
            recipients: List of recipients with their variables
                    Each recipient should have 'phone_number' and 'variables' keys
            user_id: User ID
            scheduled_at: Optional scheduled delivery time
            options: Optional batch processing options
            
        Returns:
            Dict: Batch details with status
            
        Raises:
            ValidationError: If template is not found or recipients format is invalid
            SMSGatewayError: If there's an error sending the messages
        """
        # Use context manager for template repository
        async with get_repository_context(TemplateRepository) as template_repo:
            # Get template
            template = await template_repo.get_by_id(template_id)
            if not template:
                raise ValidationError(message=f"Template {template_id} not found")
            
            # Check authorization
            if template.user_id != user_id:
                raise ValidationError(message="Not authorized to use this template")
            
            # Validate recipients format
            for idx, recipient in enumerate(recipients):
                if "phone_number" not in recipient:
                    raise ValidationError(message=f"Recipient at index {idx} is missing 'phone_number'")
                if "variables" not in recipient:
                    raise ValidationError(message=f"Recipient at index {idx} is missing 'variables'")
            
            # Create messages for each recipient
            messages = []
            for recipient in recipients:
                # Apply template for each recipient
                message_text = await template_repo.apply_template(
                    template_id=template_id,
                    variables=recipient["variables"]
                )
                
                # Check for missing variables
                import re
                missing_vars = re.findall(r"{{([a-zA-Z0-9_]+)}}", message_text)
                if missing_vars:
                    # Skip this recipient but continue with others
                    continue
                
                # Create message
                messages.append(
                    MessageCreate(
                        phone_number=recipient["phone_number"],
                        message=message_text,
                        scheduled_at=scheduled_at,
                        custom_id=recipient.get("custom_id")
                    )
                )
            
            if not messages:
                raise ValidationError(message="No valid recipients found after applying templates")
        
        # Create batch metadata
        batch_metadata = {
            "template_id": template_id,
            "recipients_count": len(recipients),
            "messages_count": len(messages)
        }
        
        # Use standard batch sending
        batch_result = await self.send_batch(
            messages=messages,
            user_id=user_id,
            options=options
        )
        
        # Add template info to result
        batch_result["template_id"] = template_id
        batch_result["template_name"] = template.name
        
        return batch_result

# Dependency injection function
async def get_sms_sender() -> SMSSender:
    """
    Dependency injection provider for SMSSender.
    
    Returns:
        SMSSender: An instance of the SMS sending service with event bus.
    """ 
    from app.services.event_bus.bus import get_event_bus
    event_bus = get_event_bus()    
    return SMSSender(event_bus)
</file>

<file path="app/schemas/message.py">
"""
Pydantic schemas for message-related API operations.
"""
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
from enum import Enum
from pydantic import BaseModel, Field, validator
from app.schemas.campaign import CampaignResponse


class MessageStatus(str, Enum):
    """Possible message statuses."""
    PENDING = "pending"
    PROCESSED = "processed"
    SENT = "sent"
    DELIVERED = "delivered"
    FAILED = "failed"
    SCHEDULED = "scheduled"
    CANCELLED = "cancelled"


class MessageCreate(BaseModel):
    """Schema for creating a new message."""
    phone_number: str = Field(..., description="Recipient phone number in E.164 format")
    message: str = Field(..., description="Message content")
    scheduled_at: Optional[datetime] = Field(None, description="Schedule message for future delivery")
    custom_id: Optional[str] = Field(None, description="Custom ID for tracking")
    variables: Optional[Dict[str, Any]] = Field(None, description="Variables for message personalization")

    
    @validator("phone_number")
    def validate_phone_number(cls, v):
        """Validate phone number format."""
        # Basic validation - will be handled more thoroughly in the service
        if not v or not (v.startswith("+") and len(v) >= 8):
            raise ValueError("Phone number must be in E.164 format (e.g. +1234567890)")
        return v
    
    @validator("message")
    def validate_message(cls, v):
        """Validate message content."""
        if not v or len(v.strip()) == 0:
            raise ValueError("Message cannot be empty")
        if len(v) > 1600:  # Allow for multi-part SMS
            raise ValueError("Message exceeds maximum length of 1600 characters")
        return v


class MessageResponse(BaseModel):
    """Schema for message response."""
    id: str = Field(..., description="Message ID")
    custom_id: Optional[str] = Field(None, description="Custom ID if provided")
    phone_number: str = Field(..., description="Recipient phone number")
    message: str = Field(..., description="Message content")
    status: MessageStatus = Field(..., description="Current message status")
    created_at: datetime = Field(..., description="Creation timestamp")
    updated_at: datetime = Field(..., description="Last update timestamp")
    scheduled_at: Optional[datetime] = Field(None, description="Scheduled delivery time")
    sent_at: Optional[datetime] = Field(None, description="Time when message was sent")
    delivered_at: Optional[datetime] = Field(None, description="Time when message was delivered")
    failed_at: Optional[datetime] = Field(None, description="Time when message failed")
    reason: Optional[str] = Field(None, description="Failure reason if applicable")
    gateway_message_id: Optional[str] = Field(None, description="ID from SMS gateway")
    user_id: str = Field(..., description="User who sent the message")
    meta_data: Optional[Dict[str, Any]] = Field(default={}, description="Additional metadata")
    
    # Personalization variables  
    variables: Optional[Dict[str, Any]] = Field(None, description="Variables used for message personalization")
    
    campaign: Optional[CampaignResponse] = Field(None, description="Campaign information if message belongs to a campaign")
    parts_count: Optional[int] = Field(None, description="Number of SMS parts")


    
    class Config:
        """Pydantic config."""
        from_attributes = True


class MessageStatusUpdate(BaseModel):
    """Schema for updating message status."""
    status: MessageStatus = Field(..., description="New message status")
    reason: Optional[str] = Field(None, description="Reason for status change (required for FAILED)")
    
    @validator("reason")
    def validate_reason(cls, v, values):
        """Validate reason field."""
        if values.get("status") == MessageStatus.FAILED and not v:
            raise ValueError("Reason is required when status is FAILED")
        return v


class BatchOptions(BaseModel):
    """Options for batch processing."""
    delay_between_messages: Optional[float] = Field(0.3, description="Delay between messages in seconds")
    fail_on_first_error: Optional[bool] = Field(False, description="Stop processing on first error")
    retry_failed: Optional[bool] = Field(True, description="Automatically retry failed messages")


class BatchMessageRequest(BaseModel):
    """Schema for batch message request."""
    messages: List[MessageCreate] = Field(..., description="List of messages to send")
    options: Optional[BatchOptions] = Field(default=None, description="Batch processing options")
    
    @validator("messages")
    def validate_messages(cls, v):
        """Validate message list."""
        if not v:
            raise ValueError("Message list cannot be empty")
        if len(v) > 1000:
            raise ValueError("Maximum batch size is 1000 messages")
        return v


class BatchMessageResponse(BaseModel):
    """Schema for batch message response."""
    batch_id: str = Field(..., description="Batch ID for tracking")
    total: int = Field(..., description="Total number of messages in batch")
    processed: int = Field(..., description="Number of messages processed")
    successful: int = Field(..., description="Number of successful messages")
    failed: int = Field(..., description="Number of failed messages")
    status: str = Field(..., description="Overall batch status")
    created_at: datetime = Field(..., description="Batch creation timestamp")
    messages: Optional[List[MessageResponse]] = Field(None, description="List of message responses")

class CampaignBulkDeleteRequest(BaseModel):
    """Schema for campaign-scoped bulk delete request."""
    status: Optional[MessageStatus] = Field(None, description="Filter by message status (e.g., 'failed', 'sent')")
    from_date: Optional[datetime] = Field(None, description="Delete messages from this date onwards (ISO format)")
    to_date: Optional[datetime] = Field(None, description="Delete messages up to this date (ISO format)")
    limit: int = Field(default=1000, le=10000, description="Maximum number of messages to delete (max 10,000)")
    confirm_delete: bool = Field(default=True, description="Confirmation flag - must be true to proceed")
    force_delete: bool = Field(default=False, description="Force delete messages with delivery events")
    confirmation_token: Optional[str] = Field(None, description="Required when force_delete=True - must be 'CONFIRM'")
    batch_size: int = Field(default=1000, le=5000, description="Process deletions in batches for server stability")
    
    @validator("limit")
    def validate_limit(cls, v):
        """Validate deletion limit for safety."""
        if v <= 0:
            raise ValueError("Limit must be greater than 0")
        if v > 10000:
            raise ValueError("Maximum limit is 10,000 messages per operation")
        return v
    
    @validator("batch_size")
    def validate_batch_size(cls, v):
        """Validate batch size for server stability."""
        if v <= 0:
            raise ValueError("Batch size must be greater than 0")
        if v > 5000:
            raise ValueError("Maximum batch size is 5,000 for server stability")
        return v
    
    @validator("confirmation_token")
    def validate_confirmation_token(cls, v, values):
        """Validate confirmation token when force delete is enabled."""
        if values.get("force_delete") and v != "CONFIRM":
            raise ValueError("confirmation_token must be 'CONFIRM' when force_delete is true")
        return v
    
    @validator("confirm_delete")
    def validate_confirmation(cls, v):
        """Ensure user confirms the bulk deletion."""
        if not v:
            raise ValueError("confirm_delete must be true to proceed with bulk deletion")
        return v
    
    @validator("from_date", "to_date")
    def validate_dates(cls, v):
        """Validate date format and timezone."""
        if v is not None:
            # Ensure datetime is timezone-aware
            if v.tzinfo is None:
                raise ValueError("Date must be timezone-aware (include timezone information)")
        return v

class GlobalBulkDeleteRequest(BaseModel):
    """Schema for global bulk delete request (by message IDs)."""
    message_ids: List[str] = Field(..., description="List of message IDs to delete")
    campaign_id: Optional[str] = Field(None, description="Optional campaign context for validation")
    confirm_delete: bool = Field(default=True, description="Confirmation flag - must be true to proceed")
    force_delete: bool = Field(default=False, description="Force delete messages with delivery events")
    confirmation_token: Optional[str] = Field(None, description="Required when force_delete=True - must be 'CONFIRM'")
    
    @validator("message_ids")
    def validate_message_ids(cls, v):
        """Validate message ID list."""
        if not v:
            raise ValueError("Message IDs list cannot be empty")
        if len(v) > 1000:
            raise ValueError("Maximum 1,000 message IDs per global bulk operation")
        
        # Check for duplicates
        if len(v) != len(set(v)):
            raise ValueError("Duplicate message IDs found in request")
        
        return v
    
    @validator("confirmation_token")
    def validate_confirmation_token(cls, v, values):
        """Validate confirmation token when force delete is enabled."""
        if values.get("force_delete") and v != "CONFIRM":
            raise ValueError("confirmation_token must be 'CONFIRM' when force_delete is true")
        return v
    
    @validator("confirm_delete")
    def validate_confirmation(cls, v):
        """Ensure user confirms the bulk deletion."""
        if not v:
            raise ValueError("confirm_delete must be true to proceed with bulk deletion")
        return v


class BulkDeleteResponse(BaseModel):
    """Schema for bulk delete operation response."""
    deleted_count: int = Field(..., description="Number of messages successfully deleted")
    campaign_id: Optional[str] = Field(None, description="Campaign ID if campaign-scoped operation")
    failed_count: int = Field(default=0, description="Number of messages that failed to delete")
    errors: List[str] = Field(default=[], description="List of error messages if any failures occurred")
    operation_type: str = Field(..., description="Type of bulk operation ('campaign' or 'global')")
    filters_applied: Dict[str, Any] = Field(default={}, description="Filters that were applied during deletion")
    execution_time_ms: Optional[int] = Field(None, description="Operation execution time in milliseconds")
    requires_confirmation: bool = Field(default=False, description="Whether force delete is needed due to existing events")
    events_count: Optional[int] = Field(None, description="Number of delivery events that would be deleted")
    events_deleted: int = Field(default=0, description="Number of delivery events actually deleted")
    safety_warnings: List[str] = Field(default=[], description="Safety warnings about delivery event deletion")
    batch_info: Optional[Dict[str, Any]] = Field(None, description="Batch processing information for large operations")
    
    class Config:
        """Pydantic config."""
        from_attributes = True
        schema_extra = {
            "example": {
                "deleted_count": 2847,
                "campaign_id": "camp_abc123",
                "failed_count": 0,
                "errors": [],
                "operation_type": "campaign",
                "filters_applied": {
                    "status": "failed",
                    "from_date": "2024-01-01T00:00:00Z"
                },
                "execution_time_ms": 3421,
                "requires_confirmation": False,
                "events_count": None,
                "events_deleted": 0,
                "safety_warnings": [],
                "batch_info": {
                    "batches_processed": 3,
                    "batch_size": 1000
                }
            }
        }


class BulkDeleteProgress(BaseModel):
    """Schema for tracking bulk delete operation progress (future use)."""
    operation_id: str = Field(..., description="Unique operation identifier")
    status: str = Field(..., description="Operation status ('pending', 'processing', 'completed', 'failed')")
    progress_percentage: int = Field(..., description="Progress percentage (0-100)")
    messages_processed: int = Field(..., description="Number of messages processed so far")
    total_messages: int = Field(..., description="Total number of messages to process")
    estimated_completion: Optional[datetime] = Field(None, description="Estimated completion time")
    errors: List[str] = Field(default=[], description="Any errors encountered during processing")
    
    class Config:
        """Pydantic config."""
        from_attributes = True



class MessageSendAcceptedResponse(BaseModel):
    """Schema for message send accepted response (202)."""
    status: str = Field(..., description="Request status", example="accepted")
    message: str = Field(..., description="Human-readable status message")
    task_id: str = Field(..., description="Task ID for tracking progress")
    phone_number: str = Field(..., description="Formatted recipient phone number")
    
    class Config:
        """Pydantic config."""
        schema_extra = {
            "example": {
                "status": "accepted",
                "message": "Message queued for sending",
                "task_id": "msg-abc123def456",
                "phone_number": "+1234567890"
            }
        }


class BatchSendAcceptedResponse(BaseModel):
    """Schema for batch send accepted response (202)."""
    status: str = Field(..., description="Request status", example="accepted")
    message: str = Field(..., description="Human-readable status message")
    batch_id: str = Field(..., description="Batch ID for tracking progress")
    total: int = Field(..., description="Total number of messages in batch")
    processed: int = Field(0, description="Number of messages processed (initially 0)")
    successful: int = Field(0, description="Number of successful messages (initially 0)")
    failed: int = Field(0, description="Number of failed messages (initially 0)")
    
    class Config:
        """Pydantic config."""
        schema_extra = {
            "example": {
                "status": "accepted",
                "message": "Batch of 5 messages queued for processing",
                "batch_id": "batch-abc123def456",
                "total": 5,
                "processed": 0,
                "successful": 0,
                "failed": 0
            }
        }
</file>

<file path="app/db/repositories/messages.py">
"""
Message repository for database operations related to SMS messages.
"""
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any, Tuple
from uuid import uuid4

from sqlalchemy import select, update, delete, and_, or_, desc, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import Session, joinedload, selectinload
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.exc import IntegrityError

import logging
from app.utils.ids import generate_prefixed_id, IDPrefix
from app.models.campaign import Campaign
from app.db.repositories.base import BaseRepository
from app.models.message import Message, MessageEvent, MessageBatch, MessageTemplate
from app.schemas.message import MessageCreate, MessageStatus

logger = logging.getLogger("inboxerr.db")

class MessageRepository(BaseRepository[Message, MessageCreate, Dict[str, Any]]):
    """Message repository for database operations."""
    
    def __init__(self, session: AsyncSession):
        """Initialize with session and Message model."""
        super().__init__(session=session, model=Message)

    async def create_message(
        self,
        *,
        phone_number: str,
        message_text: str,
        user_id: str,
        custom_id: Optional[str] = None,
        scheduled_at: Optional[datetime] = None,
        metadata: Optional[Dict[str, Any]] = None,
        batch_id: Optional[str] = None,
        campaign_id: Optional[str] = None
    ) -> Message:
        """
        Create a new message with related objects in a single transaction.
        """
        # Set initial status based on scheduling
        initial_status = MessageStatus.SCHEDULED if scheduled_at else MessageStatus.PENDING
        
        # Calculate SMS parts
        parts_count = (len(message_text) + 159) // 160  # 160 chars per SMS part
        
        # Generate IDs upfront
        message_id = generate_prefixed_id(IDPrefix.MESSAGE)
        event_id = generate_prefixed_id(IDPrefix.EVENT)
        
        # Create message instance
        message = Message(
            id=message_id,  # Pre-assign ID
            custom_id=custom_id or str(uuid4()),
            phone_number=phone_number,
            message=message_text,
            status=initial_status,
            scheduled_at=scheduled_at,
            user_id=user_id,
            meta_data=metadata or {},
            parts_count=parts_count,
            batch_id=batch_id,
            campaign_id=campaign_id
        )
        
        # Create event instance with pre-assigned message_id
        event = MessageEvent(
            id=event_id,
            message_id=message_id,
            event_type="created",
            status=initial_status,
            data={
                "phone_number": phone_number,
                "scheduled_at": scheduled_at.isoformat() if scheduled_at else None,
                "campaign_id": campaign_id
            }
        )
        
        # Begin transaction using savepoint if a transaction is already active
        async with self.session.begin_nested():
            # Add both objects to session
            self.session.add(message)
            self.session.add(event)
            
            # Handle campaign update if needed
            if campaign_id:
                try:
                    # Execute campaign update with direct SQL for better performance in high volume
                    await self.session.execute(
                        update(Campaign)
                        .where(Campaign.id == campaign_id)
                        .values(total_messages=Campaign.total_messages + 1)
                    )
                except Exception as e:
                    # Log but don't fail the message creation if campaign update fails
                    logger.error(f"Error updating campaign {campaign_id} message count: {e}")
        
        
        return message

    async def update_message_status(
        self,
        *,
        message_id: str,
        status: str,
        event_type: str,
        reason: Optional[str] = None,
        gateway_message_id: Optional[str] = None,
        data: Optional[Dict[str, Any]] = None
    ) -> Optional[Message]:
        """
        Update message status with improved concurrency handling.
        
        Args:
            message_id: Message ID
            status: New status
            event_type: Event type triggering this update
            reason: Optional reason for status change
            gateway_message_id: Optional gateway message ID
            data: Optional additional data
            
        Returns:
            Message: Updated message or None
        """
        # Get the message first to check if it exists
        message = await self.get_by_id(message_id)
        if not message:
            return None
        
        # Update status-specific timestamp
        now = datetime.now(timezone.utc)
        update_data = {
            "status": status,
            "updated_at": now,
        }
        
        if status == MessageStatus.SENT:
            update_data["sent_at"] = now
        elif status == MessageStatus.DELIVERED:
            update_data["delivered_at"] = now
        elif status == MessageStatus.FAILED:
            update_data["failed_at"] = now
            update_data["reason"] = reason
        
        # Set gateway message ID if provided
        if gateway_message_id:
            update_data["gateway_message_id"] = gateway_message_id
        
        # Set a specific savepoint for this operation
        async with self.session.begin_nested() as nested:
            try:
                # Update the message
                await self.session.execute(
                    update(Message)
                    .where(Message.id == message_id)
                    .values(**update_data)
                )
                
                # Create event for status change with unique ID
                event_id = generate_prefixed_id(IDPrefix.EVENT)  # Generate new ID for each event
                event = MessageEvent(
                    id=event_id,
                    message_id=message_id,
                    event_type=event_type,
                    status=status,
                    data=data or {}
                )
                
                # Add event to session
                self.session.add(event)
                
                # Commit the nested transaction
                await nested.commit()
            except Exception as e:
                # Transaction will automatically roll back on exception
                logger.error(f"Error updating message status: {e}")
                return None
        
        
        return message

    async def create_batch(
        self,
        *,
        user_id: str,
        name: str,
        total: int,
        batch_id: Optional[str] = None,
    ) -> MessageBatch:
        """
        Create a new message batch.
        
        Args:
            user_id: User ID
            name: Batch name
            total: Total number of messages
            
        Returns:
            MessageBatch: Created batch
        """
        batch_id =  batch_id or generate_prefixed_id(IDPrefix.BATCH)
        batch = MessageBatch(
            id=batch_id,
            name=name,
            total=total,
            processed=0,
            successful=0,
            failed=0,
            status=MessageStatus.PENDING,
            user_id=user_id
        )
        
        async with self.session.begin():
            self.session.add(batch)
        
        
        return batch

    async def update_batch_progress(
        self,
        *,
        batch_id: str,
        increment_processed: int = 0,
        increment_successful: int = 0,
        increment_failed: int = 0,
        status: Optional[str] = None
    ) -> Optional[MessageBatch]:
        """
        Update batch progress with proper transaction handling.
        
        Args:
            batch_id: Batch ID
            increment_processed: Increment processed count
            increment_successful: Increment successful count
            increment_failed: Increment failed count
            status: Optional new status
            
        Returns:
            MessageBatch: Updated batch or None
        """
        # Use a proper transaction
        async with self.session.begin():
            # Get batch with FOR UPDATE to prevent race conditions
            query = select(MessageBatch).where(MessageBatch.id == batch_id)
            result = await self.session.execute(query.with_for_update())
            batch = result.scalar_one_or_none()
            
            if not batch:
                return None
            
            # Update counts
            batch.processed += increment_processed
            batch.successful += increment_successful
            batch.failed += increment_failed
            
            # Update status if provided
            if status:
                batch.status = status
                
            # If all messages processed, update status and completion time
            if batch.processed >= batch.total:
                batch.status = MessageStatus.PROCESSED if batch.failed == 0 else "partial"
                batch.completed_at = datetime.now(timezone.utc)
            
            # Add batch to session
            self.session.add(batch)
        
        # Get updated batch
        query = select(MessageBatch).where(MessageBatch.id == batch_id)
        result = await self.session.execute(query)
        updated_batch = result.scalar_one_or_none()
        
        return updated_batch

    async def update_batch_progress_safe(
        self,
        *,
        batch_id: str,
        increment_processed: int = 0,
        increment_successful: int = 0,
        increment_failed: int = 0,
        status: Optional[str] = None
    ) -> Optional[MessageBatch]:
        """
        Update batch progress with proper transaction handling.
        
        Args:
            batch_id: Batch ID
            increment_processed: Increment processed count
            increment_successful: Increment successful count
            increment_failed: Increment failed count
            status: Optional new status
            
        Returns:
            MessageBatch: Updated batch or None
        """
        # Get a completely fresh session for this operation
        from app.db.session import async_session_factory
        
        async with async_session_factory() as fresh_session:
            try:
                async with fresh_session.begin():
                    # Get batch with SELECT FOR UPDATE to prevent race conditions
                    query = select(MessageBatch).where(MessageBatch.id == batch_id)
                    result = await fresh_session.execute(query.with_for_update())
                    batch = result.scalar_one_or_none()
                    
                    if not batch:
                        return None
                    
                    # Update counts
                    batch.processed += increment_processed
                    batch.successful += increment_successful
                    batch.failed += increment_failed
                    
                    # Update status if provided
                    if status:
                        batch.status = status
                        
                    # If all messages processed, update status and completion time
                    if batch.processed >= batch.total:
                        batch.status = MessageStatus.PROCESSED if batch.failed == 0 else "partial"
                        batch.completed_at = datetime.now(timezone.utc)
                    
                    # Add the updated batch to the session
                    fresh_session.add(batch)
                    
                    # Get updated batch after updates are applied
                    # This is automatically refreshed at transaction commit
                    
                # Now outside the transaction, we can safely refresh
                query = select(MessageBatch).where(MessageBatch.id == batch_id)
                result = await fresh_session.execute(query)
                updated_batch = result.scalar_one_or_none()
                    
                return updated_batch
            
            except Exception as e:
                logger.error(f"Error in update_batch_progress_safe: {e}")
                return None
            finally:
                # Explicitly close session to prevent connection leaks
                await fresh_session.close()

    async def get_by_custom_id(self, custom_id: str) -> Optional[Message]:
        """
        Get message by custom ID.
        
        Args:
            custom_id: Custom ID
            
        Returns:
            Message: Found message or None
        """
        return await self.get_by_attribute("custom_id", custom_id)

    async def get_by_gateway_id(self, gateway_id: str) -> Optional[Message]:
        """
        Get message by gateway ID.
        
        Args:
            gateway_id: Gateway message ID
            
        Returns:
            Message: Found message or None
        """
        return await self.get_by_attribute("gateway_message_id", gateway_id)

    async def list_messages_for_user(
        self,
        *,
        user_id: str,
        status: Optional[str] = None,
        phone_number: Optional[str] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        campaign_id: Optional[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Message], int]:
        """
        List messages for user with filtering.
        
        Args:
            user_id: User ID
            status: Optional status filter
            phone_number: Optional phone number filter
            from_date: Optional from date filter
            to_date: Optional to date filter
            campaign_id: Optional campaign ID filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Message], int]: List of messages and total count
        """
        # Base query
        query = (
            select(Message)
            .options(
                joinedload(Message.campaign),       # Eager load campaign
                selectinload(Message.events)        # Eager load events if needed
            )
            .where(Message.user_id == user_id)
        )
        count_query = select(func.count()).select_from(Message).where(Message.user_id == user_id)
        
        # Apply filters
        if status:
            query = query.where(Message.status == status)
            count_query = count_query.where(Message.status == status)
        
        if phone_number:
            query = query.where(Message.phone_number == phone_number)
            count_query = count_query.where(Message.phone_number == phone_number)

        if campaign_id:
            query = query.where(Message.campaign_id == campaign_id)
            count_query = count_query.where(Message.campaign_id == campaign_id)
        
        if from_date:
            try:
                from_date_obj = datetime.fromisoformat(from_date.replace('Z', '+00:00'))
                query = query.where(Message.created_at >= from_date_obj)
                count_query = count_query.where(Message.created_at >= from_date_obj)
            except ValueError:
                pass
        
        if to_date:
            try:
                to_date_obj = datetime.fromisoformat(to_date.replace('Z', '+00:00'))
                query = query.where(Message.created_at <= to_date_obj)
                count_query = count_query.where(Message.created_at <= to_date_obj)
            except ValueError:
                pass
        
        # Order by created_at desc
        query = query.order_by(desc(Message.created_at))
        
        # Pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        messages = result.scalars().all()
        total = count_result.scalar_one()
        
        return messages, total

    async def get_messages_for_batch(
        self,
        *,
        batch_id: str,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Message], int]:
        """
        Get messages for a batch.
        
        Args:
            batch_id: Batch ID
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Message], int]: List of messages and total count
        """
        # Base query
        query = (
            select(Message)
            .options(joinedload(Message.campaign))
            .where(Message.batch_id == batch_id)
        )
        count_query = select(func.count()).select_from(Message).where(Message.batch_id == batch_id)
        
        # Order by created_at desc
        query = query.order_by(desc(Message.created_at))
        
        # Pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        messages = result.scalars().all()
        total = count_result.scalar_one()
        
        return messages, total
    
    async def get_messages_for_campaign(
        self,
        *,
        campaign_id: str,
        status: Optional[str] = None,
        skip: int = 0,
        limit: int = 20
    ) -> Tuple[List[Message], int]:
        """
        Get messages for a campaign.
        
        Args:
            campaign_id: Campaign ID
            status: Optional status filter
            skip: Number of records to skip
            limit: Maximum number of records to return
            
        Returns:
            Tuple[List[Message], int]: List of messages and total count
        """
        # Base query
        query = (
            select(Message)
            .options(joinedload(Message.campaign))  # This fixes the DetachedInstanceError
            .where(Message.campaign_id == campaign_id)
        )
        count_query = select(func.count()).select_from(Message).where(Message.campaign_id == campaign_id)
        
        # Apply status filter
        if status:
            query = query.where(Message.status == status)
            count_query = count_query.where(Message.status == status)
        
        # Order by created_at desc
        query = query.order_by(desc(Message.created_at))
        
        # Apply pagination
        query = query.offset(skip).limit(limit)
        
        # Execute queries
        result = await self.session.execute(query)
        count_result = await self.session.execute(count_query)
        
        messages = result.scalars().all()
        total = count_result.scalar_one()
        
        return messages, total
    
    async def get_by_id(self, id: str) -> Optional[Message]:
        """
        Get a message by ID with eager loading of relationships.
        
        This overrides the BaseRepository method to add eager loading
        for campaign and events relationships.
        
        Args:
            id: Message ID
            
        Returns:
            Message: Found message with relationships loaded, or None
        """
        query = (
            select(Message)
            .options(
                joinedload(Message.campaign),    # Eager load campaign
                selectinload(Message.events)     # Eager load events collection
            )
            .where(Message.id == id)
        )
        
        result = await self.session.execute(query)
        return result.scalars().first()

    async def get_retryable_messages(
        self,
        *,
        max_retries: int = 3,
        limit: int = 50
    ) -> List[Message]:
        """
        Get messages that can be retried.
        
        Args:
            max_retries: Maximum number of retry attempts
            limit: Maximum number of messages to return
            
        Returns:
            List[Message]: List of retryable messages
        """
        # Query for failed messages that can be retried
        query = select(Message).where(
            and_(
                Message.status == MessageStatus.FAILED,
                or_(
                    Message.meta_data.is_(None),  # No metadata at all
                    ~Message.meta_data.contains({"retry_count": max_retries})  # retry_count less than max
                )
            )
        ).order_by(Message.failed_at).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def check_events_for_messages(
        self,
        *,
        message_ids: List[str],
        user_id: str
    ) -> Tuple[int, List[str]]:
        """
        Check how many delivery events exist for given messages.
        
        This method performs a security-conscious check to count events only for messages
        that belong to the specified user, preventing information leakage about other
        users' messages.
        
        Args:
            message_ids: List of message IDs to check for events
            user_id: User ID for security validation
            
        Returns:
            Tuple[int, List[str]]: (total_events_count, message_ids_with_events)
            - total_events_count: Total number of events that would be deleted
            - message_ids_with_events: List of message IDs that have associated events
            
        Security:
            - Only counts events for messages owned by the specified user
            - Prevents cross-user information disclosure
            
        Performance:
            - Uses efficient subquery for user validation
            - Single database round-trip for both counts and message list
        """
        from app.models.message import MessageEvent
        
        # Count total events for user's messages
        events_count_query = select(func.count(MessageEvent.id)).where(
            and_(
                MessageEvent.message_id.in_(message_ids),
                # Security: ensure messages belong to user
                MessageEvent.message_id.in_(
                    select(Message.id).where(
                        and_(
                            Message.id.in_(message_ids),
                            Message.user_id == user_id
                        )
                    )
                )
            )
        )
        
        # Get distinct message IDs that have events (for user's messages only)
        messages_with_events_query = select(MessageEvent.message_id.distinct()).where(
            and_(
                MessageEvent.message_id.in_(message_ids),
                # Security: ensure messages belong to user
                MessageEvent.message_id.in_(
                    select(Message.id).where(
                        and_(
                            Message.id.in_(message_ids),
                            Message.user_id == user_id
                        )
                    )
                )
            )
        )
        
        # Execute both queries
        events_result = await self.session.execute(events_count_query)
        messages_result = await self.session.execute(messages_with_events_query)
        
        total_events = events_result.scalar_one()
        messages_with_events = [row[0] for row in messages_result.fetchall()]
        
        logger.debug(
            f"Event check for user {user_id}: {total_events} events found "
            f"across {len(messages_with_events)} messages"
        )
        
        return total_events, messages_with_events
    

    async def bulk_delete_with_batching(
        self,
        *,
        message_ids: List[str],
        user_id: str,
        force_delete: bool = False,
        batch_size: int = 1000
    ) -> Tuple[int, List[str], Dict[str, Any]]:
        """
        Bulk delete messages with batching for server stability and event safety.
        
        This method processes large deletion operations in smaller batches to prevent
        server overload, connection timeouts, and database lock contention. It handles
        both safe deletion (messages without events) and force deletion (with events).
        
        Args:
            message_ids: List of message IDs to delete
            user_id: User ID for authorization
            force_delete: Whether to delete messages that have delivery events
            batch_size: Number of messages to process per batch (max 5000)
            
        Returns:
            Tuple[int, List[str], Dict[str, Any]]: (total_deleted, failed_ids, batch_info)
            - total_deleted: Total number of messages successfully deleted
            - failed_ids: List of message IDs that failed to delete
            - batch_info: Dictionary with batch processing statistics
            
        Server Stability Features:
            - Processes deletions in configurable batch sizes
            - Includes inter-batch delays to prevent DB overload
            - Each batch is atomic (all succeed or all fail per batch)
            - Graceful handling of partial failures
            
        Event Safety:
            - When force_delete=False: Only deletes messages without events
            - When force_delete=True: Deletes events first, then messages
            - Two-phase deletion prevents foreign key violations
        """
        import asyncio
        
        total_deleted = 0
        all_failed_ids = []
        batches_processed = 0
        events_deleted_total = 0
        
        # Validate batch size
        if batch_size > 5000:
            batch_size = 5000
            logger.warning(f"Batch size capped at 5000 for stability")
        
        logger.info(
            f"Starting batched deletion: {len(message_ids)} messages, "
            f"batch_size={batch_size}, force_delete={force_delete}, user={user_id}"
        )
        
        # Process in batches
        for i in range(0, len(message_ids), batch_size):
            batch_ids = message_ids[i:i + batch_size]
            batches_processed += 1
            
            try:
                # Process this batch
                if force_delete:
                    deleted, failed, events_deleted = await self._delete_batch_with_events(
                        batch_ids, user_id
                    )
                    events_deleted_total += events_deleted
                else:
                    deleted, failed = await self._delete_batch_safe(batch_ids, user_id)
                
                total_deleted += deleted
                all_failed_ids.extend(failed)
                
                logger.debug(
                    f"Batch {batches_processed}: deleted {deleted}, failed {len(failed)}"
                )
                
                # Small delay between batches to prevent overwhelming DB
                if i + batch_size < len(message_ids):  # Don't delay after last batch
                    await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Batch {batches_processed} failed completely: {e}")
                # Add all IDs from failed batch to failed list
                all_failed_ids.extend(batch_ids)
        
        batch_info = {
            "batches_processed": batches_processed,
            "batch_size": batch_size,
            "total_messages": len(message_ids),
            "events_deleted": events_deleted_total
        }
        
        logger.info(
            f"Batched deletion completed: {total_deleted} deleted, "
            f"{len(all_failed_ids)} failed, {batches_processed} batches, "
            f"{events_deleted_total} events deleted"
        )
        
        return total_deleted, all_failed_ids, batch_info
    

    async def _delete_batch_safe(
        self,
        message_ids: List[str],
        user_id: str
    ) -> Tuple[int, List[str]]:
        """
        Delete a batch of messages safely (only messages without delivery events).
        
        This method performs safe deletion by only removing messages that have no
        associated delivery events. Messages with events are skipped and returned
        in the failed list, allowing the caller to handle them appropriately.
        
        Args:
            message_ids: List of message IDs to delete in this batch
            user_id: User ID for authorization
            
        Returns:
            Tuple[int, List[str]]: (deleted_count, failed_message_ids)
            - deleted_count: Number of messages successfully deleted
            - failed_message_ids: List of message IDs that have events (skipped)
            
        Safety Features:
            - Only deletes messages without delivery events
            - Preserves delivery tracking data by default
            - Atomic transaction per batch
            - User authorization on every message
            
        Performance:
            - Single transaction per batch
            - Efficient subquery to identify safe messages
            - Minimal database round-trips
        """
        from app.models.message import MessageEvent
        
        try:
            # Work within existing transaction - don't start a new one
            # Find messages that have NO events (safe to delete)
            messages_without_events_query = select(Message.id).where(
                and_(
                    Message.id.in_(message_ids),
                    Message.user_id == user_id,
                    # Only messages with NO events
                    ~Message.id.in_(
                        select(MessageEvent.message_id.distinct()).where(
                            MessageEvent.message_id.in_(message_ids)
                        )
                    )
                )
            )
            
            result = await self.session.execute(messages_without_events_query)
            safe_message_ids = [row[0] for row in result.fetchall()]
            
            if not safe_message_ids:
                # All messages have events - none can be safely deleted
                return 0, message_ids
            
            # Delete only the safe messages
            delete_query = delete(Message).where(
                and_(
                    Message.id.in_(safe_message_ids),
                    Message.user_id == user_id
                )
            )
            
            delete_result = await self.session.execute(delete_query)
            deleted_count = delete_result.rowcount
            
            # Messages that couldn't be deleted (have events)
            failed_ids = [mid for mid in message_ids if mid not in safe_message_ids]
            
            if failed_ids:
                logger.debug(
                    f"Safe deletion: {deleted_count} deleted, {len(failed_ids)} skipped (have events)"
                )
            else:
                logger.debug(f"Safe deletion: {deleted_count} deleted, no events found")
            
            return deleted_count, failed_ids
                
        except Exception as e:
            logger.error(f"Safe batch deletion failed: {e}")
            return 0, message_ids  # All IDs failed
    

    async def _delete_batch_with_events(
        self,
        message_ids: List[str],
        user_id: str
    ) -> Tuple[int, List[str], int]:
        """
        Delete a batch of messages WITH their delivery events (force deletion).
        
        This method performs force deletion by removing both messages and their
        associated delivery events in a two-phase process. Events are deleted first
        to avoid foreign key constraint violations, then messages are deleted.
        
        Args:
            message_ids: List of message IDs to delete in this batch
            user_id: User ID for authorization
            
        Returns:
            Tuple[int, List[str], int]: (deleted_count, failed_message_ids, events_deleted)
            - deleted_count: Number of messages successfully deleted
            - failed_message_ids: List of message IDs that failed to delete
            - events_deleted: Number of delivery events deleted
            
        Force Deletion Process:
            1. Delete all delivery events for the messages (prevents FK violations)
            2. Delete the messages themselves
            3. Both operations in single atomic transaction
            
        Data Loss Warning:
            - This permanently destroys delivery tracking data
            - Should only be used when explicitly confirmed by user
            - May impact delivery analytics and compliance records
            
        Performance:
            - Two-phase deletion in single transaction
            - Efficient bulk operations with IN clauses
            - User authorization on every operation
        """
        from app.models.message import MessageEvent
        
        try:
            # Work within existing transaction - don't start a new one
            # Phase 1: Delete events first (to avoid FK constraint violations)
            events_delete_query = delete(MessageEvent).where(
                and_(
                    MessageEvent.message_id.in_(message_ids),
                    # Security: only delete events for user's messages
                    MessageEvent.message_id.in_(
                        select(Message.id).where(
                            and_(
                                Message.id.in_(message_ids),
                                Message.user_id == user_id
                            )
                        )
                    )
                )
            )
            
            events_result = await self.session.execute(events_delete_query)
            events_deleted = events_result.rowcount
            
            # Phase 2: Delete messages
            messages_delete_query = delete(Message).where(
                and_(
                    Message.id.in_(message_ids),
                    Message.user_id == user_id
                )
            )
            
            messages_result = await self.session.execute(messages_delete_query)
            messages_deleted = messages_result.rowcount
            
            # Determine which messages failed to delete
            if messages_deleted < len(message_ids):
                # Query to find which messages still exist (failed to delete)
                remaining_query = select(Message.id).where(
                    and_(
                        Message.id.in_(message_ids),
                        Message.user_id == user_id
                    )
                )
                
                remaining_result = await self.session.execute(remaining_query)
                remaining_ids = [row[0] for row in remaining_result.fetchall()]
                failed_ids = remaining_ids
            else:
                failed_ids = []
            
            logger.info(
                f"Force deletion batch: {messages_deleted} messages deleted, "
                f"{events_deleted} events deleted, {len(failed_ids)} failed"
            )
            
            return messages_deleted, failed_ids, events_deleted
                
        except Exception as e:
            logger.error(f"Force deletion batch failed: {e}")
            return 0, message_ids, 0  # All IDs failed, no events deleted
    
    
    async def bulk_delete_campaign_messages(
        self,
        *,
        campaign_id: str,
        user_id: str,
        status: Optional[str] = None,
        from_date: Optional[str] = None,
        to_date: Optional[str] = None,
        limit: int = 10000,
        force_delete: bool = False,
        batch_size: int = 1000
    ) -> Tuple[int, List[str], Dict[str, Any]]:
        """
        Bulk delete messages for a campaign with event safety and server stability.
        
        This method efficiently deletes multiple messages belonging to a specific campaign
        with optional filtering by status and date range. Enhanced with event safety
        checking and batched processing for server stability under high load.
        
        Args:
            campaign_id: Campaign ID - messages must belong to this campaign
            user_id: User ID - for authorization (messages must belong to this user)
            status: Optional status filter (e.g., 'failed', 'sent')
            from_date: Optional from date filter (ISO format string)
            to_date: Optional to date filter (ISO format string)
            limit: Maximum number of messages to delete (default 10K, max 10K for safety)
            force_delete: Whether to delete messages that have delivery events
            batch_size: Number of messages to process per batch for stability
            
        Returns:
            Tuple[int, List[str], Dict[str, Any]]: (deleted_count, failed_ids, metadata)
            - deleted_count: Number of messages successfully deleted
            - failed_ids: List of message IDs that failed to delete
            - metadata: Dictionary with operation details including:
                - requires_confirmation: Whether force delete is needed
                - events_count: Number of events that would be deleted
                - events_deleted: Number of events actually deleted
                - batch_info: Batch processing statistics
                - safety_warnings: List of safety warnings
            
        Event Safety:
            - When force_delete=False: Returns safety warning if events exist
            - When force_delete=True: Deletes both messages and events
            - Two-phase deletion prevents foreign key violations
            
        Server Stability:
            - Processes large operations in configurable batches
            - Includes delays between batches to prevent DB overload
            - Graceful handling of partial failures
            
        Performance:
            - Handles 30K deletions efficiently with batching
            - Uses optimized queries with proper indexes
            - Single transaction per batch for consistency
        """
        # Validate limit for safety
        if limit > 10000:
            limit = 10000
        
        # Build query to get message IDs that match criteria
        id_subquery = (
            select(Message.id)
            .where(
                and_(
                    Message.campaign_id == campaign_id,
                    Message.user_id == user_id
                )
            )
            .limit(limit)
        )
        
        # Apply optional filters
        if status:
            id_subquery = id_subquery.where(Message.status == status)
        
        if from_date:
            try:
                from_date_obj = datetime.fromisoformat(from_date.replace('Z', '+00:00'))
                id_subquery = id_subquery.where(Message.created_at >= from_date_obj)
            except ValueError:
                pass
        
        if to_date:
            try:
                to_date_obj = datetime.fromisoformat(to_date.replace('Z', '+00:00'))
                id_subquery = id_subquery.where(Message.created_at <= to_date_obj)
            except ValueError:
                pass
        
        try:
            # Get the message IDs that match criteria
            id_result = await self.session.execute(id_subquery)
            message_ids = [row[0] for row in id_result.fetchall()]
            
            if not message_ids:
                return 0, [], {
                    "requires_confirmation": False,
                    "events_count": 0,
                    "events_deleted": 0,
                    "batch_info": {"batches_processed": 0, "batch_size": batch_size},
                    "safety_warnings": []
                }
            
            # Check for events if not force deleting
            if not force_delete:
                events_count, messages_with_events = await self.check_events_for_messages(
                    message_ids=message_ids,
                    user_id=user_id
                )
                
                if events_count > 0:
                    # Return safety warning instead of proceeding
                    return 0, [], {
                        "requires_confirmation": True,
                        "events_count": events_count,
                        "events_deleted": 0,
                        "batch_info": {"batches_processed": 0, "batch_size": batch_size},
                        "safety_warnings": [
                            f"Cannot delete {len(message_ids)} message(s) because {events_count} delivery/status events exist.",
                            "Please confirm deletion to remove both message(s) and all associated events."
                        ]
                    }
            
            # Proceed with deletion using batching
            deleted_count, failed_ids, batch_info = await self.bulk_delete_with_batching(
                message_ids=message_ids,
                user_id=user_id,
                force_delete=force_delete,
                batch_size=batch_size
            )
            
            logger.info(
                f"Campaign bulk delete completed: campaign={campaign_id}, "
                f"deleted={deleted_count}, failed={len(failed_ids)}, "
                f"force_delete={force_delete}, batches={batch_info.get('batches_processed', 0)}"
            )
            
            return deleted_count, failed_ids, {
                "requires_confirmation": False,
                "events_count": 0,
                "events_deleted": batch_info.get("events_deleted", 0),
                "batch_info": batch_info,
                "safety_warnings": []
            }
            
        except Exception as e:
            logger.error(f"Error in bulk_delete_campaign_messages: {e}")
            raise e

    
    async def bulk_delete_messages(
        self,
        *,
        message_ids: List[str],
        user_id: str,
        campaign_id: Optional[str] = None,
        force_delete: bool = False
    ) -> Tuple[int, List[str], Dict[str, Any]]:
        """
        Global bulk delete messages by message IDs with event safety.
        
        This method efficiently deletes multiple messages by their specific IDs with
        user authorization and event safety checking. Designed for edge cases like
        cross-campaign cleanup, orphaned message removal, and power user operations.
        
        Args:
            message_ids: List of message IDs to delete (max 1000 for safety)
            user_id: User ID - for authorization (messages must belong to this user)
            campaign_id: Optional campaign context for additional validation
            force_delete: Whether to delete messages that have delivery events
            
        Returns:
            Tuple[int, List[str], Dict[str, Any]]: (deleted_count, failed_ids, metadata)
            - deleted_count: Number of messages successfully deleted
            - failed_ids: List of message IDs that failed to delete
            - metadata: Dictionary with operation details including:
                - requires_confirmation: Whether force delete is needed
                - events_count: Number of events that would be deleted
                - events_deleted: Number of events actually deleted
                - safety_warnings: List of safety warnings
            
        Event Safety:
            - When force_delete=False: Returns safety warning if events exist
            - When force_delete=True: Deletes both messages and events
            - Two-phase deletion prevents foreign key violations
            
        Use Cases:
            - Cross-campaign message cleanup by power users
            - Orphaned message removal during system maintenance
            - Selective message deletion from UI multi-select
            - Compliance-driven deletion by specific message IDs
            
        Performance:
            - Handles up to 1K deletions efficiently
            - Uses IN clause with message ID list
            - Smaller batches for safety vs campaign-scoped operations
        """

        # Validate input
        if not message_ids:
            return 0, [], {
                "requires_confirmation": False,
                "events_count": 0,
                "events_deleted": 0,
                "safety_warnings": []
            }
        
        # Safety limit - smaller than campaign-scoped for global operations
        if len(message_ids) > 1000:
            logger.warning(f"Global bulk delete limited to 1000 messages, received {len(message_ids)}")
            message_ids = message_ids[:1000]
        
        # Remove duplicates while preserving order
        unique_message_ids = list(dict.fromkeys(message_ids))
        
        try:
            # Check for events if not force deleting
            if not force_delete:
                events_count, messages_with_events = await self.check_events_for_messages(
                    message_ids=unique_message_ids,
                    user_id=user_id
                )
                
                if events_count > 0:
                    # Return safety warning instead of proceeding
                    return 0, [], {
                        "requires_confirmation": True,
                        "events_count": events_count,
                        "events_deleted": 0,
                        "safety_warnings": [
                            f"Cannot delete {len(unique_message_ids)} message(s) because {events_count} delivery/status events exist.",
                            "Please confirm deletion to remove both message(s) and all associated events."
                        ]
                    }
            
            # Proceed with deletion (using smaller batches for global operations)
            deleted_count, failed_ids, batch_info = await self.bulk_delete_with_batching(
                message_ids=unique_message_ids,
                user_id=user_id,
                force_delete=force_delete,
                batch_size=500  # Smaller batches for global operations
            )
            
            # Additional campaign context validation for failed messages
            if campaign_id and failed_ids:
                # Filter failed IDs to only those that actually belong to the campaign
                campaign_failed_query = select(Message.id).where(
                    and_(
                        Message.id.in_(failed_ids),
                        Message.user_id == user_id,
                        Message.campaign_id == campaign_id
                    )
                )
                result = await self.session.execute(campaign_failed_query)
                campaign_failed_ids = [row[0] for row in result.fetchall()]
                
                # Update failed list to only include campaign-context failures
                failed_ids = campaign_failed_ids
            
            logger.info(
                f"Global bulk delete completed: deleted={deleted_count}, "
                f"failed={len(failed_ids)}, force_delete={force_delete}, "
                f"campaign_context={campaign_id}, events_deleted={batch_info.get('events_deleted', 0)}"
            )
            
            return deleted_count, failed_ids, {
                "requires_confirmation": False,
                "events_count": 0,
                "events_deleted": batch_info.get("events_deleted", 0),
                "safety_warnings": []
            }
            
        except Exception as e:
            logger.error(f"Error in global bulk_delete_messages: {e}")
            raise e
</file>

</files>
